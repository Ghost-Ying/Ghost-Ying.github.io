<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>爱影客</title>
  
  
  <link href="https://tuumest.cn/atom.xml" rel="self"/>
  
  <link href="https://tuumest.cn/"/>
  <updated>2025-11-04T14:25:18.988Z</updated>
  <id>https://tuumest.cn/</id>
  
  <author>
    <name>Rupert-Tears</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>日拱一卒 Vol.001</title>
    <link href="https://tuumest.cn/blog/bd61c145.html/"/>
    <id>https://tuumest.cn/blog/bd61c145.html/</id>
    <published>2025-11-04T13:47:18.000Z</published>
    <updated>2025-11-04T14:25:18.988Z</updated>
    
    <content type="html"><![CDATA[<p><strong>问题：</strong></p><p>假设你负责电商交易数据的分析，现有以下三张核心表：</p><ul><li><code>orders</code>（订单表）：order_id, user_id, order_time, total_amount</li><li><code>order_items</code>（订单明细）：item_id, order_id, product_id, quantity, price</li><li><code>users</code>（用户表）：user_id, registration_time, city</li></ul><p><strong>业务需求：</strong></p><p>计算2023年第二季度（4月-6月）每个城市的“高价值用户数”。</p><p>定义：高价值用户需同时满足以下条件：</p><ol><li>在该季度内下单次数 ≥ 3次</li><li>该季度累计订单金额 ≥ 5000元</li><li>首次注册时间在2023年之前</li></ol><p>请写出SQL实现，并说明你的优化思路?</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    u.city,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> u.user_id) <span class="keyword">AS</span> high_value_user_count</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">    users u</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> (</span><br><span class="line">    <span class="comment">-- 步骤1：在订单层聚合，先过滤出Q2的订单，并计算每个用户的指标</span></span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">        user_id,</span><br><span class="line">        <span class="built_in">COUNT</span>(order_id) <span class="keyword">AS</span> order_count,</span><br><span class="line">        <span class="built_in">SUM</span>(total_amount) <span class="keyword">AS</span> total_amount</span><br><span class="line">    <span class="keyword">FROM</span> </span><br><span class="line">        orders</span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">        order_time <span class="operator">&gt;=</span> <span class="string">&#x27;2023-04-01&#x27;</span> </span><br><span class="line">        <span class="keyword">AND</span> order_time <span class="operator">&lt;</span> <span class="string">&#x27;2023-07-01&#x27;</span>  <span class="comment">-- 使用&lt;号是更标准的做法，避免包含7月1日零点</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">        user_id</span><br><span class="line">    <span class="keyword">HAVING</span> </span><br><span class="line">        <span class="built_in">COUNT</span>(order_id) <span class="operator">&gt;=</span> <span class="number">3</span> </span><br><span class="line">        <span class="keyword">AND</span> <span class="built_in">SUM</span>(total_amount) <span class="operator">&gt;=</span> <span class="number">5000</span>  <span class="comment">-- 步骤2：在聚合后直接过滤出高价值用户</span></span><br><span class="line">) o </span><br><span class="line"><span class="keyword">ON</span> u.user_id <span class="operator">=</span> o.user_id  <span class="comment">-- 关联上满足高价值行为的用户</span></span><br><span class="line"><span class="keyword">WHERE</span> </span><br><span class="line">    u.registration_time <span class="operator">&lt;</span> <span class="string">&#x27;2023-01-01&#x27;</span>  <span class="comment">-- 步骤3：进一步过滤出老用户</span></span><br><span class="line">    <span class="comment">-- AND u.city IS NOT NULL -- 可选：处理城市为NULL的情况</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    u.city;</span><br></pre></td></tr></table></figure><p><strong>优化思路说明：</strong></p><ol><li><strong>减少嵌套：</strong> 将订单聚合和高价值用户判定合并到一个子查询中，使用 <code>HAVING</code>子句进行过滤，结构更清晰。</li><li><strong>性能考量：</strong><ul><li>建议在 <code>orders(user_id, order_time)</code>上建立联合索引，这样在按用户分组和按时间过滤时可以利用索引。</li><li>建议在 <code>users(user_id, registration_time)</code>上建立索引以加速关联和筛选。</li></ul></li><li><strong>严谨性：</strong> 使用明确的日期格式，并考虑了时间范围的边界。</li></ol><h3 id="索引是什么？"><a href="#索引是什么？" class="headerlink" title="索引是什么？"></a>索引是什么？</h3><p>简单来说，<strong>索引就像一本书的目录</strong>。如果没有目录，你要找某个主题的内容，只能一页一页地翻（全表扫描）。而有了目录，你可以快速定位到内容所在的页码（数据所在的数据块），大大加快查找速度。</p><p>在数据库中，索引是一种独立的数据结构（如B-Tree, Bitmap, LSM-Tree等），它存储了表中某些列（索引键）的值以及这些值对应数据行的物理地址指针。</p><p><strong>在你这个查询中的具体作用：</strong></p><ul><li><strong>子查询部分：</strong> <code>SELECT user_id, COUNT(...) FROM orders WHERE order_time &gt;= &#39;2023-04-01&#39; AND order_time &lt; &#39;2023-07-01&#39; GROUP BY user_id</code><ul><li>如果没有索引，查询引擎需要<strong>全表扫描</strong> <code>orders</code>表，读取每一行数据，检查时间条件，然后进行分组聚合。如果表有几十亿条记录，这将非常缓慢。</li><li>如果存在 <code>(user_id, order_time)</code>的联合索引：<ul><li><strong>过滤 (<code>WHERE</code>)：</strong> 引擎可以快速地在索引树中定位到 <code>order_time</code>在 <code>[&#39;2023-04-01&#39;, &#39;2023-07-01&#39;)</code>范围内的索引条目，避免扫描全部数据。</li><li><strong>分组 (<code>GROUP BY user_id</code>)：</strong> 因为索引的第一列就是 <code>user_id</code>，并且索引条目是按照 <code>(user_id, order_time)</code>排序的。这意味着<strong>同一个user_id的所有订单（在时间范围内的）在索引中是紧挨着存储的</strong>。引擎可以顺序读取索引，轻松地完成分组计数和求和，这是一个非常高效的过程（称为“索引排序分组”）。</li></ul></li></ul></li><li><strong>主查询部分：</strong> <code>INNER JOIN ... ON u.user_id = o.user_id WHERE u.registration_time &lt; &#39;2023-01-01&#39;</code><ul><li><code>users</code>表上的 <code>(user_id, registration_time)</code>索引同样可以加速关联时的查找和注册时间的过滤。</li></ul></li></ul><h4 id="场景一：Hive-x2F-Spark-SQL-on-HDFS"><a href="#场景一：Hive-x2F-Spark-SQL-on-HDFS" class="headerlink" title="场景一：Hive &#x2F; Spark SQL (on HDFS)"></a>场景一：Hive &#x2F; Spark SQL (on HDFS)</h4><ul><li><p><strong>核心特点：</strong> 数据存储在HDFS上，格式多为列式（ORC, Parquet）。<strong>传统意义上的B-Tree索引在原生Hive中几乎不存在</strong>，因为HDFS不支持随机写，更新索引成本极高。</p></li><li><p><strong>“索引”的替代方案：</strong></p><ol><li><p><strong>分区（Partitioning）：</strong> 这是Hive&#x2F;Spark中<strong>最重要、最高效</strong>的“粗粒度索引”。你可以按日期（如 <code>dt</code>）对 <code>orders</code>表进行分区。</p><p>sql</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 创建分区表</span><br><span class="line">CREATE TABLE orders (...)</span><br><span class="line">PARTITIONED BY (dt STRING); -- 按天分区，例如 dt=&#x27;2023-04-01&#x27;</span><br><span class="line"></span><br><span class="line">-- 你的查询条件会变成，性能极大提升</span><br><span class="line">WHERE dt &gt;= &#x27;2023-04-01&#x27; AND dt &lt;= &#x27;2023-06-30&#x27;</span><br></pre></td></tr></table></figure><p><em>引擎只会读取2023年Q2这91个分区的数据，而不是全表。</em></p></li><li><p><strong>分桶（Bucketing）：</strong> 如果经常按 <code>user_id</code>进行分组或关联，可以对表进行分桶。</p><p>sql</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orders (...)</span><br><span class="line">PARTITIONED BY (dt STRING)</span><br><span class="line">CLUSTERED BY (user_id) INTO 1024 BUCKETS;</span><br></pre></td></tr></table></figure><p><em>相同 <code>user_id</code>的数据会落在同一个桶内，能优化 <code>GROUP BY</code>和 <code>JOIN</code>。</em></p></li><li><p><strong>ORC&#x2F;Parquet文件内部索引：</strong> 列式存储格式本身会为每个数据块记录min&#x2F;max等统计信息。如果查询 <code>order_time &gt; &#39;2023-04-01&#39;</code>，引擎可以直接跳过那些最大值 <code>&lt; &#39;2023-04-01&#39;</code>的数据块。这可以看作是一种“轻量级索引”。</p></li></ol></li><li><p><strong>面试回答建议：</strong> 当被问及Hive&#x2F;Spark的优化时，应优先强调 <strong>分区和分桶</strong> 的设计，而不是谈B-Tree索引。</p></li></ul><h4 id="场景二：Doris-x2F-ClickHouse-x2F-StarRocks-MPP数据库"><a href="#场景二：Doris-x2F-ClickHouse-x2F-StarRocks-MPP数据库" class="headerlink" title="场景二：Doris &#x2F; ClickHouse &#x2F; StarRocks (MPP数据库)"></a>场景二：Doris &#x2F; ClickHouse &#x2F; StarRocks (MPP数据库)</h4><ul><li><p><strong>核心特点：</strong> 这类现代OLAP数据库<strong>原生支持多种索引</strong>，并且针对大数据分析场景做了深度优化。</p><ol><li><p><strong>前缀索引（Aggregate Key）：</strong> 这是Doris的核心特性。在建表时，你指定的排序列（例如 <code>(user_id, order_time)</code>）会自动构成一个稀疏索引，数据按这些列排序存储。你的查询条件完美匹配前缀，可以极速定位数据。</p><p>sql</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orders (</span><br><span class="line">    user_id BIGINT,</span><br><span class="line">    order_time DATETIME,</span><br><span class="line">    ...</span><br><span class="line">)</span><br><span class="line">DUPLICATE KEY(user_id, order_time) -- 指定排序列，自动生成前缀索引</span><br><span class="line">...;</span><br></pre></td></tr></table></figure></li><li><p><strong>Bloom Filter索引：</strong> 特别适用于高基数列的等值查询（如 <code>user_id = 123</code>）。Doris会自动为某些列创建Bloom Filter，可以快速判断“某个user_id肯定不在这个数据块中”，从而跳过该数据块。</p></li><li><p><strong>二级索引（BitMap Index）：</strong> 适用于低基数列（如 <code>city</code>, <code>product_category</code>）的等值或范围查询。</p></li></ol></li></ul><h3 id="核心定义"><a href="#核心定义" class="headerlink" title="核心定义"></a>核心定义</h3><ul><li><strong>高基数列：</strong> 指该列中<strong>不重复值数量非常多</strong>（接近总行数）的列。</li><li><strong>低基数列：</strong> 指该列中<strong>不重复值数量非常少</strong>（远小于总行数）的列。</li></ul><p><strong>一个简单的比喻：</strong></p><ul><li>想象一个学校的学生花名册。<ul><li><strong>高基数</strong> 就像 <strong>“学生身份证号”</strong>：几乎每个学生都有一个独一无二的号码。</li><li><strong>低基数</strong> 就像 <strong>“学生性别”</strong>：只有“男”、“女”等少数几个值。</li><li><strong>中等基数</strong> 就像 <strong>“学生班级”</strong>：有几十个不同的班级，但有很多学生共享同一个班级。</li></ul></li></ul><h3 id="为什么区分高低基数很重要？"><a href="#为什么区分高低基数很重要？" class="headerlink" title="为什么区分高低基数很重要？"></a>为什么区分高低基数很重要？</h3><p>区分高低基数的主要意义在于<strong>指导我们选择最合适的数据处理技术和优化策略</strong>。</p><h4 id="1-索引选择"><a href="#1-索引选择" class="headerlink" title="1. 索引选择"></a>1. 索引选择</h4><p>这是你最开始的提问背景，也是最直接的应用。</p><ul><li><strong>低基数列：</strong> 适合 <strong>位图索引</strong><ul><li><strong>原理：</strong> 为每个不同的值（如 ‘M’, ‘F’）创建一个位图（bitmap），每一位表示一行是否是该值。例如：<ul><li><code>gender = &#39;M&#39;</code>的位图：<code>10101...</code>(表示第1、3、5…行是男性)</li><li><code>gender = &#39;F&#39;</code>的位图：<code>01010...</code>(表示第2、4…行是女性)</li></ul></li><li><strong>优势：</strong> 对 <code>WHERE gender = &#39;M&#39; AND status = &#39;active&#39;</code>这样的多条件查询，只需对两个位图进行高效的 <code>AND</code>操作，速度极快。</li><li><strong>劣势：</strong> 如果列基数很高，位图会变得非常稀疏和巨大，反而浪费空间、降低性能。</li></ul></li><li><strong>高基数列：</strong> 适合 <strong>B-Tree索引</strong> 或 <strong>Bloom Filter索引</strong><ul><li><strong>B-Tree索引：</strong> 适合范围查询（<code>WHERE user_id &gt; 1000</code>）和排序。</li><li><strong>Bloom Filter索引：</strong> 这是一种概率型数据结构，主要用于快速判断“某个值<strong>肯定不存在</strong>”于某个数据块中。对于 <code>WHERE user_id = 123456</code>这样的点查询，Bloom Filter可以快速跳过那些肯定不包含 <code>user_id=123456</code>的数据文件，减少IO。Doris&#x2F;ClickHouse 广泛使用它来优化高基数列的查询。</li></ul></li></ul><h4 id="2-数据编码与压缩"><a href="#2-数据编码与压缩" class="headerlink" title="2. 数据编码与压缩"></a>2. 数据编码与压缩</h4><ul><li><strong>低基数列：</strong> 非常适合 <strong>字典编码</strong><ul><li><strong>原理：</strong> 创建一个字典：<code>&#123;&#39;M&#39; -&gt; 1, &#39;F&#39; -&gt; 2&#125;</code>，然后将表中所有的 ‘M’ 替换成 1，所有的 ‘F’ 替换成 2。存储时只需存储紧凑的数字1和2，以及一个小小的字典表。压缩率非常高。</li></ul></li><li><strong>高基数列：</strong> 字典编码效果不佳，因为字典本身会非常大。通常采用其他通用压缩算法。</li></ul><h4 id="3-数据分布与分桶策略"><a href="#3-数据分布与分桶策略" class="headerlink" title="3. 数据分布与分桶策略"></a>3. 数据分布与分桶策略</h4><p>在 Spark&#x2F;Hive 中，当我们使用 <code>CLUSTERED BY</code>（分桶）时，选择分桶键至关重要。</p><ul><li><strong>理想的分桶键应该是高基数列</strong>，如 <code>user_id</code>。这样才能保证数据被均匀地分散到各个桶中，避免数据倾斜。如果用一个低基数列（如 <code>gender</code>）分桶，会导致数据严重倾斜（可能只有2个桶，每个桶非常大）。</li></ul><h4 id="4-查询性能"><a href="#4-查询性能" class="headerlink" title="4. 查询性能"></a>4. 查询性能</h4><ul><li>**对低基数列进行 <code>GROUP BY</code>或 <code>DISTINCT</code>**：速度非常快，因为需要处理的分组很少。<ul><li><code>SELECT city, COUNT(*) FROM users GROUP BY city;</code>– 即使表很大，分组数也有限。</li></ul></li><li>**对高基数列进行 <code>GROUP BY</code>或 <code>DISTINCT</code>**：可能会很慢，消耗大量内存，因为需要为海量的不重复值维护中间状态。<ul><li><code>SELECT user_id, COUNT(*) FROM click_log GROUP BY user_id;</code>– 如果用户数上亿，这个查询压力很大。</li></ul></li></ul><h3 id="面试场景"><a href="#面试场景" class="headerlink" title="面试场景"></a>面试场景</h3><p>如果面试官追问：“为什么你建议在Doris里对 <code>user_id</code>使用Bloom Filter索引，而不是位图索引？”</p><p>你现在可以这样回答：</p><p>“因为 <code>user_id</code>是一个典型的高基数列，它的不重复值数量巨大（可能上亿）。如果使用位图索引，会产生上亿个非常稀疏的位图，占用大量空间且性能低下。而Bloom Filter索引是专门为这种高基数列的等值查询优化设计的，它用很小的空间代价就能快速过滤掉不相关的数据块，性价比非常高。相反，如果是对 <code>order_status</code>这种只有几个状态值的低基数列进行过滤，位图索引就是最佳选择。”</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">--  技术镜鉴：</span><br><span class="line">1. 谓词下推：在数据量大的情况下，经可能将where条件放置在内层的子查询中，使它尽可能早的过滤掉大量不符合条件的数据。</span><br><span class="line">2. 关联条件：在关联条件中可以加入对关联表的整体过滤条件，从而减少子查询的层级。</span><br><span class="line">3. 数据过滤：对完整查询结果可以在最后使用having函数进行过滤。</span><br><span class="line">4. 边界考虑：在实际生产环境中，应该考虑到数据可能为空的情况，并提出解决和处理的方案。</span><br><span class="line">5. 构建索引：通过对数据表构建合适的索引，加快数据检索和关联的效率。</span><br><span class="line"></span><br><span class="line">-- 索引是什么？简单来说索引就像一本书的目录。倘若没有目录，你要找某个主题的内容，就只能一页一页的翻找（全表扫描）。而有了目录，就可以快速定位到内容所在的位置（数据所在的数据块），大大的加速了查询效率。</span><br><span class="line">-- 官方概念：索引是一种独立的数据结构，它存储了表中某些列（索引键）的值以及这些值对应数据行的物理地址指针。</span><br><span class="line">-- 对比差异：</span><br><span class="line">-- 如果没有索引：查询引擎需要全表扫描，读取每一行数据，检查时间条件，然后分组聚合。若表有几十亿条数据，这将是一个非常缓慢的过程。</span><br><span class="line">-- 若存在 user_id,order_time的联合索引：</span><br><span class="line">-- 1. where过滤，引擎可以快速定位到order_time在时间范围内的索引条目，避免全表扫描。</span><br><span class="line">-- 2. group by分组，因为索引的第一列是user_id，并且索引条目是按照（user_id,order_time）排序的。这意味着每一个user_id的所有订单在索引中是紧挨着存储的。引擎就可以顺序读取索引，轻松的完成分组和聚合，这个高效的过程被称之为：“索引排序分组”</span><br><span class="line">-- 如何创建索引：不同数据库中语法略微不同，但核心思想一致。</span><br><span class="line">-- 类MySQL语法</span><br><span class="line">create index idx_orders_user_time on orders(user_id, order_time);</span><br><span class="line">-- 不同的查询引擎索引机制有着本质的不同比如：hive/spark sql基于hdfs存储 和 doris/clickhouse</span><br><span class="line"></span><br><span class="line">-- hive/spark 其核心的特点是数据存储在hdfs上，格式为列式，传统的索引在原生的hive中几乎不存在，因为hdfs不支持随机写，更新索引的成本极高。</span><br><span class="line">-- 因此作为索引的替代方案：着重于分区和分桶的设计 (分区和分桶是“设计时”的优化，而索引是“运行时”的优化)</span><br><span class="line">-- 1.分区（partition）是最重要、最高效的“粗粒度索引”，可以按照日期dt进行分区，读取特点分区的数据。</span><br><span class="line">-- 2.分桶（bucket）若按照user_id进行分组或关联，可以对表进行分桶。相同的user_id的数据会落在同一个桶中，能够优化group by 和join。</span><br><span class="line">-- 3.orc/parquet文件内部索引：列式存储格式会为每个数据块记录min/max等统计信息，若查询order_tiem &gt; 20251103 引擎可以直接跳过不符合的数据块，这可以看做一种轻量级的索引。</span><br><span class="line"></span><br><span class="line">-- doris为代表的MPP（大规模并行处理）数据库</span><br><span class="line">-- 原生支持多种索引，且针对大数据分析场景做了深度优化。</span><br><span class="line">-- 1. 前缀索引，这是doris的核心特性。在建表时，你指定的排序列（如user_id，order_time）会自动构成一个稀疏索引，数据按这些排序存储。当前查询条件完美匹配前缀，可以极速定位数据。</span><br><span class="line">-- 2. 布隆过滤器：特别适用于高基数列的等值查询（如user_id = 123）.doris会自动为某些列创建 bloom filter，可以快速判断某个 user_id肯定不会在这个数据块中，从而跳过这个数据块。</span><br><span class="line">-- 3. 二级索引：适用于低基数列（如city,product_category）的等值或者范围查询。</span><br><span class="line"></span><br><span class="line">-- 性能优化的势能轨迹：首先给出通用的索引概念，然后主动反问“当前的技术栈是什么”是基于hive/spark的离线数仓，还是doris的实时数仓？知己知彼，对症下游，根据反馈细谈针对数仓的具体优化方案。技术的广度与沟通的能力最终要映射在业务场景的理解能力。</span><br><span class="line"></span><br><span class="line">-- 高基数列:指列中不重复数值非常多的列。</span><br><span class="line">-- 低基数列:指列中不重复数值非常少的列。</span><br><span class="line">-- 比如一个公司的花名册，员工的身份证号就是高基数列，每一个员工都有一个独一无二的身份证号。而员工的性别就是一个低基数列，只有男女等少数几个值。中等奇数即公司的部门，一个公司可能有十几二十个部门。</span><br><span class="line">-- 区分高低基数的主要意义在于指导我们选择最合适的数据处理技术和优化策略。</span><br><span class="line">-- 低基数列适合位图索引，原理创建一个位图bitmap，每一位表示一行是否有该值，如果基数列很高，位图就会变得非常稀疏和巨大，反而浪费空间、降低性能。且适合于字典编码，将内容映射为连续的数字，压缩效率高。</span><br><span class="line">-- 高基数列适合于B-Tree索引（适合范围查询）或者 bloom filter索引（一种概率型模型，主要用于快速判断“某个值肯定不在”某个数据块中）。</span><br><span class="line">-- 数据分布与分桶策略：理想的分桶键应该是高基数列，这样才能保证数据被均匀的分散到各个桶中，避免数据倾斜。</span><br><span class="line">-- 为什么建议在Doris里对 user_id使用Bloom Filter索引，而不是位图索引？</span><br><span class="line">-- 因为 user_id是一个典型的高基数列，它的不重复值数量巨大（可能上亿）。</span><br><span class="line">-- 如果使用位图索引，会产生上亿个非常稀疏的位图，占用大量空间且性能低下。</span><br><span class="line">-- 而Bloom Filter索引是专门为这种高基数列的等值查询优化设计的，它用很小的空间代价就能快速过滤掉不相关的数据块，性价比非常高。</span><br><span class="line">-- 相反，如果是对 order_status这种只有几个状态值的低基数列进行过滤，位图索引就是最佳选择。 </span><br></pre></td></tr></table></figure><h3 id="内容归纳与升华"><a href="#内容归纳与升华" class="headerlink" title="内容归纳与升华"></a>内容归纳与升华</h3><p>我将你的精彩总结重新组织成一个更体系化的框架，方便你面试时清晰地呈现知识深度。</p><h4 id="一、SQL优化核心思想"><a href="#一、SQL优化核心思想" class="headerlink" title="一、SQL优化核心思想"></a><strong>一、SQL优化核心思想</strong></h4><ol><li><strong>减少数据量原则</strong>：这是优化的黄金法则。<ul><li><strong>谓词下推</strong>：在查询的最内层尽早过滤数据。</li><li><strong>有效索引</strong>：通过索引直接定位所需数据块，避免全表扫描。</li></ul></li><li><strong>简化计算复杂度原则</strong>：<ul><li><strong>减少嵌套</strong>：扁平化的SQL更易读，也给了查询优化器更多优化空间。</li><li>**合理使用<code>HAVING</code>**：仅对聚合后的结果进行过滤，避免先聚合大量数据再丢弃。</li></ul></li><li><strong>鲁棒性原则</strong>：<ul><li><strong>边界情况处理</strong>：考虑<code>NULL</code>值、数据倾斜、空值等生产环境中常见问题。</li></ul></li></ol><h4 id="二、索引技术选型矩阵（面试核心武器）"><a href="#二、索引技术选型矩阵（面试核心武器）" class="headerlink" title="二、索引技术选型矩阵（面试核心武器）"></a><strong>二、索引技术选型矩阵（面试核心武器）</strong></h4><p>这是一个可以直观展示你技术深度的框架：</p><table><thead><tr><th align="left"><strong>列类型</strong></th><th align="left"><strong>核心问题</strong></th><th align="left"><strong>Hive&#x2F;Spark (离线批处理)</strong></th><th align="left"><strong>Doris&#x2F;ClickHouse (实时交互)</strong></th><th align="left"><strong>优化目标</strong></th></tr></thead><tbody><tr><td align="left"><strong>所有列</strong></td><td align="left"><strong>如何快速跳过无关数据？</strong></td><td align="left"><strong>分区(Partitioning)</strong>  • 按时间、地域等粗粒度划分</td><td align="left"><strong>排序(Ordering Key)</strong>  • 数据按排序列物理排序</td><td align="left"><strong>粗粒度数据裁剪</strong></td></tr><tr><td align="left"><strong>高基数列</strong>  (如<code>user_id</code>)</td><td align="left"><strong>如何高效进行等值查询？</strong></td><td align="left"><strong>分桶(Clustering&#x2F;Bucketing)</strong>  • 将数据散列到多个桶，优化<code>JOIN</code>&#x2F;<code>GROUP BY</code></td><td align="left"><strong>Bloom Filter索引</strong>  • 概率性判断“数据不存在”，快速跳过数据块</td><td align="left"><strong>避免数据倾斜，加速点查询</strong></td></tr><tr><td align="left"><strong>低基数列</strong>  (如<code>status</code>)</td><td align="left"><strong>如何高效进行多条件过滤？</strong></td><td align="left"><strong>列式存储统计信息</strong>  • ORC&#x2F;Parquet的Min&#x2F;Max索引</td><td align="left"><strong>位图索引(Bitmap Index)</strong>  • 对每个值生成位图，支持快速<code>AND</code>&#x2F;<code>OR</code>运算</td><td align="left"><strong>极致压缩，快速多条件过滤</strong></td></tr></tbody></table><p><strong>面试话术建议</strong>：“当谈到优化时，我的思路是一个决策树：首先看业务场景和技术栈。如果是Hive&#x2F;Spark，我优先考虑<strong>分区和分桶</strong>的设计。如果是Doris，我会利用其<strong>前缀索引和Bloom Filter</strong>。其次，我会分析查询模式涉及的列是<strong>高基数还是低基数</strong>，从而选择最合适的索引类型，比如对<code>user_id</code>用Bloom Filter，对<code>status</code>用位图索引。”</p><h4 id="三、高低基数：数据处理的“第一性原理”"><a href="#三、高低基数：数据处理的“第一性原理”" class="headerlink" title="三、高低基数：数据处理的“第一性原理”"></a><strong>三、高低基数：数据处理的“第一性原理”</strong></h4><p>你的理解非常准确。高低基数是决定数据分布、存储、压缩和查询性能的底层属性。</p><ul><li><strong>低基数 -&gt; 高重复 -&gt; 适合压缩、位图索引</strong>（思考：如何利用“重复”）</li><li><strong>高基数 -&gt; 低重复 -&gt; 适合散列、Bloom Filter</strong>（思考：如何管理“唯一”）</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;问题：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;假设你负责电商交易数据的分析，现有以下三张核心表：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;orders&lt;/code&gt;（订单表）：order_id, user_id, order_time, total_amount&lt;/</summary>
      
    
    
    
    <category term="大数据" scheme="https://tuumest.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="技术镜鉴" scheme="https://tuumest.cn/tags/%E6%8A%80%E6%9C%AF%E9%95%9C%E9%89%B4/"/>
    
  </entry>
  
  <entry>
    <title>每日分享 Spark</title>
    <link href="https://tuumest.cn/blog/e1a8e7f.html/"/>
    <id>https://tuumest.cn/blog/e1a8e7f.html/</id>
    <published>2024-03-07T11:30:14.000Z</published>
    <updated>2025-11-04T14:51:06.823Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、简述什么是spark？"><a href="#一、简述什么是spark？" class="headerlink" title="一、简述什么是spark？"></a>一、简述什么是spark？</h1><blockquote><ol><li>spark是一个计算引擎；</li><li>spark是基于内存运算的，比传统的hadoop计算引擎速度要快；</li><li>spark支持多种部署模式，单机部署、独立部署模式、yarn、mesos、k8s等；</li><li>spark可以读取多种数据存储系统或者组件数据，例如hdfs、hbase、hive等；</li></ol></blockquote><blockquote><p>首先描述spark的简介，一句话概括。其次描述其特点，最后理论到实践，描述其应用场景。</p><p>简介 → 特点 → 应用场景</p><ul><li>简介：spark由scala语言构建的、内存计算引擎，针对大规模数据集和复杂的数据处理任务，提供了高效的数据处理能力。</li><li>特点：<ul><li>易用：<ul><li>（数据介质自由）它可以和任何存储系统进行连接，如本地存储系统、HDFS、Hive、Hbase等；</li><li>（资源管理器自由）资源管理器可选本地模式、独立部署模式、yarn、mesos、k8s等；</li><li>（编程语言自由）spark提供了丰富的api和易于使用的编程模型；</li></ul></li><li>快速：<ul><li>源于其利用内存进行计算和基于RDD的弹性数据集模型；</li></ul></li><li>通用：（神通广大）<ul><li>批处理、流处理、交互式查询（spark sql）、机器学习（MLlib）</li></ul></li></ul></li><li>应用场景：<ul><li>大规模数据处理：适用于PB级别的数据，可以快速执行复杂的数据转换和分析任务。</li><li>实时数据处理：spark streaming 处理实时流数据，支持低时延的数据处理需求。</li><li>机器学习：MLlib提供了一系列机器学习算法，可用于大规模数据的建模和预测。</li><li>数据探索和可视化：通过spark  sql 和 dataframe api 可以进行交互式的数据探索和分析，支持复杂的查询和可视化操作。</li></ul></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、简述什么是spark？&quot;&gt;&lt;a href=&quot;#一、简述什么是spark？&quot; class=&quot;headerlink&quot; title=&quot;一、简述什么是spark？&quot;&gt;&lt;/a&gt;一、简述什么是spark？&lt;/h1&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;spark是一</summary>
      
    
    
    
    <category term="大数据" scheme="https://tuumest.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="每日分享" scheme="https://tuumest.cn/tags/%E6%AF%8F%E6%97%A5%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>华为OD 机考面试题</title>
    <link href="https://tuumest.cn/blog/de43bd01.html/"/>
    <id>https://tuumest.cn/blog/de43bd01.html/</id>
    <published>2024-03-06T15:37:13.000Z</published>
    <updated>2025-11-04T14:51:28.672Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>昨天晚上参加了华为OD的机考，记录一下考试流程，顺便做个复盘。</p></blockquote><h1 id="一、华为OD机考流程"><a href="#一、华为OD机考流程" class="headerlink" title="一、华为OD机考流程"></a>一、华为OD机考流程</h1><ul><li>考试流程<ul><li>邮箱接收考试地址</li><li>电脑网页登入，确认个人信息</li><li>开启摄像头，并拍照（考试过程全程开启）</li><li>开启全屏幕录制</li><li>手机扫描二维码，进入小程序页面（必须在这个页面停留，手机常亮，直到交卷）</li><li>开始考试（3到算法题，一共400分，分值分布100、100、150，150及格）</li></ul></li></ul><h1 id="二、灰度图"><a href="#二、灰度图" class="headerlink" title="二、灰度图"></a>二、灰度图</h1><blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">题干：</span><br><span class="line">黑白图像常采用灰度图的方式存储，即图像的每个像素填充一个灰阶值。</span><br><span class="line"><span class="number">256</span>阶灰度图是一个灰阶值取值范围为<span class="number">0</span>-<span class="number">255</span>的灰阶矩阵，<span class="number">0</span>表示全黑、<span class="number">255</span>表示全白，范围内的其他值表示不同的灰度。</span><br><span class="line">比如下面的图像及其对应的灰阶矩阵</span><br><span class="line"><span class="number">10</span> <span class="number">10</span> <span class="number">255</span> <span class="number">34</span> <span class="number">0</span> <span class="number">1</span> <span class="number">255</span> <span class="number">8</span> <span class="number">0</span> <span class="number">3</span> <span class="number">255</span> <span class="number">6</span> <span class="number">0</span> <span class="number">5</span> <span class="number">255</span> <span class="number">4</span> <span class="number">0</span> <span class="number">7</span> <span class="number">255</span> <span class="number">2</span> <span class="number">0</span> <span class="number">9</span> <span class="number">255</span> <span class="number">21</span></span><br><span class="line"><span class="number">1</span>、所有数值以空格分隔</span><br><span class="line"><span class="number">2</span>、前两个数分别表示矩阵的行数和列数</span><br><span class="line"><span class="number">3</span>、从第三个数开始，每两个数一组，每组第一个数是灰阶值，第二个数表示该灰阶值从左到右，从上到下（可理解为将二维数组按行存储在一维矩阵中）的连续像素个数。比如题目所述例子，“<span class="number">255</span> <span class="number">34</span>”表示有连续<span class="number">34</span>个像素的灰阶值是<span class="number">255</span>。</span><br><span class="line">如此，图像软件在打开此格式灰度图的时候，就可以根据此算法从压缩数据恢复出原始灰度图矩阵。</span><br><span class="line">请从输入的压缩数恢复灰度图原始矩阵，并返回指定像素的灰阶值。</span><br><span class="line"></span><br><span class="line"><span class="number">10</span> <span class="number">10</span> <span class="number">255</span> <span class="number">34</span> <span class="number">0</span> <span class="number">1</span> <span class="number">255</span> <span class="number">8</span> <span class="number">0</span> <span class="number">3</span> <span class="number">255</span> <span class="number">6</span> <span class="number">0</span> <span class="number">5</span> <span class="number">255</span> <span class="number">4</span> <span class="number">0</span> <span class="number">7</span> <span class="number">255</span> <span class="number">2</span> <span class="number">0</span> <span class="number">9</span> <span class="number">255</span> <span class="number">21</span></span><br><span class="line"><span class="number">3</span> <span class="number">4</span></span><br><span class="line">输入包括两行，第一行是灰度图压缩数据，第二行表示一个像素位置的行号和列号，如：<span class="number">0</span> <span class="number">0</span> 表示左上角像素。</span><br><span class="line"></span><br><span class="line">附加条件：</span><br><span class="line">输入数据表示的灰阶矩阵的指定像素的灰阶值。</span><br><span class="line"><span class="number">1</span>、系统保证输入的压缩数据是合法有效的，不会出现数据越界、数值不合法等无法恢复的场景；</span><br><span class="line"><span class="number">2</span>、系统保证输入的像素坐标是合法的，不会出现不在矩阵中的像素；</span><br><span class="line"><span class="number">3</span>、矩阵的行和列数范围为：(<span class="number">0</span>,<span class="number">100</span>]；</span><br><span class="line"><span class="number">4</span>、灰阶值取值范围：[<span class="number">0</span>, <span class="number">255</span>]；</span><br></pre></td></tr></table></figure></blockquote><p>​由于长期使用ChatGPT等工具原因，考试不能百度、代码也不能补全，因此很多基础的语法都有些生疏了。科学飞速发展的阶段，我享受科技进步带来的便捷，并逐渐产生依赖。当有一天它不能使用时，亦或是像如今的手机一样，AI时代的到临，人类该何去何从？越说越远了，哈哈哈，上代码吧。下面是我的解题思路及全部代码。(遇事不决、暴力破解)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:52 2024/3/5</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">one</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 黑白图像常采用灰度图的方式存储，即图像的每个像素填充一个灰阶值，256阶灰度图是一个灰阶值取值范围为0-255的灰阶矩阵，0表示全黑、255表示全白，范围内的其他值表示不同的灰度，比如下面的图像及其对应的灰阶矩阵</span></span><br><span class="line"><span class="comment">     * 但在计算机中实际存储时，会使用压缩算法，其中一种压缩格式和描述如下：</span></span><br><span class="line"><span class="comment">     * 10 10 255 34 0 1 255 8 0 3 255 6 0 5 255 4 0 7 255 2 0 9 255 21</span></span><br><span class="line"><span class="comment">     * 1、所有数值以空格分隔</span></span><br><span class="line"><span class="comment">     * 2、前两个数分别表示矩阵的行数和列数</span></span><br><span class="line"><span class="comment">     * 3、从第三个数开始，每两个数一组，每组第一个数是灰阶值，第二个数表示该灰阶值从左到右，从上到下（可理解为将二维数组按行存储在一维矩阵中）的连续像素个数。比如题目所述例子，“255 34”表示有连续34个像素的灰阶值是255。</span></span><br><span class="line"><span class="comment">     * 如此，图像软件在打开此格式灰度图的时候，就可以根据此算法从压缩数据恢复出原始灰度图矩阵。</span></span><br><span class="line"><span class="comment">     * 请从输入的压缩数恢复灰度图原始矩阵，并返回指定像素的灰阶值。</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * 10 10 255 34 0 1 255 8 0 3 255 6 0 5 255 4 0 7 255 2 0 9 255 21</span></span><br><span class="line"><span class="comment">     * 3 4</span></span><br><span class="line"><span class="comment">     * 输入包括两行，第一行是灰度图压缩数据，第二行表示一个像素位置的行号和列号，如：0 0 表示左上角像素。</span></span><br><span class="line"><span class="comment">     * 0</span></span><br><span class="line"><span class="comment">     * 输入数据表示的灰阶矩阵的指定像素的灰阶值。</span></span><br><span class="line"><span class="comment">     * 1、系统保证输入的压缩数据是合法有效的，不会出现数据越界、数值不合法等无法恢复的场景；</span></span><br><span class="line"><span class="comment">     * 2、系统保证输入的像素坐标是合法的，不会出现不在矩阵中的像素；</span></span><br><span class="line"><span class="comment">     * 3、矩阵的行和列数范围为：(0,100]；</span></span><br><span class="line"><span class="comment">     * 4、灰阶值取值范围：[0, 255]；</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * 3 5</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 21:10 2024/3/5</span></span><br><span class="line"><span class="comment">         * Description:</span></span><br><span class="line"><span class="comment">         * 10 10 56 34 99 1 87 8 99 3 255 6 99 5 255 4 99 7 255 2 99 9 255 21</span></span><br><span class="line"><span class="comment">         * 3 4</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// 1. 输入两行数据,第一行代表图形压缩的数据,第二行代表读取的位置;</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">origin</span> <span class="operator">=</span> <span class="string">&quot;10 10 56 34 99 1 87 8 99 3 255 6 99 5 255 4 99 7 255 2 99 9 255 21\n1 1&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 56 56 56 56 56 56</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 56 56 56 56 56 56</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 56 56 56 56 56 56</span></span><br><span class="line"><span class="comment">         * 56 56 56 56 99 87 87 87 87 87</span></span><br><span class="line"><span class="comment">         * 87 87 87 99 99 99 255 255 255 255</span></span><br><span class="line"><span class="comment">         * 255 255 99 99 99 99 99 255 255 255</span></span><br><span class="line"><span class="comment">         * 255 99 99 99 99 99 99 99 255 255</span></span><br><span class="line"><span class="comment">         * 99 99 99 99 99 99 99 99 99 255</span></span><br><span class="line"><span class="comment">         * 255 255 255 255 255 255 255  255 255 255</span></span><br><span class="line"><span class="comment">         * 255 255 255 255 255 255 255  255 255 255</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">grayValue</span> <span class="operator">=</span> getGrayValue(origin);</span><br><span class="line">        System.out.println(grayValue);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String <span class="title function_">getGrayValue</span><span class="params">(String origin)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> origin.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">0</span>];</span><br><span class="line">        String[] split = data.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="type">String</span> <span class="variable">row</span> <span class="operator">=</span> split[<span class="number">0</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">column</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; split.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span> || i == <span class="number">1</span>) &#123;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                list.add(split[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;row: &quot;</span> + row);</span><br><span class="line">        System.out.println(<span class="string">&quot;column: &quot;</span> + column);</span><br><span class="line">        System.out.println(<span class="string">&quot;list: &quot;</span> + list);</span><br><span class="line">        <span class="comment">// 10 行 10列  100个元素</span></span><br><span class="line">        <span class="comment">// 3 行 4列  (3-1)*10+4 = 24;     第24个元素</span></span><br><span class="line">        <span class="comment">// list 奇数是颜色 偶数是位置</span></span><br><span class="line">        <span class="comment">// 判断 目标元素数值&gt;第一个奇数位置,若小于或等于则取这个奇数前一位的数值作为颜色</span></span><br><span class="line">        <span class="comment">//      目标元素数值&lt;第一个奇数位置,用第一个奇数位置的值加第二个奇数位置的值,继续判断,依次类推找出元素颜色;</span></span><br><span class="line">        <span class="comment">// 新建颜色list</span></span><br><span class="line">        List&lt;String&gt; colour = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 新建位置list</span></span><br><span class="line">        List&lt;String&gt; location = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; list.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                colour.add(list.get(i));</span><br><span class="line">                flag = <span class="literal">false</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                location.add(list.get(i));</span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;colour:&quot;</span> + colour);</span><br><span class="line">        System.out.println(<span class="string">&quot;location:&quot;</span> + location);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入参为 3,4</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">input</span> <span class="operator">=</span> origin.split(<span class="string">&quot;\n&quot;</span>)[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">row_value</span> <span class="operator">=</span> input.split(<span class="string">&quot; &quot;</span>)[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">column_value</span> <span class="operator">=</span> input.split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>];</span><br><span class="line">        System.out.println(<span class="string">&quot;row_value: &quot;</span> + row_value);</span><br><span class="line">        System.out.println(<span class="string">&quot;column_value: &quot;</span> + column_value);</span><br><span class="line">        <span class="type">int</span> <span class="variable">target_location</span> <span class="operator">=</span> (Integer.parseInt(row_value) - <span class="number">1</span>) * Integer.parseInt(row) + Integer.parseInt(column_value);</span><br><span class="line">        System.out.println(<span class="string">&quot;target_location:&quot;</span> + target_location);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// target_location 24</span></span><br><span class="line">        <span class="comment">// location:[34, 1, 8, 3, 6, 5, 4, 7, 2, 9, 21]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// index</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> getIndex(location, target_location);</span><br><span class="line">        System.out.println(<span class="string">&quot;index:&quot;</span> + index);</span><br><span class="line">        System.out.println(<span class="string">&quot;result:&quot;</span> + colour.get(index));</span><br><span class="line">        <span class="keyword">return</span> colour.get(index);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">getIndex</span><span class="params">(List&lt;String&gt; location, <span class="type">int</span> target_location)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; location.size(); i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">location_sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i1</span> <span class="operator">=</span> <span class="number">0</span>; i1 &lt; i+<span class="number">1</span>; i1++) &#123;</span><br><span class="line">                location_sum += Integer.parseInt(location.get(i1));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (target_location &lt;= location_sum) &#123;</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>现在我们在借助科技的力量，看看ChatGPT怎么解。</p><p>输入题干后，我们的人工智能GPT，15秒就给出了解题思路和答案，接下来让我们一起看一下吧。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">主要任务是从给定的压缩数据中恢复出原始灰度图矩阵，并且能够根据给定的像素位置返回其灰阶值。</span><br><span class="line"><span class="number">1.</span>解析输入数据：首先，我们需要解析输入的压缩数据和像素位置。</span><br><span class="line"><span class="number">2.</span>恢复灰度图矩阵：根据压缩数据，我们可以构建原始的灰度图矩阵。对于每一组压缩数据，我们将对应的灰阶值填充到矩阵中连续的像素位置。</span><br><span class="line"><span class="number">3.</span>返回指定像素的灰阶值：根据给定的像素位置，我们可以直接在恢复的灰度图矩阵中查找对应位置的灰阶值。</span><br></pre></td></tr></table></figure><blockquote><p>下面是相应的Java代码实现：</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 读取输入</span></span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">scanner</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">String</span> <span class="variable">compressData</span> <span class="operator">=</span> scanner.nextLine(); <span class="comment">// 压缩数据</span></span><br><span class="line">        String[] pixelPosition = scanner.nextLine().split(<span class="string">&quot; &quot;</span>); <span class="comment">// 像素位置</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 解析压缩数据</span></span><br><span class="line">        String[] compressArr = compressData.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="type">int</span> <span class="variable">rows</span> <span class="operator">=</span> Integer.parseInt(compressArr[<span class="number">0</span>]); <span class="comment">// 矩阵行数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">cols</span> <span class="operator">=</span> Integer.parseInt(compressArr[<span class="number">1</span>]); <span class="comment">// 矩阵列数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 恢复灰度图矩阵</span></span><br><span class="line">        <span class="type">int</span>[][] matrix = <span class="keyword">new</span> <span class="title class_">int</span>[rows][cols];</span><br><span class="line">        <span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> <span class="number">2</span>; <span class="comment">// 从第三个数开始解析压缩数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; rows; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; cols; j++) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">grayValue</span> <span class="operator">=</span> Integer.parseInt(compressArr[index]); <span class="comment">// 当前灰阶值</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">pixelCount</span> <span class="operator">=</span> Integer.parseInt(compressArr[index + <span class="number">1</span>]); <span class="comment">// 连续像素个数</span></span><br><span class="line">                matrix[i][j] = grayValue; <span class="comment">// 填充灰度图矩阵</span></span><br><span class="line">                index += <span class="number">2</span>; <span class="comment">// 移动到下一组压缩数据</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回指定像素的灰阶值</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">row</span> <span class="operator">=</span> Integer.parseInt(pixelPosition[<span class="number">0</span>]);</span><br><span class="line">        <span class="type">int</span> <span class="variable">col</span> <span class="operator">=</span> Integer.parseInt(pixelPosition[<span class="number">1</span>]);</span><br><span class="line">        <span class="type">int</span> <span class="variable">grayValue</span> <span class="operator">=</span> matrix[row][col];</span><br><span class="line">        System.out.println(grayValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>很显然，他提供的逻辑是错误的，看来GTP3.5的智商还是不够啊，期待GPT7的到来，今晚听360周总说要在2027年，智商高达140，正常人也就100的智商，爱因斯坦160的智商，这样的超级人工智能，福兮祸兮，三年后一见分晓。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;昨天晚上参加了华为OD的机考，记录一下考试流程，顺便做个复盘。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一、华为OD机考流程&quot;&gt;&lt;a href=&quot;#一、华为OD机考流程&quot; class=&quot;headerlink&quot; title=&quot;一、华为OD</summary>
      
    
    
    
    <category term="算法" scheme="https://tuumest.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="算法" scheme="https://tuumest.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>每日分享 Hive</title>
    <link href="https://tuumest.cn/blog/dcd37475.html/"/>
    <id>https://tuumest.cn/blog/dcd37475.html/</id>
    <published>2024-03-05T15:25:34.000Z</published>
    <updated>2025-11-04T14:51:17.148Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Hive是什么？"><a href="#一、Hive是什么？" class="headerlink" title="一、Hive是什么？"></a>一、Hive是什么？</h1><blockquote><p>Hive提供了一种SQL(结构化查询)语言，可以将结构化的文件映射为一张表，查询存储在HDFS上的数据或者其他在HDFS上的文件系统，例如HBase。</p><p>优点：</p><ul><li>操作接口采用类SQL的语法，快速开发</li><li>避免学习MapReduce，减小学习成本</li><li>支持用户自定义函数</li><li>处理大数据便捷</li></ul><p>缺点：</p><ul><li>执行延迟比较高，自动生成的MapReduce作业比较慢</li><li>表达能力有限，体现在迭代式算法无法表达、MapReduce数据处理流程限制，无限实现效率更高的算法</li><li>不支持记录级别的更新、插入、删除操作</li></ul></blockquote><h1 id="二、Hive架构"><a href="#二、Hive架构" class="headerlink" title="二、Hive架构"></a>二、Hive架构</h1><blockquote><p>用户接口（Client）：CLI、JDBC&#x2F;ODBC、WEBUI</p><p>元数据（Meta store）：表名、表所属数据库、表的拥有者、列&#x2F;分区字段、表的类型（内、外部表）、表的数据所在目录等</p><p>驱动器（Driver）</p><ul><li>解析器：将SQL字符串转换成抽象语法树AST，一般都用第三方工具完成，比如antlr</li><li>编译器：将AST编译成逻辑执行计划</li><li>优化器：将逻辑执行计划进行优化</li><li>执行器：将逻辑执行计划转化为物理计划，例如MR&#x2F;SPARK</li></ul></blockquote><h1 id="三、Hive内外部表"><a href="#三、Hive内外部表" class="headerlink" title="三、Hive内外部表"></a>三、Hive内外部表</h1><blockquote><p>内部表、外部表：是否被external修饰</p><p>内部表存储的位置：hive.metastore.warehouse.dir（默认是 &#x2F;user&#x2F;hive&#x2F;warehouse）</p><p>外部表数据存储位置是自己规定的（如果没有location）在HDFS上的&#x2F;user&#x2F;hive&#x2F;warehouse下以外部表的表名创建一个文件夹</p><p>内部表的数据由Hive自身管理</p><p>外部表的数据有HDFS管理</p><p>创建表：</p><p>创建内部表时，数据将移动到数据仓库指向的路径</p><p>创建外部表时，仅记录数据所在路径</p><p>删除表：</p><p>删除内部表时，元数据和数据一起被删除</p><p>删除外部表时，只删除元数据</p><p>外部表的优点：</p><ul><li>外部表不会加载到Hive的默认仓库，减少数据的传输，同时还能和其他外部表共享数据</li><li>使用外部表，hive不会修改源数据，不用担心数据损坏或丢失</li></ul></blockquote><h1 id="四、Hive数据倾斜"><a href="#四、Hive数据倾斜" class="headerlink" title="四、Hive数据倾斜"></a>四、Hive数据倾斜</h1><blockquote><ul><li><p>什么是数据倾斜？</p><p>数据倾斜主要表现在 map&#x2F;reduce 程序执行时，reduce节点大部分执行完毕，但有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长。</p><p>这是因为某一个key的条数比其他key多很多，这条key节点所处理的数据量比其他节点大很多，从而导致某几个节点迟迟运行不完。</p></li><li><p>数据倾斜的原因</p><p>join：其中一个表较小、但key集中，分发到某一个或者几个reduce上的数据远远高于平均值</p><p>大表与大表：，但分桶的判断字段0值、空值过多，这些空值都由一个reduce处理，非常慢</p><p>group by：group by维度过小，某值的数量过多，处理某值的reduce非常耗时</p><p>count distinct，某特殊值过多，处理此特殊值的reduce非常耗时</p></li><li><p>原因：</p><ul><li>key分布不均匀</li><li>业务数据本身特性</li><li>建表是考虑不周</li><li>某些sql本身就有数据倾斜</li></ul></li><li><p>现象：</p><ul><li>任务进度长时间维持在99%，查看任务监控页面，只发现有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。</li><li>单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。最长时长远远大于平均时长。</li></ul></li><li><p>解决方案</p><ul><li><p>参数调节</p><ul><li>hive.map.aggr &#x3D; true</li><li>hive.groupby,skewindata &#x3D; true</li></ul></li><li><p>map端部分聚合，相当于combiner</p></li><li><p>有数据倾斜的时候进行负载均衡，当选型设置为true，生成的查询计划会有两个MR Job。</p><ul><li>第一个mr job中，map的输出结果会随机的分布到reduce中，每个reduce做部分聚合操作，并输出结果。这样处理的结果是相同的group by key有可能被分发到不同的reduce中，从而达到负载均衡的目的。</li><li>第二个mr job再根据预处理的数据结果按照 group by key分布到reduce中（这个过程保证相同的key被分布到同一个reduce中）最后完成最终的聚合操作。</li></ul></li><li><p>sql语句调节</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">如何join：</span><br><span class="line">关于驱动表的选取，选用join key分布最均匀的表最为驱动表，做好列裁剪和filter操作，以达到两表做join的时候，数据量相对较少。</span><br><span class="line">使用map join让小的维度表（1000条以下记录数）先进内存，在map端完成reduce</span><br><span class="line">    </span><br><span class="line">大表join大表：</span><br><span class="line">把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于nul值关联不上，处理后并不影响最终结果。</span><br><span class="line">    </span><br><span class="line">count distinct大量相同特殊值：</span><br><span class="line">count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果+1</span><br><span class="line">    </span><br><span class="line">group by维度过小：</span><br><span class="line">采用sum() group by的方式来替换count（distinct）完成计算</span><br><span class="line">    </span><br><span class="line">特殊情况特殊处理：</span><br><span class="line">业务逻辑优化效果一般的情况下，可以将数据倾斜的数据单独拿出来处理，最后union回去。</span><br></pre></td></tr></table></figure></li></ul></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、Hive是什么？&quot;&gt;&lt;a href=&quot;#一、Hive是什么？&quot; class=&quot;headerlink&quot; title=&quot;一、Hive是什么？&quot;&gt;&lt;/a&gt;一、Hive是什么？&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Hive提供了一种SQL(结构化查询)语言，可以将</summary>
      
    
    
    
    <category term="大数据" scheme="https://tuumest.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="每日分享" scheme="https://tuumest.cn/tags/%E6%AF%8F%E6%97%A5%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>每日分享 Kafka</title>
    <link href="https://tuumest.cn/blog/c8b53445.html/"/>
    <id>https://tuumest.cn/blog/c8b53445.html/</id>
    <published>2024-03-04T14:46:17.000Z</published>
    <updated>2025-11-04T14:51:13.085Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Kafka是什么"><a href="#一、Kafka是什么" class="headerlink" title="一、Kafka是什么?"></a>一、Kafka是什么?</h1><blockquote><p>Kafka 一个分布式的、多分区、多副本、多订阅者，流式数据处理的平台。它具有消息系统（发布、订阅）的能力，也有实时流式数据处理和分析的能力，我们更多偏向于把它当做消息队列系统来使用。</p><ul><li>以时间复杂度O(1)的方式提供持久化能力，即使应对TB以上的数据也能保证常数时间复杂度的访问性能</li><li>高吞吐率，廉价商用机器也能作做到单机每秒100K条以上的消息传输</li><li>支持消息分区、及分布式消费，同时保证每个Partition内的消息顺序传输</li><li>同时支持离线数据处理和实时数据处理</li><li>支持在线水平拓展</li></ul></blockquote><h1 id="二、相关名词解释"><a href="#二、相关名词解释" class="headerlink" title="二、相关名词解释"></a>二、相关名词解释</h1><h2 id="（1）Broker-代理服务器"><a href="#（1）Broker-代理服务器" class="headerlink" title="（1）Broker 代理服务器"></a>（1）Broker 代理服务器</h2><blockquote><p>一个kafka代理服务器也被称之为Broker，它接收生产者发送的消息并存入磁盘。</p><p>Broker同时服务消费者拉取分区消息的请求，返回目前已经提交的消息。</p></blockquote><h2 id="（2）Topic-主题"><a href="#（2）Topic-主题" class="headerlink" title="（2）Topic 主题"></a>（2）Topic 主题</h2><blockquote><p>在kafka中消息以主题（topic）来分类，每一个主题都对应一个消息队列。</p></blockquote><h2 id="（3）Partition-分区"><a href="#（3）Partition-分区" class="headerlink" title="（3）Partition 分区"></a>（3）Partition 分区</h2><blockquote><p>topic物理上的分组，一个topic可以分为多个partition，每一个partition是一个有序的队列。</p></blockquote><h2 id="（4）Segment-段"><a href="#（4）Segment-段" class="headerlink" title="（4）Segment 段"></a>（4）Segment 段</h2><blockquote><p>partition物理上由多个segment组成。</p></blockquote><h2 id="（5）offset-偏移量"><a href="#（5）offset-偏移量" class="headerlink" title="（5）offset 偏移量"></a>（5）offset 偏移量</h2><blockquote><p>每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中每个消息都有一个连续的序号叫做offset，用于partition唯一标记一条信息。</p></blockquote><h2 id="（6）Leader-x2F-Follower-主副本-x2F-从副本"><a href="#（6）Leader-x2F-Follower-主副本-x2F-从副本" class="headerlink" title="（6）Leader&#x2F;Follower 主副本&#x2F;从副本"></a>（6）Leader&#x2F;Follower 主副本&#x2F;从副本</h2><blockquote><p>分区的副本。为了保障数据的高可用，分区都会有一些副本，每个分区都会有一个leader主副本负责读写数据，follower从副本只负责同步leader主副本数据，不对外提供服务。</p></blockquote><h2 id="（7）Consumer-group-消费者组"><a href="#（7）Consumer-group-消费者组" class="headerlink" title="（7）Consumer group 消费者组"></a>（7）Consumer group 消费者组</h2><blockquote><p>消费者组，由多个消费者组成，一个组内只会有一个消费者去消费一个分区的消息。</p></blockquote><h2 id="（8）Coordinator-协调者"><a href="#（8）Coordinator-协调者" class="headerlink" title="（8）Coordinator 协调者"></a>（8）Coordinator 协调者</h2><blockquote><p>协调者，主要是为消费者组分配分区以及重平衡操作（Rebalance）。</p></blockquote><h2 id="（9）Producer-生产者"><a href="#（9）Producer-生产者" class="headerlink" title="（9）Producer 生产者"></a>（9）Producer 生产者</h2><blockquote><p>生产者，负责发送消息</p></blockquote><h2 id="（10）Consumer-消费者"><a href="#（10）Consumer-消费者" class="headerlink" title="（10）Consumer 消费者"></a>（10）Consumer 消费者</h2><blockquote><p>消费者，负责消费消息</p></blockquote><h1 id="三、消息队列模型"><a href="#三、消息队列模型" class="headerlink" title="三、消息队列模型"></a>三、消息队列模型</h1><blockquote><p>对于传统的消息队列系统支持两个模型：点对点、发布订阅；</p><ul><li>点对点：消息只能被一个消费者消费，消费完后就删除</li><li>发布订阅：相当于广播模式，消息可以被所有的消费者消费</li></ul><p>Kafka通过消费者组实现了同时支持上述2个模型，如果说消费者都属于一个消费者组，那么消息只能被一个消费者消费，即点对点的模型；如果每个消费者都是一个单独的消费者组，那么便是发布订阅模型。</p></blockquote><h1 id="四、kafka-通信过程原理"><a href="#四、kafka-通信过程原理" class="headerlink" title="四、kafka 通信过程原理"></a>四、kafka 通信过程原理</h1><blockquote><ol><li>生产者启动的时候会指定bootstrap.servers，通过指定的broker地址，kafka就会和这些broker创建tcp连接。</li><li>连接到任意一台broker之后，然后发送获取元数据请求（有哪些主题、主题有哪些分区、分区有哪些副本、分区的主从副本等信息）</li><li>接着创建和所有的broker的tcp连接</li><li>发送消息</li><li>消费者和生产者一样，也会指定bootstrap.servers属性，然后选择一台broker创建tcp连接，发送请求到找到协调者所在的broker</li><li>再和协调者broker创建tcp连接，获取元数据</li><li>根据分区leader节点所在的broker节点，和这些broker分别创建连接</li><li>消费消息</li></ol></blockquote><p><img src="/blog/c8b53445.html/image-20240304235511766.png" alt="image-20240304235511766"></p><h1 id="五、发送消息时如何选择分区"><a href="#五、发送消息时如何选择分区" class="headerlink" title="五、发送消息时如何选择分区?"></a>五、发送消息时如何选择分区?</h1><blockquote><p>主要有两种方式：</p><ol><li>轮询，安装顺序消息依次发送到不同的分区</li><li>随机，随机发送到某个分区</li></ol><p>如果消息指定key，那么会根据消息的key进行hash，然后对partition数量取模，绝对落在哪个分区上。所以，对于相同key的消息来说，总会发送到一个分区上，也就是我们常常说的消息分区有序性。</p></blockquote><h1 id="六、分区的意义及优势？"><a href="#六、分区的意义及优势？" class="headerlink" title="六、分区的意义及优势？"></a>六、分区的意义及优势？</h1><blockquote><p>若不分区的话，消息只能落在一个节点上，这样就算再好的服务器，性能也是承受不住的。</p><p>实际上，分布式系统都面临这个问题，要么收到消息时，进行消息切分，要么提前切分。kafka选择了前者，通过分区可以把数据均匀的分布到不同节点。</p><p>分区带来负载均衡和横向扩展的能力。</p><p>发送消息时可以根据分区的数量落在不同的kafka服务器节点上，提升了并发写消息的性能，消费消息的时候有和消费者绑定了关系，可以从不同节点的不同分区消费消息，提高的读取消息的能力。</p><p>另外一个就是分区引入了副本，冗余的副本保证了kafka的高可用和高持久性。</p></blockquote><h1 id="七、消费者组和消费者重平衡"><a href="#七、消费者组和消费者重平衡" class="headerlink" title="七、消费者组和消费者重平衡"></a>七、消费者组和消费者重平衡</h1><blockquote><p>kafka中消费者组订阅topic主题的消息，一般来说消费者的数量最好和所有主题分区的数量保持一一致。</p><ul><li>消费者数量&lt;分区数量，必然会有一个消费者消费多个分区</li><li>消费者数量&gt;分区数量，必然有一个消费者没有分区可以消费</li></ul></blockquote><p><img src="/blog/c8b53445.html/image-20240305001717005.png" alt="image-20240305001717005"></p><blockquote><p>消费者消费的分区是怎么分配的，有先加入的消费者时候怎么办？</p><p>由协调器来组织完成，每一次新的消费者加入消费者组时，都会先向协调器发送请求，从而获取分区的分配，这个分区分配的算法逻辑由协调者来完成。</p><p>重平衡就是指有新消费者加入的情况。例如起初我们只有消费者A在消费数据，后来加入了消费者B和C，这时候分区就需要被重新分配了，这就是重平衡，也叫做再平衡，这个期间会导致整个消费者停止工作，重平衡期间都无法消费消息。</p><p>发送重平衡的决定因素：消费者数量、主题数量（用正则订阅的主题）、分区数量，其中任何一个改变都会触发重平衡</p><p>重平衡的过程：</p><p>重平衡的机制依赖于消费者和协调器直接的心跳来维持，消费者会有一个独立的线程会定时去发送心跳给协调者，可以通过heartbeat.interval.ms来控制发送心跳的间隔时间。</p><ol><li>每个消费者第一次加入组的时候都会向协调者发送join group请求，第一个发送请求的消费者会成为“群主”，协调者会返回群成员列表给群主</li><li>群主执行分区分配策略，然后把分配结果通过sync group请求发送给协调者，协调者收到分区分配结果</li><li>其他成员想协调者发送 sync group，协调者把每个消费者的分区分别响应给他们</li></ol></blockquote><p><img src="/blog/c8b53445.html/image-20240305003002497.png" alt="image-20240305003002497"></p><h1 id="八、分区分配策略"><a href="#八、分区分配策略" class="headerlink" title="八、分区分配策略"></a>八、分区分配策略</h1><blockquote><p>主要有3种分配策略：</p><p>range，默认策略，对分区进行排序，越靠前的消费者能够分配到的分区数越多。</p><p><img src="/blog/c8b53445.html/image-20240305003219550.png" alt="image-20240305003219550"></p><p>默认策略的弊端（根据主题进行分配的）在于如果消费者组订阅了多个主题，就可能会导致分区分配不均衡。</p><p><img src="/blog/c8b53445.html/image-20240305003639102.png" alt="image-20240305003639102"></p><p>RoundRobin</p><p>这个就是我们常说的轮询，会根据所有的主题进行轮询分配，不会出现range那种主题越多可能导致分区分配不均衡的问题。</p><p><img src="/blog/c8b53445.html/image-20240305003908189.png" alt="image-20240305003908189"></p><p>Sticky</p><p>粘性策略：在分配均衡的前提下，让分区的分配更小的改动。</p><p>比如P0\P1分配给消费者A，那么下一次尽量还是分配给A。这样做的好处是连接可以复用，要消费消息总是要和broker去连接的，如果能保持上一次分配分区的话，那就不用频繁的销毁创建连接了。</p></blockquote><h1 id="九、如何保证消息可靠？"><a href="#九、如何保证消息可靠？" class="headerlink" title="九、如何保证消息可靠？"></a>九、如何保证消息可靠？</h1><blockquote><p>什么是消息可靠？就是如何确保消息一定能发送到服务器并进行存储，并且发生宕机等异常场景，能够从备份数据中恢复。</p><p>消息的可靠性需要从3方面来保证：</p><ul><li>第一：发送端能否保证发送的消息是可靠的</li><li>第二：kafka broker 自身保证不丢数据，安全落盘</li><li>第三：接收端能够可靠的消费消息</li></ul><p>发送端：通过ack机制，定义不同策略。</p><p>发送端如何保证高可用？源于kafka健壮的副本（replication）策略。通过调节其副本相关参数，可以使得kafka在性能和可靠性之间运转的游刃有余。replication的数量可以在server.properties中配置。</p><p>kafka中的消息是以topic进行分类的，生产者通过topic向kafka broker发送消息，消费者通过topic读取数据。然而topic在物料层面又能以partition为分组，一个topic可以分成若干个partition，kafka中的消息又以顺序的方式存储在文件中。</p><p>kafka中的topic的partition有N个副本（replicas）。N个replicas中，其中一个replicas为leader，其他都是follower，leader处理partition中的读写请求，其余follower定期去复制leader上的数据。</p><p>如果leader发生故障或者挂掉，一个新的leader被选举并接收客户端的消息成功写入。kafka确保从同步副本列表中选举一个副本为leader。</p><p>当生产者向leader发生数据时，可以通过request.required.acks参数来设置可靠性的级别：</p><p>1：默认级别，意味着生产者在ISR中的leader已成功收到数据并确认后发送下一条信息。如果leader宕机了，则会丢失数据。</p><p>0：这个意味着生产者无需等待来着broker的确认而继续发送下一批消息，这种情况下消息的传输效率是最高的，但数据可靠性是最低的。此时retires参数失效，因为客户端无法判断是否失败，也就无法重试。</p><p>all&#x2F;-1：生产者需要等待ISR中所有的follower都确认接收到数据后才算完成一次发送，可靠性最高，但这样也不能保证数据不丢失，比如ISR中只有一个leader时，就会变成acks&#x3D;1的情况。</p><p>retries &#x3D; N，设置一个非常大的值，让生产者发送消息失败后不断重试。</p><p>kafka自身：消息的写入是通过page cache异步写入磁盘的，因此仍然存在丢失消息的可能。针对kafka自身丢消息可能设置的参数：</p><ul><li>replication.factor&#x3D;N，设置一个较大的值，保证至少有2个或以上的副本；</li><li>min.insync.replicas&#x3D;N，代表消息如何才能被认为是写入成功，设置大于1的数，保证至少写一个或者以上的副本才算写入成功</li><li>unclear.leader.election.enable&#x3D;false，这个设置意味着没有完全同步的分区副本不能成为leader副本，如果true的话，那些没有完全同步的副本成为leader副本后，就会有消息丢失的风险。</li></ul><p>接收到：若配置了自动提交，万一消费的数据没有处理完，就自动提交了offset，然后consumer直接宕机了，未处理完的数据丢失了，下次也消费不到了。故而消费端是靠offset来保证的。</p><p>消费者丢失数据，通过关闭自动提交即可，改为业务处理成功后手动提交。</p><p>因为重平衡发送的时候，消费者会去读上一次的偏移量，自动提交默认是5秒一次，这个会导致重复消费或者丢失消息。</p><p>enable.auto.commit&#x3D;false，设置为手动提交。</p><p>auto.offset.reset&#x3D;earliest，这个参数代表没有偏移量可以提交或者broker上不存在偏移量的时候，消费者如何处理。earliest代表从分区的开始位置读取，可能会重复读取消息，但不会丢失。另外一种latest表示从末尾读取，有概率丢失消息。</p></blockquote><h1 id="十、副本同步原理"><a href="#十、副本同步原理" class="headerlink" title="十、副本同步原理"></a>十、副本同步原理</h1><blockquote><p>Kafka的副本分为leader主副本和follower从副本。其中只有leader主副本会对外提供辅助，follower从副本只负责与leader保持数据同步，作为数据冗余容灾的作用。</p><p>在Kafka中所有的副本集合统称为AR（assigned replicas），和leader主副本保持同步的副本集合称之为ISR（InSyncReplicas）</p><p>ISR是一个动态集合，维持这个集合通过replica.lag.time.max.ms参数来控制，这个代表落后leader副本的最长时间，默认为10秒，所以只要follower副本没有落后leader副本10秒以上，就认为是和leader是同步的</p><p>HW（high watermark）：高水位，也叫做复制点，表示副本间同步的位置。</p><p>LEO（log end offset）：下一条待写入消息的位移</p><p>如下图所示，0<del>4绿色表示已经提交的消息，这些消息已经在副本之间进行同步，消费者可以看见这些消息并且进行消费，4</del>6黄色的则是表示未提交的消息，可能还没有在副本间同步，这些消息对于消费者是不可见的。</p><p><img src="/blog/c8b53445.html/640.jpeg" alt="图片"></p><p>副本间同步的过程依赖的就是HW和LEO的更新，以他们的值变化来演示副本同步消息的过程，绿色表示Leader副本，黄色表示Follower副本。</p><p>首先，生产者不停地向Leader写入数据，这时候Leader的LEO可能已经达到了10，但是HW依然是0，两个Follower向Leader请求同步数据，他们的值都是0。</p><p><img src="/blog/c8b53445.html/image-20240305012659056.png" alt="image-20240305012659056"></p><p>然后，消息还在继续写入，Leader的LEO值又发生了变化，两个Follower也各自拉取到了自己的消息，于是更新自己的LEO值，但是这时候Leader的HW依然没有改变。</p><p><img src="/blog/c8b53445.html/image-20240305012712392.png" alt="image-20240305012712392"></p><p>此时，Follower再次向Leader拉取数据，这时候Leader会更新自己的HW值，取Follower中的最小的LEO值来更新。</p><p><img src="/blog/c8b53445.html/image-20240305012730661.png" alt="image-20240305012730661"></p><p>之后，Leader响应自己的HW给Follower，Follower更新自己的HW值，因为又拉取到了消息，所以再次更新LEO，流程以此类推。</p><p><img src="/blog/c8b53445.html/image-20240305012740752.png" alt="image-20240305012740752"></p></blockquote><h1 id="十一、Kafka为什么快？"><a href="#十一、Kafka为什么快？" class="headerlink" title="十一、Kafka为什么快？"></a>十一、Kafka为什么快？</h1><blockquote><ul><li>顺序IO：kafka写消息到分区采用顺序追加的方式，也就是顺序写入磁盘，不是随机写入，这个速度比普通的随机IO快非常多，几乎可以和网络IO相媲美。</li><li>Page Cache 和零拷贝：kafka在写入消息数据的时候通过mmap内存映射的方式，不是真正立刻写入磁盘，而是利用操作系统的文件缓存page cache异步写入，提高写入消息的性能，另外消费消息的时候又通过sendfile实现了零拷贝。</li><li>批处理和压缩：kafka发送消息时，不是一条条的发送的，而是会把多条消息合并为一个批次进行处理发送，消费也是一个道理一次拉取一批次的消息进行消费。并且producer、broker、consumer都使用了优化后的压缩算法，发送和消费消息使用压缩节省了网络传输的开销，broker存储使用压缩降低了磁盘存储空间。</li></ul></blockquote><h1 id="十二、CAP原理"><a href="#十二、CAP原理" class="headerlink" title="十二、CAP原理"></a>十二、CAP原理</h1><blockquote><p>CAP是“一致性（Consistency）、可用性（Availability）以及分区容忍性（Partition Tolerance）”的缩写。</p><p>1）C 即一致性（Consistency）：要求分布式系统要保障，一旦数据写入到分布式存储系统之后，所有访问数据的请求不管是访问分布式存储的那个节点上，查到到该写入的数据都是一致的，不能出现3个副本中有的副本有该条数据，有的副本没有该条数据（插入问题），更不能是有的副本该条数据和另外一个副本该条数据是不一样的（更新问题）。</p><p>2）A 即可用性（Availability）：可用性就是要求分布式系统要保障，一旦数据写入到分布式存储系统之后，所有访问该数据的请求都可以正常响应，不管该数据能不能查到，又或者该条数据查出来的一不一致，不能出现查询该数据时出现长期等待或者报错的发生。</p><p>3）P 即分区容忍性 （Partition Tolerance）：分区容忍性时要求分布式系统要保障，一旦数据写入到分布式存储系统的主本文件后，因为网络的的问题无法同步到副本的时候，系统依然能够对外提供服务，网络在分布式系统来讲是不敢绝对保障的，如果因为网络问题，导致写入数据无法向副本同步，这时候就是分区的情况出现，但网络的绝对的可靠从科学角度上来讲是无法做到的，因此，所有分布式系统必须是满足“P”的存在，不然就只能使用单机系统来解决，那就不是分布式系统了。</p><p>综上所述，分布式系统基本上所有的都必须满足“P”，在“A”和“C”之间来选择，要么是AP，要么是CP。</p><p>CAP原理定义的就是3个原则在分布式存储系统中只能满足其中两个，无法全部都满足，因为要求网络绝对的可靠是不可能的，因此，所有的分布式系统都必须满足P，然后AP和CP之间做出抉择，是保性能牺牲一致（AP），或者是保一致牺牲性能（CP）要根据实际的应用场景来确定。</p><p>Kafka提供了一些配置，用户可以根据具体的业务需求，进行不同的配置，使得Kafka满足AP或者CP，或者它们之间的一种平衡。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">比如下面这种配置，就保证强一致性，使得Kafka满足CP。任意写入一条数据，都需要等到replicate到所有节点之后才返回ack；接下来，在任意节点都可以消费到这条数据，即是在有节点宕机的情况下，包括主节点。</span><br><span class="line"></span><br><span class="line">replication.factor = 3</span><br><span class="line">min.insync.replicas = 3</span><br><span class="line">acks = all</span><br><span class="line"></span><br><span class="line">而下面的配置，就主要保证可用性，使得Kafka满足AP。对于任意写入一条数据，当主节点commmit了之后就返回ack；如果主节点在数据被replicate到从节点之前就宕机，这时，重新选举之后，消费端就读不到这条数据。这种配置，保证了availability，但是损失了consistency。</span><br><span class="line"></span><br><span class="line">replication.factor = 3</span><br><span class="line">min.insync.replicas = 3</span><br><span class="line">acks = 1</span><br><span class="line"></span><br><span class="line">还有一种配置是公认比较推荐的一种配置，基于这种配置，损失了一定的consistency和availability，使得Kafka满足的是一种介于AP和CP之间的一种平衡状态。因为，在这种配置下，可以在容忍一个节点（包括主节点）宕机的情况下，任然保证数据强一致性和整体可用性；但是，有两个节点宕机的情况，就整体不可用了。</span><br><span class="line"></span><br><span class="line">replication.factor = 3</span><br><span class="line">min.insync.replicas = 2</span><br><span class="line">acks = all</span><br></pre></td></tr></table></figure><p>对于这种配置，其实Kafka不光可以容忍一个节点宕机，同时也可以容忍这个节点和其它节点产生网络分区，它们都可以看成是Kafka的容错（Fault tolerance）机制。</p><p>除了上面的几个常用配置项，下面这个配置项也跟consistency和availability相关。这个配置项的作用是控制，在所有节点宕机之后，如果有一个节点之前不是在ISR列表里面，启动起来之后是否可以成为leader。当设置成默认值false时，表示不可以，因为这个节点的数据很可能不是最新的，如果它成为了主节点，那么就可能导致一些数据丢失，从而损失consistency，但是却可以保证availability。如果设置成true，则相反。这个配置项让用户可以基于自己的业务需要，在consistency和availability之间做一个选择。</p><p>unclean.leader.election.enable&#x3D;false</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、Kafka是什么&quot;&gt;&lt;a href=&quot;#一、Kafka是什么&quot; class=&quot;headerlink&quot; title=&quot;一、Kafka是什么?&quot;&gt;&lt;/a&gt;一、Kafka是什么?&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Kafka 一个分布式的、多分区、多副本、多订</summary>
      
    
    
    
    <category term="大数据" scheme="https://tuumest.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="每日分享" scheme="https://tuumest.cn/tags/%E6%AF%8F%E6%97%A5%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>八角笼中</title>
    <link href="https://tuumest.cn/blog/667e5b09.html/"/>
    <id>https://tuumest.cn/blog/667e5b09.html/</id>
    <published>2024-01-03T16:05:10.000Z</published>
    <updated>2025-11-04T15:06:18.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="八角笼中"><a href="#八角笼中" class="headerlink" title="八角笼中"></a><center>八角笼中</center></h1><p><img src="/blog/667e5b09.html/%E5%85%AB%E8%A7%92%E7%AC%BC%E4%B8%AD.jpg" alt="八角笼中"></p><p>​生活是牢笼，也是枷锁，人这一生到底为了什么，其实就是两个字，钱和情，为了这两个字尝尽了人生的酸甜苦辣，复杂的社会，看不透的人心，放不下的责任，走不完的坎坷，越不过的无奈，撑过的日子只有自己知道，该经历的不该经历的全经历了，该忍受的不该忍受的都一一吞下了，还畏惧什么呢？</p><p>​所以，你要相信，任何时候，命运给你一个比别人低的起点，是想告诉你，是让你用你的一生去奋斗出一个绝地反击的故事，破笼而出，博出生路，这个故事关于独立，关于梦想，关于勇气，关于坚忍，他不是一个水到渠成的童话，没有一点点人间疾苦，这个故事是有志者事竟成，破釜沉舟，百二秦关终属楚，这个故事是苦心人天不负，卧薪尝胆，三千越甲可吞吴 。</p><p>​—— 生如野草 不屈不挠</p><p><img src="/blog/667e5b09.html/%E8%8B%8F%E6%9C%A82.jpg" alt="苏木2"></p><p>​生如野草，不屈不挠，没有伞的孩子只有努力奔跑，我们无法选择人生的起点，但可以决定人生的终点。只有登上山顶才能看到那一边的风光，剩下的只管努力与坚持，时间会给我们答案。</p><p>​没有背景，也没有依靠，就是一个平凡而普通的人，想要的都只是踮起脚尖努力争取。虽然生活很难，但不努力，日子更难…</p><p>​     —— 加油，普通人</p><p><img src="/blog/667e5b09.html/%E8%8B%8F%E6%9C%A83.jpg" alt="苏木3"></p><h2 id="经典台词"><a href="#经典台词" class="headerlink" title="经典台词"></a>经典台词</h2><ul><li>我们无法选择自己的起点，但可以决定人生的终点。</li><li>动手之前，先想清楚，不然后果要自己承担。</li><li>选择比努力更重要，你做你擅长的事，我去做我擅长的事，一切都还来得及。</li><li>破笼而出，博出生路。</li><li>人生亦云的有很多，逆流而上的寥寥无几。大浪淘沙，谁主沉浮，自有定律。</li><li>知道打水漂吗？不论你把石头打出去飘多远，这块石头最终都会沉下去。</li><li>生活既是囚笼也是枷锁。放弃就是认命，争取就是垫脚石。</li><li>人们通常都更愿意相信他们想要相信的东西。</li><li>不是所有人都会得到他们想要的，但却有一些人会得到他们应得的。</li><li>青春犹如八角笼中的拳击手，需要勇往直前才能展示自己的价值和力量。</li><li>我姐一辈子就想出那个村子，但她那样子出不去嘞。我就想能拿冠军，能去很远嘞地方，告诉她外面的世界是什么样子。</li><li>做好自己的事情，不要管别人的评价和看法。</li><li>人生不可预知，但我们可以坚定自己的方向。</li><li>格斗就是我们这辈子唯一的出路。</li><li>或许路不相同，可谁又不是在笼子，没有伞的孩子，只能努力奔跑。</li><li>只有登上山顶才能看到那一边的风光，剩下的只管努力与坚持，时间会给我们最后的答案！</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;八角笼中&quot;&gt;&lt;a href=&quot;#八角笼中&quot; class=&quot;headerlink&quot; title=&quot;八角笼中&quot;&gt;&lt;/a&gt;&lt;center&gt;八角笼中&lt;/center&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/667e5b09.html/%E5%85%AB%E8%A7</summary>
      
    
    
    
    <category term="电影" scheme="https://tuumest.cn/categories/%E7%94%B5%E5%BD%B1/"/>
    
    
    <category term="观后感" scheme="https://tuumest.cn/tags/%E8%A7%82%E5%90%8E%E6%84%9F/"/>
    
  </entry>
  
  <entry>
    <title>数仓项目-概念及架构</title>
    <link href="https://tuumest.cn/blog/e250d75f.html/"/>
    <id>https://tuumest.cn/blog/e250d75f.html/</id>
    <published>2024-01-02T16:29:23.000Z</published>
    <updated>2024-01-02T16:32:27.220Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：采集项目-amp-数据仓库项目的区别"><a href="#一：采集项目-amp-数据仓库项目的区别" class="headerlink" title="一：采集项目&amp;数据仓库项目的区别"></a>一：采集项目&amp;数据仓库项目的区别</h1><ol><li><p>从功能角度：</p></li><li><ol><li>采集：以数据为主，传输为主；</li><li>数仓：以数据的计算为主，同时也能存储数据；</li></ol></li><li><p>从技术角度：</p></li><li><ol><li>采集：flume、kafka、datax、maxwell</li><li>数仓：mysql、hdfs、spark、flink、mr、hive</li></ol></li></ol><h1 id="二：数据库和数据仓库的区别"><a href="#二：数据库和数据仓库的区别" class="headerlink" title="二：数据库和数据仓库的区别"></a>二：数据库和数据仓库的区别</h1><ol><li><p>从名称上区分：</p></li><li><ol><li>数据库：database（基础、核心的数据）</li><li>数据仓库：data warehouse（货栈、大商店、小卖店），注重于对外提供服务</li></ol></li><li><p>从数据的来源区分：</p></li><li><ol><li>数据库：企业中基础核心的业务数据</li><li>数据仓库：数据库中的数据</li></ol></li><li><p>从数据存储的角度区分：</p></li><li><ol><li>数据库：核心作用是查找业务数据</li></ol></li><li><ol><li><ol><li>如何存储有利于查询：行式存储（底层使用索引），不能存储海量数据；</li></ol></li></ol></li><li><ol><li>数据仓库：统计分析数据</li></ol></li><li><ol><li><ol><li>如何存储有利于统计、分析：列式存储，可以存储海量数据；</li></ol></li></ol></li><li><p>从数据的价值区分：</p></li><li><ol><li>数据库：保障全企业、全业务的正常运行；</li><li>数据仓库：将数据的统计的结果为企业的经营决策提供数据支撑；</li></ol></li><li><ol><li><ol><li>数据仓库不是数据流转的终点，需要将统计的结果通过可视化呈现；</li></ol></li></ol></li></ol><h1 id="三：数据流转的过程"><a href="#三：数据流转的过程" class="headerlink" title="三：数据流转的过程"></a>三：数据流转的过程</h1><ol><li><ol><li>用户</li><li>业务服务器</li><li>数据存储：行为数据库（文件）</li><li>数据的统计分析：数据仓库</li><li>数据可视化</li></ol></li></ol><h1 id="四：数据统计分析的基本步骤"><a href="#四：数据统计分析的基本步骤" class="headerlink" title="四：数据统计分析的基本步骤"></a>四：数据统计分析的基本步骤</h1><ol><li><p>确定数据源；</p></li><li><p>加工数据；（可以过滤、补全、脱敏等）</p></li><li><p>统计数据；</p></li><li><p>分析数据；</p></li><li><ol><li>spark on hive；（spark 解析 sql）</li><li>hive on spark；（hive 解析 sql）</li></ol></li></ol><h1 id="五：数据仓库-架构"><a href="#五：数据仓库-架构" class="headerlink" title="五：数据仓库-架构"></a>五：数据仓库-架构</h1><p>如果将数据库（MySQL）直接作为数据仓库的数据源，存在的问题：</p><ol><li><p>业务数据库的数据存储为行式存储，而数据仓库的数据要求为列式存储；</p></li><li><ol><li>数据不能直接对接：行式数据转换为列式数据</li></ol></li><li><p>业务数据库中存储的数据不是海量数据，但数据仓库要求为海量数据；</p></li><li><ol><li>数据不能直接对接：数据量不够</li></ol></li><li><p>数据库不是为数据仓库服务的</p></li><li><ol><li>数据仓库在对接数据库数据时，会对数据库的性能造成影响；</li><li>数据仓库应该设计一个自己的数据源；</li></ol></li><li><ol><li><ol><li>同步数据库数据，为了代替和补充数据库；</li><li>汇总数据库数据（海量数据）</li></ol></li></ol></li></ol><h1 id="六：数据采集和数据仓库-架构"><a href="#六：数据采集和数据仓库-架构" class="headerlink" title="六：数据采集和数据仓库-架构"></a>六：数据采集和数据仓库-架构</h1><p>数据仓库中的数据源需要从数据库中周期性（以天为单位）同步；一般情况下，这个同步的过程，称之为“采集”；</p><p>数据采集的时候，如果想要将数据同步到数据仓库的数据源，那么就必须知道业务数据库的表结构；那么采集项目和数据仓库项目就存在耦合性，因此需要解耦合，解耦合的核心就在于增加中间件；数据源为文件或者表，因此最好的中间件就是HDFS；</p><p><img src="/blog/e250d75f.html/1704212740438-0dced7e4-1274-4288-973f-d2c865c38e71.png" alt="img"></p><p><img src="/blog/e250d75f.html/1704212766094-dd64e588-44c5-4965-9d2f-b2678047e7d0.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：采集项目-amp-数据仓库项目的区别&quot;&gt;&lt;a href=&quot;#一：采集项目-amp-数据仓库项目的区别&quot; class=&quot;headerlink&quot; title=&quot;一：采集项目&amp;amp;数据仓库项目的区别&quot;&gt;&lt;/a&gt;一：采集项目&amp;amp;数据仓库项目的区别&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="数仓项目" scheme="https://tuumest.cn/categories/%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="Hadoop" scheme="https://tuumest.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>OpenTSDB原理、存储及查询</title>
    <link href="https://tuumest.cn/blog/3af10b62.html/"/>
    <id>https://tuumest.cn/blog/3af10b62.html/</id>
    <published>2023-12-04T14:19:05.000Z</published>
    <updated>2023-12-04T14:22:33.579Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h1><p>OpenTSDB（Open Time Series Database）是一个用于存储和检索时间序列数据的分布式、可扩展的开源数据库系统。它特别适用于大规模、高性能的监控和分析应用程序，如网络监控、服务器性能监控、传感器数据存储等。</p><h1 id="二：特点"><a href="#二：特点" class="headerlink" title="二：特点"></a>二：特点</h1><ol><li><strong>时间序列数据存储：</strong> OpenTSDB 主要用于存储时间序列数据，这是一种按时间顺序存储的数据，通常表示随时间变化的测量数据，如服务器负载、传感器读数、网络流量等。</li><li><strong>分布式架构：</strong> OpenTSDB 采用分布式架构，可以在多台服务器上存储和查询大量时间序列数据。这使得它适用于大规模的数据集和高负载应用。</li><li><strong>快速插入和查询：</strong> OpenTSDB 针对高性能而设计，可以快速插入和查询时间序列数据。它使用 HBase 作为后端存储引擎，具有高度优化的数据检索机制。</li><li><strong>多维数据模型：</strong> OpenTSDB 具有多维数据模型，允许您为不同的时间序列数据添加标签和标识。这可以帮助您组织和查询数据，以满足各种需求。</li><li><strong>开源：</strong> OpenTSDB 是开源项目，基于 Apache 2.0 许可证，可以免费使用和定制。</li><li><strong>社区支持：</strong> OpenTSDB 拥有活跃的社区支持，这意味着您可以获得各种文档、教程和插件，以满足不同应用程序的需求。</li><li><strong>可扩展性：</strong> OpenTSDB 具有良好的可扩展性，可以轻松地添加新数据源、添加新查询函数和支持更多数据点。</li><li><strong>生态系统集成：</strong> OpenTSDB 可以集成到各种监控和数据分析生态系统中，如 Grafana、Prometheus、Elasticsearch 等。</li></ol><h1 id="三：JSON格式数据的存储"><a href="#三：JSON格式数据的存储" class="headerlink" title="三：JSON格式数据的存储"></a>三：JSON格式数据的存储</h1><p>在OpenTSDB中，使用JSON格式输入数据通常遵循以下原理和实现过程：</p><h2 id="3-1-数据结构定义"><a href="#3-1-数据结构定义" class="headerlink" title="3.1 数据结构定义"></a>3.1 数据结构定义</h2><p>JSON格式数据在OpenTSDB中遵循特定的结构和字段约定。典型的JSON格式数据包括以下关键字段：</p><ul><li><strong>metric</strong>: 表示时间序列数据的名称或指标。</li><li><strong>timestamp</strong>: 表示数据点的时间戳。</li><li><strong>value</strong>: 表示数据点的值。</li><li><strong>tags</strong>: 用于标记和描述数据点的附加信息，通常是键值对的形式，例如设备ID、地理位置等。</li></ul><h2 id="3-2-数据示例"><a href="#3-2-数据示例" class="headerlink" title="3.2 数据示例"></a>3.2 <strong>数据示例</strong></h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;metric&quot;</span><span class="punctuation">:</span> <span class="string">&quot;temperature&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="number">1637016000</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="number">25.5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;tags&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;sensor_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;12345&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;location&quot;</span><span class="punctuation">:</span> <span class="string">&quot;room_1&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>这个示例表示了一个名称为 “temperature” 的时间序列数据点，其时间戳为 1637016000，值为 25.5，同时包含了两个标签：<strong>sensor_id</strong>为 “12345” 和 <strong>location</strong>为 “room_1”。</p><h2 id="3-3-数据导入过程"><a href="#3-3-数据导入过程" class="headerlink" title="3.3 数据导入过程"></a>3.3 <strong>数据导入过程</strong></h2><p>OpenTSDB提供了API或工具，允许用户将符合JSON格式的数据导入到数据库中。用户可以使用HTTP请求或命令行工具等方式将数据发送到OpenTSDB的数据输入端点。</p><ul><li>对于HTTP请求方式，通常是通过POST请求将JSON数据发送到OpenTSDB的API端点。</li><li>命令行工具如 <strong>tsdb-cli</strong> 或其他类似工具也可以被用来从命令行发送JSON格式的数据到OpenTSDB。</li></ul><h2 id="3-4-数据处理和存储"><a href="#3-4-数据处理和存储" class="headerlink" title="3.4 数据处理和存储"></a>3.4 <strong>数据处理和存储</strong></h2><p>OpenTSDB接收到JSON格式的数据后，会解析并根据数据中的时间戳、指标、值以及标签信息将数据存储到适当的位置。OpenTSDB利用其基于HBase的存储引擎来有效地存储和管理时间序列数据。</p><h2 id="3-5-HTTP写入"><a href="#3-5-HTTP写入" class="headerlink" title="3.5 HTTP写入"></a>3.5 HTTP写入</h2><p>使用HTTP的POST请求将JSON数据发送到OpenTSDB的API端点是一种常见的方式，用于将时间序列数据导入到OpenTSDB数据库中。这种方法允许用户通过HTTP协议向OpenTSDB发送数据，并指定数据的指标、时间戳、值以及标签信息。</p><h3 id="步骤概述："><a href="#步骤概述：" class="headerlink" title="步骤概述："></a>步骤概述：</h3><ol><li><strong>构造JSON数据：</strong> 首先，需要构造符合OpenTSDB预期格式的JSON数据。这包括指定指标（metric）、时间戳（timestamp）、值（value），以及标签（tags）等信息。</li><li><strong>发送HTTP POST请求：</strong> 使用任何支持HTTP POST请求的编程语言或工具，将构造好的JSON数据发送到OpenTSDB的API端点。</li><li><strong>处理响应（可选）：</strong> 如果需要，可以处理来自OpenTSDB API的响应，以验证数据是否成功导入或执行其他操作。</li></ol><h3 id="详细步骤："><a href="#详细步骤：" class="headerlink" title="详细步骤："></a>详细步骤：</h3><h4 id="构造JSON数据："><a href="#构造JSON数据：" class="headerlink" title="构造JSON数据："></a>构造JSON数据：</h4><p>构造一个符合OpenTSDB要求的JSON数据对象，确保包含必要的字段如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">jsonCopy code</span><br><span class="line">&#123;</span><br><span class="line">    &quot;metric&quot;: &quot;your_metric_name&quot;,</span><br><span class="line">    &quot;timestamp&quot;: your_timestamp,</span><br><span class="line">    &quot;value&quot;: your_value,</span><br><span class="line">    &quot;tags&quot;: &#123;</span><br><span class="line">        &quot;tag1&quot;: &quot;value1&quot;,</span><br><span class="line">        &quot;tag2&quot;: &quot;value2&quot;,</span><br><span class="line">        // 可选的其他标签</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>“metric”</strong>: 指标名称，表示要存储的时间序列数据类型。</li><li><strong>“timestamp”</strong>: 时间戳，表示数据点的时间。</li><li><strong>“value”</strong>: 数据点的值。</li><li><strong>“tags”</strong>: 附加标签，以键值对的形式提供更多信息。</li></ul><h4 id="发起HTTP请求："><a href="#发起HTTP请求：" class="headerlink" title="发起HTTP请求："></a>发起HTTP请求：</h4><p>当使用Java开发来将JSON数据发送到OpenTSDB的API端点时，可以使用Java的HTTP客户端库，比如Apache HttpClient 或者 Java原生的 <strong>HttpURLConnection</strong> 类。以下是使用Java原生 <strong>HttpURLConnection</strong> 的示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.HttpURLConnection;</span><br><span class="line"><span class="keyword">import</span> java.net.URL;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OpenTSDBDataSender</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 构造JSON数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">json</span> <span class="operator">=</span> <span class="string">&quot;&#123;\&quot;metric\&quot;:\&quot;temperature\&quot;,\&quot;timestamp\&quot;:1637016000,\&quot;value\&quot;:25.5,\&quot;tags\&quot;:&#123;\&quot;sensor_id\&quot;:\&quot;12345\&quot;,\&quot;location\&quot;:\&quot;room_1\&quot;&#125;&#125;&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置OpenTSDB的API端点URL</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">opentsdbURL</span> <span class="operator">=</span> <span class="string">&quot;http://your_opentsdb_instance/api/put&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">HttpURLConnection</span> <span class="variable">connection</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 创建URL对象</span></span><br><span class="line">            <span class="type">URL</span> <span class="variable">url</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">URL</span>(opentsdbURL);</span><br><span class="line">            <span class="comment">// 打开连接</span></span><br><span class="line">            connection = (HttpURLConnection) url.openConnection();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 设置请求方法为POST</span></span><br><span class="line">            connection.setRequestMethod(<span class="string">&quot;POST&quot;</span>);</span><br><span class="line">            <span class="comment">// 设置请求头部信息</span></span><br><span class="line">            connection.setRequestProperty(<span class="string">&quot;Content-Type&quot;</span>, <span class="string">&quot;application/json&quot;</span>);</span><br><span class="line">            connection.setRequestProperty(<span class="string">&quot;Accept&quot;</span>, <span class="string">&quot;application/json&quot;</span>);</span><br><span class="line">            connection.setDoOutput(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取连接的输出流</span></span><br><span class="line">            <span class="keyword">try</span> (<span class="type">OutputStream</span> <span class="variable">outputStream</span> <span class="operator">=</span> connection.getOutputStream()) &#123;</span><br><span class="line">                <span class="type">byte</span>[] input = json.getBytes(StandardCharsets.UTF_8);</span><br><span class="line">                <span class="comment">// 将JSON数据写入输出流</span></span><br><span class="line">                outputStream.write(input);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取响应码</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">responseCode</span> <span class="operator">=</span> connection.getResponseCode();</span><br><span class="line">            <span class="keyword">if</span> (responseCode == HttpURLConnection.HTTP_OK) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Data successfully sent to OpenTSDB.&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Failed to send data. Status code: &quot;</span> + responseCode);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 在finally块中关闭连接</span></span><br><span class="line">            <span class="keyword">if</span> (connection != <span class="literal">null</span>) &#123;</span><br><span class="line">                connection.disconnect();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 <strong>HttpURLConnection</strong> 类建立HTTP连接，并将JSON数据通过POST请求发送到OpenTSDB的API端点。在实际使用中，需要替换 <strong>opentsdbURL</strong> 为你OpenTSDB实例的API端点URL，并根据需要修改JSON数据内容。</p><h1 id="四：数据的查询"><a href="#四：数据的查询" class="headerlink" title="四：数据的查询"></a>四：数据的查询</h1><h2 id="4-1-聚合函数查询"><a href="#4-1-聚合函数查询" class="headerlink" title="4.1 聚合函数查询"></a>4.1 聚合函数查询</h2><p>若你想获取某段时间的某个 metric（指标）的数据，可以使用 OpenTSDB 提供的查询 API。OpenTSDB 提供了强大的查询功能，允许你执行各种类型的查询来获取时间序列数据。通常情况下，你可以使用类似于以下形式的查询来检索所需的数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=&lt;start_timestamp&gt;&amp;end=&lt;end_timestamp&gt;&amp;m=&lt;aggregation_function&gt;:&lt;metric_name&gt;&#123;&lt;tag_key&gt;=&lt;tag_value&gt;&#125;</span><br></pre></td></tr></table></figure><p>解释一下这个查询 URL 的各个部分：</p><ul><li><strong>your_opentsdb_instance</strong>: OpenTSDB 实例的 URL。</li><li><strong>start&#x3D;<start_timestamp></start_timestamp></strong> 和 <strong>end&#x3D;<end_timestamp></end_timestamp></strong>: 表示查询的时间范围，使用时间戳表示起始时间和结束时间。</li><li><strong>m&#x3D;<aggregation_function>:<metric_name>{<tag_key>&#x3D;<tag_value>}</tag_value></tag_key></metric_name></aggregation_function></strong>: 这部分指定了要查询的指标（metric）。**<aggregation_function>** 表示聚合函数（如 sum、avg、max、min 等），**<metric_name>** 是指标的名称，**{<tag_key>&#x3D;<tag_value>}** 可以用于指定查询的标签条件。</tag_value></tag_key></metric_name></aggregation_function></li></ul><p>以下是一个示例查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=1637010000&amp;end=1637020000&amp;m=sum:temperature&#123;location=room_1&#125;</span><br></pre></td></tr></table></figure><p>这个示例查询将在时间戳范围 1637010000 到 1637020000 内，对名称为 <strong>temperature</strong>，标签 <strong>location&#x3D;room_1</strong> 的数据执行求和（sum）的操作。你可以根据需要修改时间范围、指标名称、聚合函数以及标签条件来执行不同的查询操作。</p><p>通过构建类似的查询 URL，并通过 HTTP GET 请求发送到 OpenTSDB 实例的 API 端点，就可以获取所需时间段内的特定 metric 数据。获取到的数据会以 JSON 或其他格式的形式返回，你可以根据返回的数据格式来进一步处理或展示这些数据。</p><h2 id="4-2-全部数据查询"><a href="#4-2-全部数据查询" class="headerlink" title="4.2 全部数据查询"></a>4.2 全部数据查询</h2><p>若你想获取某个时间段内某个指标（metric）的全部数据，可以进行一个简单的查询，而无需指定具体的聚合函数。你可以将查询参数设置为请求该时间段内的所有数据点。以下是一个示例查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=&lt;start_timestamp&gt;&amp;end=&lt;end_timestamp&gt;&amp;m=&lt;metric_name&gt;</span><br></pre></td></tr></table></figure><p>解释这个查询 URL 的各个部分：</p><ul><li><strong>your_opentsdb_instance</strong>: OpenTSDB 实例的 URL。</li><li><strong>start&#x3D;<start_timestamp></start_timestamp></strong> 和 <strong>end&#x3D;<end_timestamp></end_timestamp></strong>: 表示查询的时间范围，使用时间戳表示起始时间和结束时间。</li><li><strong>m&#x3D;<metric_name></metric_name></strong>: 这部分指定了要查询的指标（metric）。**<metric_name>** 是指标的名称，而没有指定聚合函数。</metric_name></li></ul><p>以下是一个示例查询：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code</span><br><span class="line">http://your_opentsdb_instance/api/query?start=1637010000&amp;end=1637020000&amp;m=temperature</span><br></pre></td></tr></table></figure><p>这个示例查询将在时间戳范围 1637010000 到 1637020000 内获取名为 <strong>temperature</strong> 的全部数据。由于没有指定聚合函数，OpenTSDB将返回在该时间范围内的所有数据点，每个数据点对应一个时间戳的值。</p><p>通过构建类似的查询 URL，并通过 HTTP GET 请求发送到 OpenTSDB 实例的 API 端点，就可以获取所需时间段内特定指标的全部数据。根据返回的数据格式，你可以进一步处理或者分析这些数据。</p><p>当使用 Java 来执行一个获取某个时间段内全部数据的查询时，你可以使用类似以下的代码来构建 HTTP GET 请求并发送到 OpenTSDB 的 API 端点：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">javaCopy code</span><br><span class="line">import java.io.BufferedReader;</span><br><span class="line">import java.io.InputStreamReader;</span><br><span class="line">import java.net.HttpURLConnection;</span><br><span class="line">import java.net.URL;</span><br><span class="line">import java.nio.charset.StandardCharsets;</span><br><span class="line"></span><br><span class="line">public class OpenTSDBDataRetriever &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // 设置时间范围和指标名称</span><br><span class="line">        long startTime = 1637010000; // 起始时间戳</span><br><span class="line">        long endTime = 1637020000; // 结束时间戳</span><br><span class="line">        String metric = &quot;temperature&quot;; // 指标名称</span><br><span class="line"></span><br><span class="line">        // 设置OpenTSDB的API端点URL</span><br><span class="line">        String opentsdbURL = &quot;http://your_opentsdb_instance/api/query&quot;;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            // 构建查询URL</span><br><span class="line">            String queryURL = opentsdbURL + &quot;?start=&quot; + startTime + &quot;&amp;end=&quot; + endTime + &quot;&amp;m=&quot; + metric;</span><br><span class="line"></span><br><span class="line">            // 创建URL对象</span><br><span class="line">            URL url = new URL(queryURL);</span><br><span class="line">            // 打开连接</span><br><span class="line">            HttpURLConnection connection = (HttpURLConnection) url.openConnection();</span><br><span class="line"></span><br><span class="line">            // 设置请求方法为GET</span><br><span class="line">            connection.setRequestMethod(&quot;GET&quot;);</span><br><span class="line">            connection.setRequestProperty(&quot;Accept&quot;, &quot;application/json&quot;);</span><br><span class="line"></span><br><span class="line">            // 获取响应</span><br><span class="line">            BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream(), StandardCharsets.UTF_8));</span><br><span class="line">            String inputLine;</span><br><span class="line">            StringBuilder response = new StringBuilder();</span><br><span class="line"></span><br><span class="line">            while ((inputLine = in.readLine()) != null) &#123;</span><br><span class="line">                response.append(inputLine);</span><br><span class="line">            &#125;</span><br><span class="line">            in.close();</span><br><span class="line"></span><br><span class="line">            // 打印查询结果</span><br><span class="line">            System.out.println(&quot;Query Result:&quot;);</span><br><span class="line">            System.out.println(response.toString());</span><br><span class="line"></span><br><span class="line">            // 关闭连接</span><br><span class="line">            connection.disconnect();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>请将 <strong>your_opentsdb_instance</strong> 替换为你的 OpenTSDB 实例的 URL。该代码使用 <strong>HttpURLConnection</strong> 建立 HTTP 连接，构建了一个包含起始时间、结束时间和指标名称的查询 URL。随后，发送 GET 请求到 OpenTSDB 的 API 端点。收到响应后，将查询结果打印输出。根据实际需要，你可以根据返回的数据格式进一步处理或分析这些数据。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h1&gt;&lt;p&gt;OpenTSDB（Open Time Series Database）是一个用于存储和检索时间序列数据的分布式、可扩展的开源</summary>
      
    
    
    
    <category term="大数据" scheme="https://tuumest.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="常用组件" scheme="https://tuumest.cn/tags/%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot 快速上手</title>
    <link href="https://tuumest.cn/blog/e2b59d56.html/"/>
    <id>https://tuumest.cn/blog/e2b59d56.html/</id>
    <published>2023-05-23T15:37:57.000Z</published>
    <updated>2023-05-23T16:53:11.960Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SpringBoot入门程序开发"><a href="#SpringBoot入门程序开发" class="headerlink" title="SpringBoot入门程序开发"></a>SpringBoot入门程序开发</h1><h2 id="Springboot-初衷"><a href="#Springboot-初衷" class="headerlink" title="Springboot 初衷"></a>Springboot 初衷</h2><p>SpringBoot是由Pivotal团队提供的全新框架，其设计目的是用来简化Spring应用的初始搭建以及开发过程。</p><ul><li>SpringBoot程序优点<ul><li>起步依赖（简化依赖配置）</li><li>自动配置（简化常用工程相关配置）</li><li>辅助功能（内置服务器，……）</li></ul></li></ul><h2 id="创建-SpringBoot-工程的四种方式"><a href="#创建-SpringBoot-工程的四种方式" class="headerlink" title="创建 SpringBoot 工程的四种方式"></a>创建 SpringBoot 工程的四种方式</h2><ul><li>基于Idea创建SpringBoot工程</li><li>基于官网创建SpringBoot工程</li><li>基于阿里云创建SpringBoot工程</li><li>手工创建Maven工程修改为SpringBoot工程</li></ul><h3 id="基于Idea创建SpringBoot工程"><a href="#基于Idea创建SpringBoot工程" class="headerlink" title="基于Idea创建SpringBoot工程"></a>基于Idea创建SpringBoot工程</h3><h4 id="①：创建新模块，选择Spring-Initializr，并配置模块相关基础信息"><a href="#①：创建新模块，选择Spring-Initializr，并配置模块相关基础信息" class="headerlink" title="①：创建新模块，选择Spring Initializr，并配置模块相关基础信息"></a>①：创建新模块，选择Spring Initializr，并配置模块相关基础信息</h4><p><img src="/blog/e2b59d56.html/image-20230524001822755.png" alt="image-20230524001822755"></p><h4 id="②：选择当前模块需要使用的技术集"><a href="#②：选择当前模块需要使用的技术集" class="headerlink" title="②：选择当前模块需要使用的技术集"></a>②：选择当前模块需要使用的技术集</h4><p><img src="/blog/e2b59d56.html/image-20230524001512848.png" alt="image-20230524001512848"></p><h4 id="③：开发控制器类"><a href="#③：开发控制器类" class="headerlink" title="③：开发控制器类"></a>③：开发控制器类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.demo.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 0:24 2023/5/24</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping(&quot;/demo&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DemoController</span> &#123;</span><br><span class="line">    <span class="meta">@GetMapping</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">test</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;DemoController is running...&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;success&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="④：运行自动生成的Application类"><a href="#④：运行自动生成的Application类" class="headerlink" title="④：运行自动生成的Application类"></a>④：运行自动生成的Application类</h4><p><img src="/blog/e2b59d56.html/image-20230524003521612.png" alt="image-20230524003521612"></p><h4 id="最简SpringBoot程序所包含的基础文件"><a href="#最简SpringBoot程序所包含的基础文件" class="headerlink" title="最简SpringBoot程序所包含的基础文件"></a>最简SpringBoot程序所包含的基础文件</h4><ul><li>pom.xml文件</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>/&gt;</span> <span class="comment">&lt;!-- lookup parent from repository --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.aiyingke<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>17<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>Application类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SpringbootBaseCreateMethod01Application</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(SpringbootBaseCreateMethod01Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><ol><li>开发SpringBoot程序可以根据向导进行联网快速制作</li><li>SpringBoot程序需要基于JDK进行制作</li><li>SpringBoot程序中需要使用何种功能通过勾选选择技术</li><li>运行SpringBoot程序通过运行Application程序入口进行</li></ol><h3 id="基于官网创建SpringBoot工程"><a href="#基于官网创建SpringBoot工程" class="headerlink" title="基于官网创建SpringBoot工程"></a>基于官网创建SpringBoot工程</h3><p>基于SpringBoot官网创建项目，地址：<a href="https://start.spring.io/">https://start.spring.io/</a></p><p><img src="/blog/e2b59d56.html/image-20230524004616611.png" alt="image-20230524004616611"></p><ol><li>打开SpringBoot官网，选择Quickstart Your Project</li><li>创建工程，并保存项目</li><li>解压项目，通过IDE导入项目</li></ol><h3 id="基于阿里云创建SpringBoot工程"><a href="#基于阿里云创建SpringBoot工程" class="headerlink" title="基于阿里云创建SpringBoot工程"></a>基于阿里云创建SpringBoot工程</h3><p>基于阿里云创建项目，地址：<a href="https://start.aliyun.com/">https://start.aliyun.com</a></p><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul><li>阿里云提供的坐标版本较低，如果需要使用高版本，进入工程后手工切换SpringBoot版本</li><li>阿里云提供的工程模板与Spring官网提供的工程模板略有不同</li></ul><h3 id="手工创建Maven工程修改为SpringBoot工程"><a href="#手工创建Maven工程修改为SpringBoot工程" class="headerlink" title="手工创建Maven工程修改为SpringBoot工程"></a>手工创建Maven工程修改为SpringBoot工程</h3><ul><li>创建普通Maven工程</li><li>继承spring-boot-starter-parent</li><li>添加依赖spring-boot-starter-web</li><li>制作引导类Application</li></ul><h4 id="手工创建项目（手工导入坐标）"><a href="#手工创建项目（手工导入坐标）" class="headerlink" title="手工创建项目（手工导入坐标）"></a>手工创建项目（手工导入坐标）</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relativePath</span>/&gt;</span> <span class="comment">&lt;!-- lookup parent from repository --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.aiyingke<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>springboot-base-create-method-01<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>17<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="手工创建项目（手工制作引导类）"><a href="#手工创建项目（手工制作引导类）" class="headerlink" title="手工创建项目（手工制作引导类）"></a>手工创建项目（手工制作引导类）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SpringbootBaseCreateMethod01Application</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(SpringbootBaseCreateMethod01Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SpringBoot入门程序开发&quot;&gt;&lt;a href=&quot;#SpringBoot入门程序开发&quot; class=&quot;headerlink&quot; title=&quot;SpringBoot入门程序开发&quot;&gt;&lt;/a&gt;SpringBoot入门程序开发&lt;/h1&gt;&lt;h2 id=&quot;Springboo</summary>
      
    
    
    
    <category term="SpringBoot" scheme="https://tuumest.cn/categories/SpringBoot/"/>
    
    
    <category term="SpringBoot" scheme="https://tuumest.cn/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 概述</title>
    <link href="https://tuumest.cn/blog/daa0264.html/"/>
    <id>https://tuumest.cn/blog/daa0264.html/</id>
    <published>2023-04-02T23:42:45.000Z</published>
    <updated>2023-04-04T00:05:21.162Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：定义"><a href="#一：定义" class="headerlink" title="一：定义"></a>一：定义</h1><p>Kafka 传统定义：Kafka 是一个分布式的基于发布&#x2F;订阅模式的消息队列，主要应用于大数据的实时处理场景。<br>发布&#x2F;订阅：消息的发布者不会将消息直接发送给消息的订阅者，而是将发送的消息分为不同的类别，订阅者只接受感兴趣的消息。<br>Kafka 愿景定义：Kafka 是一个开源的分布式事件流平台，被多数公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><h1 id="二：消息队列"><a href="#二：消息队列" class="headerlink" title="二：消息队列"></a>二：消息队列</h1><p>在大数据领域通常采用 Kafka 作为消息队列。</p><h2 id="2-1-传统消息队列的应用场景"><a href="#2-1-传统消息队列的应用场景" class="headerlink" title="2.1 传统消息队列的应用场景"></a>2.1 传统消息队列的应用场景</h2><p>传统的消息队列主要应用于：缓存&#x2F;消峰、解耦和异步通信。</p><h3 id="缓存-x2F-消峰："><a href="#缓存-x2F-消峰：" class="headerlink" title="缓存&#x2F;消峰："></a>缓存&#x2F;消峰：</h3><p>有助于控制和优化数据流系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p><p><img src="/blog/daa0264.html/image-20230404080211198.png" alt="image.png"></p><h3 id="解耦："><a href="#解耦：" class="headerlink" title="解耦："></a>解耦：</h3><p>允许独立的扩展或者修改两边的处理过程，只要确保他们遵循同样的数据接口约束。</p><p><img src="/blog/daa0264.html/image-20230404080231754.png" alt="image.png"></p><h3 id="异步通信："><a href="#异步通信：" class="headerlink" title="异步通信："></a>异步通信：</h3><p>允许用户把消息放入队列中，但不立即处理它，然后在需要的时候再处理它们。</p><p><img src="/blog/daa0264.html/image-20230404080240773.png" alt="image.png"></p><h2 id="2-2-消息队列的两种模式"><a href="#2-2-消息队列的两种模式" class="headerlink" title="2.2 消息队列的两种模式"></a>2.2 消息队列的两种模式</h2><h3 id="点对点模式："><a href="#点对点模式：" class="headerlink" title="点对点模式："></a>点对点模式：</h3><p>消费者主动拉去消息，收到消息后清除消息。</p><p><img src="/blog/daa0264.html/image-20230404080249793.png" alt="image.png"></p><h3 id="发布-x2F-订阅模式："><a href="#发布-x2F-订阅模式：" class="headerlink" title="发布&#x2F;订阅模式："></a>发布&#x2F;订阅模式：</h3><ul><li>可以有多个topic主题；</li><li>消费者消费数据后，不删除数据；</li><li>每个消费者相互独立，都可以消费到数据。</li></ul><p><img src="/blog/daa0264.html/image-20230404080303157.png"></p><h1 id="三：Kafka-基础架构"><a href="#三：Kafka-基础架构" class="headerlink" title="三：Kafka 基础架构"></a>三：Kafka 基础架构</h1><p><img src="/blog/daa0264.html/image-20230404080309991.png" alt="image-20230404080309991"><br>（1）Producer：消息生产者，向 Kafka broker 发消息的客户端。<br>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。<br>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。<br>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic。<br>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic。<br>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker （服务器）上，一个 topic 可以分为多个 partition，每个 parition 是一个有序的队列。<br>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个 Follower。<br>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。<br>（9)Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和 Leader 数据同步。Leader 发生故障时，某一个 Follower 会成为新的 Leader。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：定义&quot;&gt;&lt;a href=&quot;#一：定义&quot; class=&quot;headerlink&quot; title=&quot;一：定义&quot;&gt;&lt;/a&gt;一：定义&lt;/h1&gt;&lt;p&gt;Kafka 传统定义：Kafka 是一个分布式的基于发布&amp;#x2F;订阅模式的消息队列，主要应用于大数据的实时处理场景。&lt;b</summary>
      
    
    
    
    <category term="Kafka" scheme="https://tuumest.cn/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="https://tuumest.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——Shuffle调优</title>
    <link href="https://tuumest.cn/blog/60787c8.html/"/>
    <id>https://tuumest.cn/blog/60787c8.html/</id>
    <published>2023-02-22T03:26:46.000Z</published>
    <updated>2023-02-22T03:29:40.474Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：ShuffleManager发展概述"><a href="#一：ShuffleManager发展概述" class="headerlink" title="一：ShuffleManager发展概述"></a>一：ShuffleManager发展概述</h1><p>在Spark的源码中，<strong>负责shuffle过程的执行、计算和处理的组件</strong>主要就是<strong>ShuffleManager</strong>，也即<strong>shuffle管理器</strong>。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会<strong>产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能</strong>。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了<strong>SortShuffleManager</strong>。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h1 id="二：HashShuffleManager运行原理"><a href="#二：HashShuffleManager运行原理" class="headerlink" title="二：HashShuffleManager运行原理"></a>二：HashShuffleManager运行原理</h1><h2 id="2-1-未经优化的HashShuffleManager"><a href="#2-1-未经优化的HashShuffleManager" class="headerlink" title="2.1 未经优化的HashShuffleManager"></a>2.1 未经优化的HashShuffleManager</h2><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p><img src="/blog/60787c8.html/1648882910142-f0b6c560-beac-475f-ae4c-669711da3b4b.png" alt="img"></p><h2 id="2-2-优化后的HashShuffleManager"><a href="#2-2-优化后的HashShuffleManager" class="headerlink" title="2.2 优化后的HashShuffleManager"></a>2.2 优化后的HashShuffleManager</h2><p>下图说明了优化后的HashShuffleManager的原理。</p><p><img src="/blog/60787c8.html/1648883523295-7bee2767-fd92-412e-94eb-c80712518570.png" alt="img"></p><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><h2 id="2-3-普通运行机制"><a href="#2-3-普通运行机制" class="headerlink" title="2.3 普通运行机制"></a>2.3 普通运行机制</h2><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><img src="/blog/60787c8.html/1648884121250-0f0397cf-5c7d-4a6e-8249-453ba11b395b.png" alt="img"></p><h2 id="2-4-bypass运行机制"><a href="#2-4-bypass运行机制" class="headerlink" title="2.4 bypass运行机制"></a>2.4 bypass运行机制</h2><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><p> * shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 </p><p>* 不是聚合类的shuffle算子（比如reduceByKey）。</p><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，<strong>启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</strong></p><p><img src="/blog/60787c8.html/1648884245925-5db22e38-be48-42a6-8668-3dbdac9cbf1c.png" alt="img"></p><h1 id="三：shuffle相关参数调优"><a href="#三：shuffle相关参数调优" class="headerlink" title="三：shuffle相关参数调优"></a>三：shuffle相关参数调优</h1><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><h2 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h2><ul><li>默认值：32k</li><li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会<strong>先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘</strong>。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h2 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h2><ul><li>默认值：48m</li><li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了<strong>每次能够拉取多少数据</strong>。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h2 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h2><ul><li>默认值：3</li><li>参数说明：shuffle read task从shuffle write task所在节点<strong>拉取属于自己的数据</strong>时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了<strong>可以重试的最大次数</strong>。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul><h2 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h2><ul><li>默认值：5s</li><li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议：建议<strong>加大间隔时长</strong>（比如60s），以<strong>增加shuffle操作的稳定性</strong>。</li></ul><h2 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h2><ul><li>默认值：0.2</li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很<strong>少使用持久化操作，建议调高这个比例</strong>，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li></ul><h2 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h2><ul><li>默认值：sort</li><li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中<strong>需要该排序机制的话，则使用默认的SortShuffleManager就可以</strong>；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h2 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h2><ul><li>默认值：200</li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h2 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h2><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><h2 id="spark-shuffle-consolidateFiles-1"><a href="#spark-shuffle-consolidateFiles-1" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h2><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><p>开发过程中的优化原则、运行前的资源参数设置调优、运行中的数据倾斜的解决方案、为了精益求精的shuffle调优。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：ShuffleManager发展概述&quot;&gt;&lt;a href=&quot;#一：ShuffleManager发展概述&quot; class=&quot;headerlink&quot; title=&quot;一：ShuffleManager发展概述&quot;&gt;&lt;/a&gt;一：ShuffleManager发展概述&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——数据倾斜调优</title>
    <link href="https://tuumest.cn/blog/ed3d1f39.html/"/>
    <id>https://tuumest.cn/blog/ed3d1f39.html/</id>
    <published>2023-02-22T03:22:33.000Z</published>
    <updated>2023-02-22T03:25:56.389Z</updated>
    
    <content type="html"><![CDATA[<p>数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h1 id="一：数据倾斜发生时的现象"><a href="#一：数据倾斜发生时的现象" class="headerlink" title="一：数据倾斜发生时的现象"></a>一：数据倾斜发生时的现象</h1><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，<strong>总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时</strong>。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><p><img src="/blog/ed3d1f39.html/1648867372204-756cdbdf-1f58-4759-a93b-74b7ea248566.png" alt="img"></p><h1 id="二：数据倾斜发生的原理"><a href="#二：数据倾斜发生的原理" class="headerlink" title="二：数据倾斜发生的原理"></a>二：数据倾斜发生的原理</h1><p>数据倾斜的原理很简单：在<strong>进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理</strong>，比如<strong>按照key进行聚合或join等操作</strong>。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是<strong>个别task可能分配到了100万数据(巨量数据)<strong>，要运行一两个小时。因此，整个</strong>Spark作业的运行进度是由运行时间最长的那个task决定的</strong>。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><h1 id="三：实例-数据倾斜"><a href="#三：实例-数据倾斜" class="headerlink" title="三：实例 数据倾斜"></a>三：实例 数据倾斜</h1><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="/blog/ed3d1f39.html/1648867732857-4e4706a4-2f59-4470-a4a8-3fb9fdfd00d2.png" alt="img"></p><h1 id="四：如何定位导致数据倾斜的代码"><a href="#四：如何定位导致数据倾斜的代码" class="headerlink" title="四：如何定位导致数据倾斜的代码"></a>四：如何定位导致数据倾斜的代码</h1><p><strong>数据倾斜只会发生在shuffle过程中</strong>。这里给大家罗列一些常用的并且可能<strong>会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition</strong>等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h2 id="4-1-某个task执行特别慢的情况"><a href="#4-1-某个task执行特别慢的情况" class="headerlink" title="4.1 某个task执行特别慢的情况"></a>4.1 某个task执行特别慢的情况</h2><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以<strong>在Spark Web UI上深入看一下当前这个stage各个task分配的数据量</strong>，从而进一步<strong>确定是不是task分配的数据不均匀导致了数据倾斜</strong>。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而<strong>运行时间特别长的task需要处理几千KB的数据</strong>，<strong>处理的数据量差了10倍</strong>。此时更加能够确定是发生了数据倾斜。</p><p><img src="/blog/ed3d1f39.html/1648870600764-44baba48-3bd0-4f0e-9c4c-06c289041072.png" alt="img"></p><p>知道<strong>数据倾斜发生在哪一个stage之后</strong>，接着我们就需要根据stage划分原理，<strong>推算出来发生倾斜的那个stage对应代码中的哪一部分</strong>，这部分代码中肯定会有一个<strong>shuffle类算子</strong>。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><h2 id="4-2-代码实例"><a href="#4-2-代码实例" class="headerlink" title="4.2 代码实例"></a>4.2 代码实例</h2><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 * stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 * stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;hdfs://...&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h2 id="4-3-某个task莫名其妙内存溢出的情况"><a href="#4-3-某个task莫名其妙内存溢出的情况" class="headerlink" title="4.3 某个task莫名其妙内存溢出的情况"></a>4.3 某个task莫名其妙内存溢出的情况</h2><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="4-4-查看导致数据倾斜的key的数据分布情况"><a href="#4-4-查看导致数据倾斜的key的数据分布情况" class="headerlink" title="4.4 查看导致数据倾斜的key的数据分布情况"></a>4.4 查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD&#x2F;Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 1. 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 2. 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect&#x2F;take到客户端打印一下，就可以看到key的分布情况。</p><h2 id="4-5-key分布示例"><a href="#4-5-key分布示例" class="headerlink" title="4.5 key分布示例"></a>4.5 key分布示例</h2><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h1 id="五：数据倾斜的解决方案"><a href="#五：数据倾斜的解决方案" class="headerlink" title="五：数据倾斜的解决方案"></a>五：数据倾斜的解决方案</h1><h2 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h2><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把<strong>数据倾斜的发生提前到了Hive ETL中</strong>，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h2 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h2><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取<strong>每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</strong></p><h2 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h2><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><img src="/blog/ed3d1f39.html/1648879873365-498c4baf-0c86-4637-b1a1-ca41e41f181e.png" alt="img"></p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><h2 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h2><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="/blog/ed3d1f39.html/1648880080869-a084c2f2-4084-411b-8401-1e4252766ddc.png" alt="img"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Long&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;String, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">long</span> <span class="variable">originalKey</span> <span class="operator">=</span> Long.valueOf(tuple._1.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title></a></h3><h2 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h2><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><img src="/blog/ed3d1f39.html/1648880677360-20f904a3-03f6-44f4-bef4-01194d0e20f6.png" alt="img"></p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Long, Row&gt;();</span><br><span class="line">                <span class="keyword">for</span>(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> tuple._1;</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                <span class="type">Row</span> <span class="variable">rdd1Value</span> <span class="operator">=</span> rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(key, <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure><h2 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h2><p><strong>方案适用场景：</strong>两个RDD&#x2F;Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD&#x2F;Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD&#x2F;Hive表中的少数几个key的数据量过大，而另一个RDD&#x2F;Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong> * 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 * 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 * 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 * 而另外两个普通的RDD就照常join即可。 * 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="/blog/ed3d1f39.html/1648881217470-bd1c1dc7-5fd4-4b13-8b3a-e9635a821029.png" alt="img"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(tuple._1, <span class="number">1L</span>);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> <span class="variable">skewedUserid</span> <span class="operator">=</span> reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> <span class="title class_">PairFlatMapFunction</span>&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                    Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(i + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">              </span><br><span class="line">        &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="keyword">public</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span></span><br><span class="line">                            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                            <span class="type">long</span> <span class="variable">key</span> <span class="operator">=</span> Long.valueOf(tuple._1.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h2 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h2><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong> * 该方案的实现思路基本和“解决方案六”类似，首先查看RDD&#x2F;Hive表中的数据分布情况，找到那个造成数据倾斜的RDD&#x2F;Hive表，比如有多个key都对应了超过1万条数据。 * 然后将该RDD的每条数据都打上一个n以内的随机前缀。 * 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 * 最后将两个处理后的RDD进行join即可。</p><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFlatMapFunction</span>&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(<span class="number">0</span> + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h2 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h2><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，<strong>预处理</strong>一部分数据，并<strong>过滤</strong>一部分数据来缓解；其次可以对某些shuffle操作<strong>提升并行度</strong>，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。&lt;/p&gt;
&lt;h1 id=&quot;一：数据倾斜发生时的现象&quot;&gt;&lt;a href=&quot;#一：数据倾斜发生时的现象&quot; class=&quot;headerlink&quot; title=&quot;一：数据倾斜发生时的现象&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——资源调优</title>
    <link href="https://tuumest.cn/blog/5f7211d0.html/"/>
    <id>https://tuumest.cn/blog/5f7211d0.html/</id>
    <published>2023-02-22T03:18:31.000Z</published>
    <updated>2023-02-22T03:21:13.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="/blog/5f7211d0.html/1648805615385-560485a2-b665-4472-a0b0-41dfc9b6a557.png" alt="img"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了cache&#x2F;persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置<strong>50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；<strong>设置的太多的话，大部分队列可能无法给予充分的资源</strong>。</li></ul><h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul><li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：<strong>每个Executor进程的内存设置4G~8G较为合适</strong>。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己<strong>团队的资源队列的最大内存限制是多少</strong>，<strong>num-executors乘以executor-memory，是不能超过队列的最大内存量的。</strong>此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列<strong>最大总内存的1&#x2F;3~1&#x2F;2</strong>，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul><li>参数说明：该参数用于设置每个<strong>Executor进程的CPU core数量</strong>。这个参数决定了<strong>每个Executor进程并行执行task线程的能力</strong>。因为<strong>每个CPU core同一时间只能执行一个task线程</strong>，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：<strong>Executor的CPU core数量设置为2~4个较为合适</strong>。同样得根据不同部门的资源队列来定，可以看看<strong>自己的资源队列的最大CPU core限制是多少</strong>，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么<strong>num-executors * executor-cores不要超过队列总CPU core的1&#x2F;3~1&#x2F;2左右比较合适</strong>，也是避免影响其他同学的作业运行。</li></ul><h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置<strong>1G左右应该就够了</strong>。唯一需要注意的一点是，如果<strong>需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大</strong>，否则会出现OOM内存溢出的问题。</li></ul><h3 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h3><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果<strong>不设置可能会直接影响你的Spark作业性能</strong>。</li><li>参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。</strong>很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），<strong>如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃</strong>。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，<strong>那么90%的Executor进程可能根本就没有task执行</strong>，<strong>也就是白白浪费了资源</strong>！因此Spark官网建议的设置原则是，设置该参数为<strong>num-executors * executor-cores的2~3倍较为合适</strong>，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h3><ul><li>参数说明：该参数用于设置<strong>RDD持久化数据</strong>在Executor内存中<strong>能占的比例</strong>，默认是<strong>0.6</strong>。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，<strong>有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中</strong>。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是<strong>如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适</strong>。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，<strong>默认是0.2</strong>。也就是说，<strong>Executor默认只有20%的内存用来进行该操作</strong>。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：<strong>如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。</strong>此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--num-executors 100 \</span><br><span class="line">--executor-memory 6G \</span><br><span class="line">--executor-cores 4 \</span><br><span class="line">--driver-memory 1G \</span><br><span class="line">--conf spark.default.parallelism=1000 \</span><br><span class="line">--conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">--conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;调优概述&quot;&gt;&lt;a href=&quot;#调优概述&quot; class=&quot;headerlink&quot; title=&quot;调优概述&quot;&gt;&lt;/a&gt;调优概述&lt;/h2&gt;&lt;p&gt;在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——开发调优</title>
    <link href="https://tuumest.cn/blog/b222527c.html/"/>
    <id>https://tuumest.cn/blog/b222527c.html/</id>
    <published>2023-02-22T02:55:14.000Z</published>
    <updated>2023-02-22T03:19:43.806Z</updated>
    
    <content type="html"><![CDATA[<p>大多数spark作业的性能主要就是消耗了shuffle过程，因为该环节包含了<strong>大量的磁盘IO、序列化、网络数据传输</strong>等操作。</p><p>影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。</p><p>开发调优、资源调优、数据倾斜调优、shuffle调优；</p><p>Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p><strong>对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。****对多次使用的RDD进行持久化</strong></p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span><br><span class="line"></span><br><span class="line">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span><br><span class="line">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span><br><span class="line">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span><br><span class="line">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">val rdd2 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span><br><span class="line">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span><br><span class="line">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span><br><span class="line">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span><br><span class="line">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p><img src="/blog/b222527c.html/1648795120926-e6c6fea0-1a01-4481-8ffc-3bdf0551b1ad.png" alt="img"></p><p>如果要求持久化数据可能丢失的情况下，还是要保证高性能，那么就在第一次计算RDD 时，消耗一些性能，对 RDD 进行 checkpoint 操作。这样，即使持久化数据丢失了，也可以直接读取其 checkpoint 数据。</p><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// 错误的做法。</span><br><span class="line"></span><br><span class="line">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span><br><span class="line">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class="line">JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line">// 分别对rdd1和rdd2执行了不同的算子操作。</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line">// 正确的做法。</span><br><span class="line"></span><br><span class="line">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span><br><span class="line">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span><br><span class="line"></span><br><span class="line">// 其实在这种情况下完全可以复用同一个RDD。</span><br><span class="line">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span><br><span class="line">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span><br><span class="line">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span><br><span class="line">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><p><strong>对多次使用的RDD进行持久化的代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://192.168.0.1:9000/hello.txt&quot;</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://192.168.0.1:9000/hello.txt&quot;</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p><strong>Spark的持久化级别</strong></p><table><thead><tr><th><strong>持久化级别</strong></th><th><strong>含义解释</strong></th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><p>持久化：写入磁盘中</p><p>序列化：将RDD的转换为字节数组</p><p><strong>如何选择一种最合适的持久化策略</strong></p><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是<strong>将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作</strong>。比如<strong>reduceByKey</strong>、<strong>join</strong>等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会<strong>发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作</strong>。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，<strong>能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。</strong>这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><p><strong>Broadcast与map进行join代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>spark.shuffle.consolidateFiles</p><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><strong>第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；</strong></p><p><img src="/blog/b222527c.html/1648804303841-ea514c86-2c77-4231-92c6-c634540d78ac.png" alt="img"></p><p><strong>第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</strong></p><p>第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="/blog/b222527c.html/1648804507865-7f2d5a07-bfd6-4943-b1f1-beb5f9c11542.png" alt="img"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><p><strong>使用reduceByKey&#x2F;aggregateByKey替代groupByKey</strong></p><p><strong>使用mapPartitions替代普通map</strong></p><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><p><strong>使用foreachPartitions替代foreach</strong></p><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><p><strong>使用filter之后进行coalesce操作</strong></p><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><p><strong>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</strong></p><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><p><strong>广播大变量的代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化： </p><p>* 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 </p><p>* 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 </p><p>* 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</p><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream&#x2F;ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，<strong>性能高10倍左右</strong>。Spark之所以默认没有使用Kryo作为序列化类库，是因为<strong>Kryo要求最好要注册所有需要进行序列化的自定义类型</strong>，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存： </p><p>* 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 </p><p>* 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 </p><p>* 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</p><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大多数spark作业的性能主要就是消耗了shuffle过程，因为该环节包含了&lt;strong&gt;大量的磁盘IO、序列化、网络数据传输&lt;/strong&gt;等操作。&lt;/p&gt;
&lt;p&gt;影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个S</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 常用算子</title>
    <link href="https://tuumest.cn/blog/bda70396.html/"/>
    <id>https://tuumest.cn/blog/bda70396.html/</id>
    <published>2023-02-20T08:43:05.000Z</published>
    <updated>2023-02-21T04:10:04.901Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：算子概述"><a href="#一：算子概述" class="headerlink" title="一：算子概述"></a>一：算子概述</h1><h2 id="1-1-什么是算子？"><a href="#1-1-什么是算子？" class="headerlink" title="1.1 什么是算子？"></a>1.1 什么是算子？</h2><ul><li><p>英文：Operator</p></li><li><p>狭义：一个函数空间到另一个函数空间的映射</p></li><li><p>广义：一个空间到另个一个空间的映射</p></li><li><p>白话：一个事物从一个状态到另一个状态的过程</p></li><li><p>实质：映射，即关系</p></li></ul><h2 id="1-2-算子的重要作用"><a href="#1-2-算子的重要作用" class="headerlink" title="1.2 算子的重要作用"></a>1.2 算子的重要作用</h2><ul><li>算子越多，灵活性越高，编程的可选方式就越多</li><li>算子越多，表现能力强，可以灵活应对各种复杂场景</li></ul><h2 id="1-3-MapReduce-和-Spark-算子比较"><a href="#1-3-MapReduce-和-Spark-算子比较" class="headerlink" title="1.3 MapReduce 和 Spark 算子比较"></a>1.3 MapReduce 和 Spark 算子比较</h2><ul><li>MapReduce 只有2个算子，map和reduce，绝大多数场景下，需要复杂的编程来完成业务需求</li><li>Spark 有80多个算子，可以灵活组合应对不同的业务场景</li></ul><h1 id="二：Spark算子"><a href="#二：Spark算子" class="headerlink" title="二：Spark算子"></a>二：Spark算子</h1><h2 id="2-1-转换算子（transformation）"><a href="#2-1-转换算子（transformation）" class="headerlink" title="2.1 转换算子（transformation）"></a>2.1 转换算子（transformation）</h2><p>此种算子不会真正的触发提交作业，只有作业被提交后才会触发转换计算</p><ul><li>value型转换算子（处理的数据项是value型）<ul><li>输入分区：输出分区 &#x3D; 1 ： 1<ul><li>map算子</li><li>flatMap算子</li><li>mapPartitions算子</li></ul></li><li>输入分区：输出分区 &#x3D; n ： 1<ul><li>union算子</li><li>cartesian算子</li></ul></li><li>输入分区 ：输出分区 &#x3D; n ： n<ul><li>groupBy算子</li></ul></li><li>输出分区为输入分区的子集<ul><li>filter算子</li><li>distinct算子</li><li>substract算子</li><li>sample算子</li><li>takeSample算子</li></ul></li><li>cache型算子<ul><li>cache算子</li><li>persist算子</li></ul></li></ul></li><li>key-value型转换算子（处理的数据类型是key-value型）<ul><li>输入分区：输出分区 &#x3D; 1： 1<ul><li>mapValues 算子</li></ul></li><li>对单个RDD聚集<ul><li>combineByKey算子</li><li>reduceByKey算子</li><li>partitionBy算子</li></ul></li><li>对两个RDD聚合<ul><li>cogroup算子</li></ul></li><li>连接<ul><li>join算子</li><li>leftOutJoin算子</li><li>rightOutJoin算子</li></ul></li></ul></li></ul><h2 id="2-2-行动算子（action）"><a href="#2-2-行动算子（action）" class="headerlink" title="2.2 行动算子（action）"></a>2.2 行动算子（action）</h2><p>这种算子会触发sparkContent提交作业；</p><ul><li>无输出（不生成文件）<ul><li>foreach算子</li></ul></li><li>HDFS<ul><li>saveAsTextFile算子</li><li>saveAsObjectFile算子</li></ul></li><li>scala集合和数据类型<ul><li>collect算子</li><li>collectAsMap算子</li><li>reduceByKeyLocally算子</li><li>lookup算子</li><li>count算子</li><li>top算子</li><li>reduce算子</li><li>fold算子</li><li>aggregate算子</li></ul></li></ul><h1 id="三：常见算子的应用场景"><a href="#三：常见算子的应用场景" class="headerlink" title="三：常见算子的应用场景"></a>三：常见算子的应用场景</h1><h2 id="3-1-转换算子（transformation）"><a href="#3-1-转换算子（transformation）" class="headerlink" title="3.1 转换算子（transformation）"></a>3.1 转换算子（transformation）</h2><h3 id="3-1-1-value型转换算子"><a href="#3-1-1-value型转换算子" class="headerlink" title="3.1.1 value型转换算子"></a>3.1.1 value型转换算子</h3><h4 id="（1）map"><a href="#（1）map" class="headerlink" title="（1）map"></a>（1）map</h4><p>类比mapreduce中的map操作，给定一个输入，由map函数操作后，成为一个新的元素输出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;Hello&quot;,&quot;Word&quot;,&quot;你好&quot;,&quot;世界&quot;),2)</span><br><span class="line">val second = first.map(_.length)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361528641-4cccbb47-74f0-4033-a43f-a80da85ecadd.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.map(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361687877-18444583-2032-4606-b1c2-59d7a858da27.png"></p><h4 id="（2）flatMap"><a href="#（2）flatMap" class="headerlink" title="（2）flatMap"></a>（2）flatMap</h4><p>给定一个二维的输入（线式输入），将返回的所有结果打平成一个一维的集合结构（点式集合输出）;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.flatMap(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361925095-d5967335-e5d8-4136-b9dc-40aeaf574e03.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt;List(x,x,x)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362079746-1298c815-0023-41a3-9c3c-d3adf7f565c4.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt; List(x+&quot;_1&quot;,x+&quot;_2&quot;,x+&quot;_3&quot;)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362222555-086c43f1-848f-4990-9fbd-d369bdb90f42.png"></p><h4 id="（3）mapPartitions"><a href="#（3）mapPartitions" class="headerlink" title="（3）mapPartitions"></a>（3）mapPartitions</h4><p>以分区为单位进行计算处理；</p><p>在map过程中，需要频繁创建额外对象时，如文件输出流操作、jdbc操作、socket操作，使用mapPartitions算子；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4,5),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt; &#123;</span><br><span class="line">// 在此处可以加入jdbc一次初始化多少次使用的代码</span><br><span class="line">partition.map(num =&gt; num * num)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.max</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362591147-55198903-ddac-4824-9735-ad45e0648084.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt;&#123;</span><br><span class="line">partition.flatMap(1 to _)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.count</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362787238-09fde06d-4c8a-4a76-bf4f-81cd63d4888d.png"></p><h4 id="（4）glom"><a href="#（4）glom" class="headerlink" title="（4）glom"></a>（4）glom</h4><p>以分区为单位，将每个分区的值形成一个数组；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;),3)</span><br><span class="line">a.glom.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362965109-f019c760-0cca-48e7-b77e-72a72b865c73.png"></p><p>由上诉得到：分组的依据是平均分组</p><h4 id="（5）union"><a href="#（5）union" class="headerlink" title="（5）union"></a>（5）union</h4><p>将2个rdd合并成一个rdd，不去重；有时可能会发生多个分区合并成一个分区的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,2)</span><br><span class="line">val b = sc.parallelize(6 to 10,2)</span><br><span class="line">a.union(b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363148863-5e326e0c-8090-44dc-9235-fc502b1b020d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a union b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363185922-78c5a488-f745-423f-ba9f-5a1a8aed72f0.png"></p><h4 id="（6）groupBy"><a href="#（6）groupBy" class="headerlink" title="（6）groupBy"></a>（6）groupBy</h4><p>输入分区和输出分区 n : n型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(1,2,3,4,5,56,67),3)</span><br><span class="line">a.groupBy(x =&gt; &#123;if(x&gt;10) &quot;&gt;10&quot; else &quot;&lt;=10&quot;&#125;).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363353742-3a6f1e5a-9e2b-4830-aa7a-6746670268cf.png"></p><h4 id="（7）filter"><a href="#（7）filter" class="headerlink" title="（7）filter"></a>（7）filter</h4><p>输出为输入的子集；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,3)</span><br><span class="line">val b = a.filter(_%4 == 0)</span><br><span class="line">b.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363470345-d3527b6f-d8cc-4cda-9075-5295be2fc72f.png"></p><h4 id="（8）distinct"><a href="#（8）distinct" class="headerlink" title="（8）distinct"></a>（8）distinct</h4><p>输出分区为输入分区的子集，全局去重；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,3)</span><br><span class="line">val b = sc.parallelize(2 to 9,3)</span><br><span class="line">a.union(b).distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363723963-576ad83b-d590-475d-81b6-09e20703585d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val c = sc.parallelize(List(&quot;小红&quot;,&quot;消化&quot;,&quot;不良&quot;,&quot;消化&quot;))</span><br><span class="line">c.distinct.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363870738-97f17c6b-c2c6-4145-be17-a744e0697ecb.png"></p><h4 id="（9）cache"><a href="#（9）cache" class="headerlink" title="（9）cache"></a>（9）cache</h4><p>cache将rdd元素从磁盘缓存到内存中，数据反复被使用的场景使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 4,2)</span><br><span class="line">a.union(b).count</span><br><span class="line">a.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365250564-1438ac38-78bf-4c84-b7c5-1337feebeeca.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 5,3)</span><br><span class="line">val c = a.union(b).cache</span><br><span class="line">c.count</span><br><span class="line">c.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365410401-cff04c88-5a41-4aa0-a8cd-3d9bc4f8c045.png"></p><h3 id="3-1-2-key-value型转换算子"><a href="#3-1-2-key-value型转换算子" class="headerlink" title="3.1.2 key-value型转换算子"></a>3.1.2 key-value型转换算子</h3><h4 id="（1）mapValues"><a href="#（1）mapValues" class="headerlink" title="（1）mapValues"></a>（1）mapValues</h4><p>输入分区：输出分区 &#x3D; 1 ： 1</p><p>针对key-value型数据中的value进行map操作，而不对key进行处理；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,1),(&quot;李四&quot;,2),(&quot;王五&quot;,3)),2)</span><br><span class="line">val second = first.mapValues(x =&gt; x+1)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629366240118-49f73664-486d-4acd-a779-46fe1aa28042.png"></p><h4 id="（2）★-combineByKey-★"><a href="#（2）★-combineByKey-★" class="headerlink" title="（2）★ combineByKey ★"></a>（2）★ combineByKey ★</h4><p>定义</p><p>def combineByKey [C] (</p><p>createCombiner: (V) &#x3D;&gt; C,</p><p>mergeValue: (C,V) &#x3D;&gt; C,</p><p>mergeCombiners: (C,C) &#x3D;&gt; C): RDD[(String, C)]</p><p>元素</p><p>createCombiner对每个分区内的同组元素如何聚合，形成一个累加器</p><p>mergeValue 将累加器与新遇到的值进行合并的方法</p><p>mergeCombiners 每个分区都是独立处理，故同一个键可以有多个累加器。如果两个或者多个分区都对应同一个键的累加器，用方法将各个分区的结果进行合并。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,4),(&quot;张一&quot;,2),(&quot;张三&quot;,3)),2)</span><br><span class="line">val second =first.combineByKey(List(_), (x:List[Int],y:Int) =&gt; y::x, (x:List[Int], y:List[Int]) =&gt; x:::y)</span><br><span class="line">second.collect </span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629367854547-9a9fa874-7f66-4d7d-a216-a511bfb5b502.png"></p><h4 id="（3）reduceByKey"><a href="#（3）reduceByKey" class="headerlink" title="（3）reduceByKey"></a>（3）reduceByKey</h4><p>按key聚合后对组进行规约处理，求和</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;小米&quot;,&quot;华硕&quot;,&quot;很郁闷&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (x,1))</span><br><span class="line">second.reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629370722139-e07aa367-7b31-4818-a81e-fb58087606bf.png"></p><h4 id="（4）join"><a href="#（4）join" class="headerlink" title="（4）join"></a>（4）join</h4><p>对 key-value 结构的rdd进行按key的join操作，最后将V部分做flatMap打平操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,11),(&quot;李四&quot;,12)),2)</span><br><span class="line">val seconed = sc.parallelize(List((&quot;张一&quot;,2),(&quot;李二&quot;,3),(&quot;李四&quot;,3)),3)</span><br><span class="line">first.join(seconed).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371167534-5d370cc0-f9b8-4785-a30a-f89d51d0d802.png"></p><h2 id="3-2-行动算子-action"><a href="#3-2-行动算子-action" class="headerlink" title="3.2 行动算子 action"></a>3.2 行动算子 action</h2><p>该类型算子会触发SparkContext提交作业，触发RDD DAG的执行</p><h4 id="（1）无输出型，不落地本地文件或hsfs文件"><a href="#（1）无输出型，不落地本地文件或hsfs文件" class="headerlink" title="（1）无输出型，不落地本地文件或hsfs文件"></a>（1）无输出型，不落地本地文件或hsfs文件</h4><p>foreach算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;华米&quot;,&quot;小米&quot;,&quot;苹果&quot;,&quot;华米&quot;,&quot;三星&quot;),2)</span><br><span class="line">first.foreach(println _)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371520895-6d080f91-c686-4d4f-9dfa-514612d51c7e.png"></p><h4 id="（2）HDFS输出型"><a href="#（2）HDFS输出型" class="headerlink" title="（2）HDFS输出型"></a>（2）HDFS输出型</h4><p>saveAsTextFile算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;三星&quot;,&quot;华为&quot;),2)</span><br><span class="line">// 指定本地保存的目录(不存在的目录)</span><br><span class="line">first.saveAsTextFile(&quot;file:///home/shijiaxin/spark_output5&quot;)</span><br><span class="line">=====================================</span><br><span class="line">===        BUG               ========</span><br><span class="line">=====================================</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629378669316-8304be2d-269b-453c-9b8b-7e7f16265a81.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">=================killed==================</span><br><span class="line"></span><br><span class="line">// 指定保存hdfs保存目录，默认路径hdfs中/user/当前用户</span><br><span class="line">first.saveAsTextFile(&quot;spark_shell_output_txt&quot;)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629372362391-1dfc078a-4ead-4c9d-8ffa-6f748a04ef61.png"></p><h4 id="（3）scala集合和数据类型"><a href="#（3）scala集合和数据类型" class="headerlink" title="（3）scala集合和数据类型"></a>（3）scala集合和数据类型</h4><p>collect算子</p><p>相当于toArray操作，将分布式RDD返回成为一个scala array数组结果，实际是Driver所在的机器节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;, &quot;华为&quot;, &quot;花粉&quot;,  &quot;苹果&quot; ), 2)</span><br><span class="line">first.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373371312-99b79ad2-7b62-4fbe-89d1-470a492d60ec.png"></p><h4 id="（4）collectAsMap算子"><a href="#（4）collectAsMap算子" class="headerlink" title="（4）collectAsMap算子"></a>（4）collectAsMap算子</h4><p>相当于 toMap 操作，将分布式 RDD的 kv 对形式，返回成为一个 scala map集合，实际上Driver所在的机器节点，在处理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,1),(&quot;礼仪&quot;,1),(&quot;张一&quot;,3),(&quot;白虎&quot;,3)),2)</span><br><span class="line">first.collectAsMap</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373634686-a0154efc-0222-4cae-9f7a-6b288f90d852.png"></p><h4 id="（5）lookup算子"><a href="#（5）lookup算子" class="headerlink" title="（5）lookup算子"></a>（5）lookup算子</h4><p>对键值对类型的 RDD操作，返回指定key对应的元素形成的Seq</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;爱华为&quot;,&quot;爱尔兰&quot;,&quot;cs&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (&#123;if (x.contains(&quot;爱&quot;)) &quot;有爱&quot; else &quot;无爱&quot;&#125;, x))</span><br><span class="line">second.lookup(&quot;有爱&quot;)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373958406-9e27c6d8-7109-47ca-a073-30376d225f0d.png"></p><h4 id="（6）reduce算子"><a href="#（6）reduce算子" class="headerlink" title="（6）reduce算子"></a>（6）reduce算子</h4><p>先对两个元素进行reduce函数操作，然后将结果和迭代器取出的下一个元素进行reduce函数操作，直到迭代器遍历完所有元素,得到最后结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3, 3)</span><br><span class="line">a.reduce(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629375707373-e049fe20-3095-484c-88c9-214f8c84ea39.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(List((&quot;one&quot;,1),(&quot;two&quot;,2),(&quot;three&quot;,3)),3)（最后的3 代表3个分区）</span><br><span class="line">val a = sc.parallelize(List((&quot;one&quot;,1),(&quot;two&quot;,2),(&quot;three&quot;,3)),2)</span><br><span class="line">a.reduce( (x,y) =&gt;(&quot;sum&quot;, x._2 + y._3))._2  // 此处不能为3（代表元组的第二列）</span><br><span class="line">a.reduce( (x,y)=&gt;(&quot;sum&quot;,x._2+y._2))._2</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629376091510-d07bf1c2-a468-4864-b43f-8c28f45d46e7.png"></p><h4 id="（7）fold算子fold算子："><a href="#（7）fold算子fold算子：" class="headerlink" title="（7）fold算子fold算子："></a>（7）fold算子fold算子：</h4><p>def fold (zeroValue: T)(op: (T, T) &#x3D;&gt; T) : T</p><p>先对rdd分区的每一个分区进行op函数</p><p>在调用op函数过程中将zeroValue参与计算</p><p>最后在对所有分区的结果调用op函数，同事在此处进行zeroValue再次参与计算</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是41 公式：（1+2+3+4+5+6+10）+10</span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),1).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120323612.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是51 公式：（1+2+3+10）+（4+5+6+10）+10 </span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),2).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120533230.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是61 公式：（1+2+10）+（3+4+10）+（5+6+10）+10 </span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),3).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120553144.png"></p><h1 id="四：其他常用算子"><a href="#四：其他常用算子" class="headerlink" title="四：其他常用算子"></a>四：其他常用算子</h1><ul><li>cartesian算子</li><li>subtract算子</li><li>sample算子</li><li>takeSample算子</li><li>persist算子</li><li>cogroup算子</li><li>leftOuterJoin算子</li><li>rightOuterJoin算子</li><li>saveAsObjectFile算子</li><li>count算子</li><li>top算子</li><li>aggregate算子</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：算子概述&quot;&gt;&lt;a href=&quot;#一：算子概述&quot; class=&quot;headerlink&quot; title=&quot;一：算子概述&quot;&gt;&lt;/a&gt;一：算子概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-什么是算子？&quot;&gt;&lt;a href=&quot;#1-1-什么是算子？&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Flink 架构设计</title>
    <link href="https://tuumest.cn/blog/50ac08de.html/"/>
    <id>https://tuumest.cn/blog/50ac08de.html/</id>
    <published>2023-02-19T17:27:27.000Z</published>
    <updated>2023-02-19T18:36:19.393Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Flink-架构设计图"><a href="#一：Flink-架构设计图" class="headerlink" title="一：Flink 架构设计图"></a>一：Flink 架构设计图</h1><p><img src="/blog/50ac08de.html/1630318437801-84613c7b-a14d-4311-b66b-9b3ac56824bd.png"></p><h2 id="1-1-分层设计说明"><a href="#1-1-分层设计说明" class="headerlink" title="1.1 分层设计说明"></a>1.1 分层设计说明</h2><ul><li>物理部署层 -deploy层<ul><li>负责解决Flink的部署模式问题</li><li>支持多种部署模式：本地部署、集群部署（standalone&#x2F;yarn&#x2F;mesos）、云（GCE&#x2F;EC2）以及 kubernetes 。</li><li>通过该层支持不同平台的部署，用户可以根据自身场景和需求使用对应的部署模式；</li></ul></li><li>Runtime核心层<ul><li>是Flink分布式计算框架的核心实现层，负责对上层不同接口提供基础服务。</li><li>支持分布式steam作业的执行、jobGraph到ExecutionGraph的映射转换以及任务调度等。</li><li>将DataStream和DataSet转成统一的可执行的Task Operator，达到在流式计算引擎下同时处理批量计算和流式计算的目的。</li></ul></li><li>API &amp; Libraries 层<ul><li>负责更好的开发用户体验，包括易用性、开发效率、执行效率、状态管理等方面。</li><li>Flink同时提供了支撑流计算和批处理的接口，同时在这基础上抽象出不同的应用类型的组件库，如：<ul><li>基于流处理的CEP（复杂事件处理库）</li><li>Table &amp; Sql库</li><li>基于批处理的FlinkML（机器学习库）</li><li>图处理库(Gelly)</li></ul></li><li>API层包括两部分<ul><li>流计算应用的DataStream API</li><li>批处理应用的DataSet API</li><li>统一的API，方便用于直接操作状态和时间等底层数据<ul><li>提供了丰富的数据处理高级API，例如Map、FllatMap操作等</li><li>并提供了比较低级的Process Function API</li></ul></li></ul></li></ul></li></ul><h1 id="二：运行模式"><a href="#二：运行模式" class="headerlink" title="二：运行模式"></a>二：运行模式</h1><h2 id="2-1-运行模式核心区分点"><a href="#2-1-运行模式核心区分点" class="headerlink" title="2.1 运行模式核心区分点"></a>2.1 运行模式核心区分点</h2><ul><li>集群生命周期和资源隔离保证</li><li>应用程序的main()方法是在客户端还是在集群上执行</li></ul><h2 id="2-2-所有模式分类说明"><a href="#2-2-所有模式分类说明" class="headerlink" title="2.2 所有模式分类说明"></a>2.2 所有模式分类说明</h2><ul><li>本地运行模式</li><li>standalone模式</li><li>集群运行模式<ul><li>经常是指flink on yarn集群模式</li><li>yarn也可以换成mesos,Kubernetes(k8s)等资源管理平台替换</li><li>共三种<ul><li>session 模式</li><li>per-job 模式</li><li>application 模式</li></ul></li></ul></li></ul><h2 id="2-3-本地运行模式"><a href="#2-3-本地运行模式" class="headerlink" title="2.3 本地运行模式"></a>2.3 本地运行模式</h2><ul><li>运行过程：一个机器启动一个进程的多线程来模拟分布式计算。</li><li>主要用于代码测试</li></ul><h2 id="2-4-standalone模式"><a href="#2-4-standalone模式" class="headerlink" title="2.4 standalone模式"></a>2.4 standalone模式</h2><ul><li>运行过程：完全独立的Flink集群的模式，各个环节均Flink自己搞定。并没有yarn、mesos的统一资源调度平台。</li><li>主要是只有纯Flink纯计算的场景，商用场景极少。</li></ul><h2 id="2-5-集群运行模式"><a href="#2-5-集群运行模式" class="headerlink" title="2.5 集群运行模式"></a>2.5 集群运行模式</h2><h3 id="（1）Flink-Session-集群（会话模式）"><a href="#（1）Flink-Session-集群（会话模式）" class="headerlink" title="（1）Flink Session 集群（会话模式）"></a>（1）Flink Session 集群（会话模式）</h3><ul><li><ul><li>集群生命周期：<ul><li>在 Flink Session 集群中，客户端连接到一个预先存在的、长期运行的集群；</li><li>该集群可以接受多个作业提交。即使所有作业完成后，集群（和 JobManager）仍将继续运行直到手动停止 session 为止。</li><li>因此，<code>Flink Session 集群的寿命不受任何 Flink 作业寿命的约束</code>。</li></ul></li><li>TaskManager slot 由 ResourceManager 在提交作业时分配，并在作业完成时释放。<ul><li>由于所有作业都共享同一集群，因此在集群资源方面存在一些竞争——例如提交工作阶段的网络带宽。</li><li>此共享设置的局限性在于，如果 TaskManager 崩溃，则在此 TaskManager 上运行 task 的所有作业都将失败；</li><li>再比如，如果 JobManager 上发生一些致命错误，它将影响集群中正在运行的所有作业。</li></ul></li><li>其他注意事项：<ul><li>拥有一个预先存在的集群可以节省大量时间申请资源和启动 TaskManager。</li><li>有种场景很重要，作业执行时间短并且启动时间长会对端到端的用户体验产生负面的影响。<ul><li>就像对简短查询的交互式分析一样，希望作业可以使用现有资源快速执行计算。</li></ul></li></ul></li><li>Flink Session 集群也被称为 session 模式下的 Flink 集群。</li><li>工作模式<ul><li>附加模式（默认）<ul><li>特点<ul><li>客户端与Flink作业集群相互同步</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将 Flink 集群提交给 YARN，但客户端保持运行，跟踪集群的状态。</li><li>如果集群失败，客户端将显示错误。如果客户端被终止，它也会通知集群关闭。</li></ul></li></ul></li><li>分离模式（-d或–detached）<ul><li>特点<ul><li>客户端与Flink作业集群相互异步，客户端提交完成后即可退出</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将Flink集群提交给YARN，然后客户端返回。</li><li>需要再次调用客户端或 YARN 工具来停止 Flink 集群。</li></ul></li></ul></li></ul></li><li>工作流程特征说明<ul><li>多个不同的FlinkJob向同一个Flink Session会话上提交作业，由这一个统一个的FlinkSession管理所有的Flink作业。</li><li>工作流程示意图</li></ul></li></ul></li><li><p><img src="/blog/50ac08de.html/1630323312227-b12cae9d-16c2-47b1-a1cf-e697d719c689.png"></p></li></ul><h3 id="（2）Flink-Job-集群（per-job模式）"><a href="#（2）Flink-Job-集群（per-job模式）" class="headerlink" title="（2）Flink Job 集群（per-job模式）"></a>（2）Flink Job 集群（per-job模式）</h3><ul><li>Flink Job 集群也被称为 job (or per-job) 模式下的 Flink 集群。</li><li>集群生命周期：<ul><li>在 Flink Job 集群中，可用的集群管理器（例如 YARN）用于为每个提交的作业启动一个集群，并且该集群仅可用于该作业。</li><li>在这里客户端首先从集群管理器请求资源启动 JobManager，然后将作业提交给在这个进程中运行的 Dispatcher。然后根据作业的资源请求惰性的分配 TaskManager。</li><li>一旦作业完成，Flink Job 集群将被拆除。</li></ul></li><li>资源隔离：<ul><li>JobManager 中的致命错误仅影响在 Flink Job 集群中运行的一个作业。</li></ul></li><li>其他注意事项：<ul><li>由于 ResourceManager 必须应用并等待外部资源管理组件来启动 TaskManager 进程和分配资源，所以其实时计算性并没有session模式强</li><li>因此 Flink Job 集群更适合长期运行、具有高稳定性要求且对较长的启动时间不敏感的大型作业；</li></ul></li><li>工作流程特征说明：<ul><li>多个不同的FlinkJob向各自由统一资源管理器(Yarn)生成的专用Flink Session会话上提交作业，由作业所属的FlinkSession管理自己的Flink作业。</li><li>工作流程示意图</li></ul></li></ul><p><img src="/blog/50ac08de.html/1630323597224-1c423290-947a-44b4-9101-7adce28b5202.png"></p><h3 id="（3）Flink-Application-集群（应用模式）"><a href="#（3）Flink-Application-集群（应用模式）" class="headerlink" title="（3）Flink Application 集群（应用模式）"></a>（3）Flink Application 集群（应用模式）</h3><ul><li>Flink Job 集群可以看做是 Flink Application 集群”客户端运行“的替代方案。</li><li>该模式为yarn session和yarn per-job模式的折中选择。</li><li>集群生命周期：<ul><li>Flink Application 集群是与Flink作业执行直接相关的运行模式，并且 <code>main()方法在集群上而不是客户端上运行</code>。</li><li>提交作业是一个单步骤过程：<ul><li>无需先启动 Flink 集群，然后将作业提交到现有的 session 集群。</li><li>将应用程序逻辑和依赖打包成一个可执行的作业 JAR 中，由入口机客户端提交jar包和相关资源到hdfs当中。</li><li>由调度启动的集群当中JobManager来拉取已由上一步上传完成的运行代码所需要的所有资源。</li><li>如果有JobManager HA设置的话，将会同时启动多个JobManager作HA保障，此时将面临JobManager的选择，最终由一个胜出的JobManager作为Active进程推进下边的执行。</li><li>由运行JobManager进程的集群入口点节点机器（ApplicationClusterEntryPoint）负责调用 main()方法来提取 JobGraph，即作为用户程序的解析和提交的客户端程序与集群进行交互，直到作业运行完成。</li><li>如果一个main()方法中有多个env.execute()&#x2F;executeAsync()调用，在Application模式下，这些作业会被视为属于同一个应用，在同一个集群中执行（如果在Per-Job模式下，就会启动多个集群）</li><li>该模式也允许我们像在 Kubernetes 上部署任何其他应用程序一样部署 Flink 应用程序。</li><li>因此，Flink Application 集群的寿命与 Flink 应用程序的寿命有关。</li></ul></li></ul></li><li>资源隔离：<ul><li>在 Flink Application 集群中，ResourceManager 和 Dispatcher 作用于单个的 Flink 应用程序，相比于 Flink Session 集群，它提供了更好的隔离。</li></ul></li><li>工作流程特征说明：<ul><li>将各个环节更进一步进行专用化处理，相当于每个FlinkJob都有一套专用的服务角色进程。</li></ul></li></ul><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><ul><li>应用场景<ul><li>本地布署模式：demo、代码测试场景。</li><li>Session模式：集群资源充分、频繁任务提交、小作业居多、实时性要求高的场景。(该模式较少）</li><li>Per-Job模式：作业少、大作业、实时性要求低的场景。</li><li>Application模式：实时性要求不太高、安全性有一定要求均可以使用，普遍适用性最强。</li></ul></li><li>生产环境使用说明<ul><li>建议用per-job或是application模式，提供了更好的资源隔离性和安全性。</li></ul></li></ul><h1 id="三：运行流程"><a href="#三：运行流程" class="headerlink" title="三：运行流程"></a>三：运行流程</h1><h2 id="3-1-工作流程图"><a href="#3-1-工作流程图" class="headerlink" title="3.1 工作流程图"></a>3.1 工作流程图</h2><p><img src="/blog/50ac08de.html/1630324699314-e7922476-6b00-4315-894e-e9d794163070.png"></p><h2 id="3-2-运行时核心角色组成"><a href="#3-2-运行时核心角色组成" class="headerlink" title="3.2 运行时核心角色组成"></a>3.2 运行时核心角色组成</h2><ul><li>由两种类型的进程组成，一个 JobManager 和一个或者多个 TaskManager。</li><li>Client 客户端不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。</li><li>提交任务完成之后，Client可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。</li><li>Client可以作为触发执行 Java&#x2F;Scala 程序的一部分运行，也可以在命令行进程.&#x2F;bin&#x2F;flink run …中运行。</li><li>可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。</li><li>Actor System<ul><li>各个角色组件互相通信的消息传递系统中间件。</li><li>Actor是一种并发编程模型，与另一种模型共享内存完全相反，Actor模型share nothing，即没有任何共享。</li><li>所有的线程(或进程)通过消息传递的方式进行合作(通信)，这些线程(或进程)称为Actor。</li><li>以其简单、高效著称</li><li>缺点<ul><li>唯一的缺点是不能实现真正意义上的并行， 而是通过并发实现的并行效果，会存在一定的不确定性。</li><li>纯消息通信，实时性和粒度控制上会略弱于共享内存的方式。</li></ul></li></ul></li></ul><h2 id="3-3-核心组成角色剖析"><a href="#3-3-核心组成角色剖析" class="headerlink" title="3.3 核心组成角色剖析"></a>3.3 核心组成角色剖析</h2><ul><li>JobManager<ul><li>JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：<ul><li>它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：<ul><li>ResourceManager<ul><li>ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的最小单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。</li></ul></li><li>Dispatcher<ul><li>Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。</li></ul></li><li>JobMaster<ul><li>JobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。</li><li>始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。</li></ul></li></ul></li></ul></li></ul></li><li>TaskManager<ul><li>TaskManager（也称为 worker）执行作业流的 task，并且缓存和交换数据流。</li><li>必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task slot。</li><li>TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子。</li></ul></li></ul><h2 id="3-4-yarn模式提交任务的工作流程"><a href="#3-4-yarn模式提交任务的工作流程" class="headerlink" title="3.4 yarn模式提交任务的工作流程"></a>3.4 yarn模式提交任务的工作流程</h2><ul><li>工作流程图：</li></ul><p><img src="/blog/50ac08de.html/1630325042367-d709bf3d-24b0-4907-9f10-64693abdf588.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Flink-架构设计图&quot;&gt;&lt;a href=&quot;#一：Flink-架构设计图&quot; class=&quot;headerlink&quot; title=&quot;一：Flink 架构设计图&quot;&gt;&lt;/a&gt;一：Flink 架构设计图&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/50ac08de.h</summary>
      
    
    
    
    <category term="Flink" scheme="https://tuumest.cn/categories/Flink/"/>
    
    
    <category term="Flink" scheme="https://tuumest.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 概述</title>
    <link href="https://tuumest.cn/blog/72cd4c87.html/"/>
    <id>https://tuumest.cn/blog/72cd4c87.html/</id>
    <published>2023-02-19T16:45:05.000Z</published>
    <updated>2023-02-19T17:27:46.943Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：产生背景"><a href="#一：产生背景" class="headerlink" title="一：产生背景"></a>一：产生背景</h1><h2 id="1-1-历史背景"><a href="#1-1-历史背景" class="headerlink" title="1.1 历史背景"></a>1.1 历史背景</h2><ul><li>随着互联网应用的快速发展，<code>实时流数据产生日益增多和普遍化</code>。如日常生活、金融、驾驶、LBS、电商等众多领域。</li><li>实时数据的处理和挖掘能够带来离线数据处理和挖掘<code>更多的社会发展和商业价值</code>。</li><li>如何快速响应和处理这些大规模的实时数据流，成为众多互联网大厂的当务之急。</li><li>在flink之前也出现了很多<code>流数据处理引擎</code>，包括storm、sparkstreaming等知名流行框架，但各自均有较明显的不足，导致没有达到理想的流处理引擎的标准要求。</li></ul><h2 id="1-2-优秀的流处理引擎"><a href="#1-2-优秀的流处理引擎" class="headerlink" title="1.2 优秀的流处理引擎"></a>1.2 优秀的流处理引擎</h2><ul><li>优秀流处理引擎标准要求<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li><li>storm<ul><li>优点：低延迟</li><li>缺点：其它要求都较差一些</li></ul></li><li>sparkstreaming<ul><li>优点：高吞吐量、容错性高</li><li>缺点：其它要求都较差一些</li></ul></li></ul><h1 id="二：基本介绍"><a href="#二：基本介绍" class="headerlink" title="二：基本介绍"></a>二：基本介绍</h1><h2 id="2-1-概念说明"><a href="#2-1-概念说明" class="headerlink" title="2.1 概念说明"></a>2.1 概念说明</h2><ul><li>由Apache软件基金会开发的开源流处理框架</li><li>其核心是用Java和Scala编写的框架和分布式处理引擎</li><li>用于对无界和有界数据流进行有状态计算。<ul><li>无界数据流: 即为实时流数据；</li><li>有界数据流：即为离线数据，也称为批处理数据；</li></ul></li></ul><h2 id="2-2-特点特征"><a href="#2-2-特点特征" class="headerlink" title="2.2 特点特征"></a>2.2 特点特征</h2><ul><li>被设计为在所有常见的集群环境中运行，以内存速度和任何规模执行计算。</li><li>能够达到实时流处理引擎的全部标准要求。<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li></ul><h1 id="三：应用场景"><a href="#三：应用场景" class="headerlink" title="三：应用场景"></a>三：应用场景</h1><h2 id="3-1-官方说明"><a href="#3-1-官方说明" class="headerlink" title="3.1 官方说明"></a>3.1 官方说明</h2><ul><li>事件驱动型应用</li><li>数据分析型应用</li><li>数据管道 ETL</li></ul><h2 id="3-2-实际情况"><a href="#3-2-实际情况" class="headerlink" title="3.2 实际情况"></a>3.2 实际情况</h2><ul><li>要求严格的实时流处理场景</li></ul><h1 id="四：代码实现"><a href="#四：代码实现" class="headerlink" title="四：代码实现"></a>四：代码实现</h1><h2 id="4-1-实现方式"><a href="#4-1-实现方式" class="headerlink" title="4.1 实现方式"></a>4.1 实现方式</h2><ul><li>Java API</li><li>Scala API</li></ul><h2 id="4-2-统一数据处理过程抽象"><a href="#4-2-统一数据处理过程抽象" class="headerlink" title="4.2 统一数据处理过程抽象"></a>4.2 统一数据处理过程抽象</h2><ul><li>将实时和批处理的数据过程，均抽象成三个过程，即Source-&gt;Transform-&gt;Sink。<ul><li>Source为源数据读入，即Source算子。</li><li>Transform是数据转换处理过程，即Transform算子。</li><li>Sink即数据接收器，即数据落地到存储层，即Sink算子。</li></ul></li><li>代码实现复杂度<ul><li>丰富的API和算子操作；</li><li>抽象封装统一性较高，支持类SQL编程，编程复杂度并不高。</li></ul></li></ul><h1 id="五：版本发展"><a href="#五：版本发展" class="headerlink" title="五：版本发展"></a>五：版本发展</h1><table><thead><tr><th><strong>版本</strong></th><th><strong>发行日期</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Flink 0.6-incubating</strong></td><td><strong>2014-08-26</strong></td><td><strong>初步得到团队内部认可，正在快速迭代中</strong></td></tr><tr><td><strong>Flink 0.9.0-milestone</strong></td><td><strong>2015-04-13</strong></td><td><strong>有重大进展，得正式对外发布，</strong></td></tr><tr><td><strong>0.9</strong></td><td><strong>2015-09-01</strong></td><td><strong>从此时开始引入阿里巴巴，并成为阿里系主干业务实时流处理引擎，内部改良优化后命名为blink</strong></td></tr><tr><td>0.10</td><td>2016-02-11</td><td></td></tr><tr><td><strong>1.0</strong></td><td><strong>2016-05-11</strong></td><td><strong>很有里程碑、代表性的一个版本</strong></td></tr><tr><td>1.1</td><td>2017-03-22</td><td></td></tr><tr><td>1.2</td><td>2017-04-26</td><td></td></tr><tr><td>1.3</td><td>2018-03-15</td><td></td></tr><tr><td>1.4</td><td>2018-03-08</td><td></td></tr><tr><td>1.5</td><td>2018-10-29</td><td></td></tr><tr><td>1.6</td><td>2018-10-29</td><td></td></tr><tr><td><strong>1、在2019年初，blink在阿里内部经过多年的商用实践，增加了N多新特性，并得到广泛应用和成熟化，正式对外开源，并捐赠给Apache Flink社区，并成为其下的分支方式开源并逐步融合后，依然以Flink为主进一步推进开源进程。****2、阿里系以9000万欧元收购了创业公司 Data Artisans，即Flink的开发团队所属公司。</strong></td><td></td><td></td></tr><tr><td>1.7</td><td>2019-02-15</td><td></td></tr><tr><td>1.8</td><td>2019-04-09</td><td></td></tr><tr><td><strong>1.9</strong></td><td><strong>2019-08-19</strong></td><td><strong>目前市占率较高的一个版本</strong></td></tr><tr><td>1.10</td><td>2020-02-11</td><td></td></tr><tr><td>1.11</td><td>2020-07-06</td><td><strong>从此版本开始，加入很多新特性，支持hadoop3.x版本</strong></td></tr><tr><td>1.12</td><td>2020-12-08</td><td></td></tr><tr><td>Flink 1.13.0</td><td>2021-04-30</td><td></td></tr><tr><td><strong>Flink 1.13.1</strong></td><td><strong>2021-05-28</strong></td><td><strong>版本迭代很快，社区很活跃，发展非常快****已是稳定版。</strong></td></tr></tbody></table><ul><li>Flink版本在早期就得到阿里认可，并进行集团内部孵化和二次开发、商用实践，命名为Blink。</li><li>Blink的主要贡献是在用户体验上，包括SQL、webUI等方面。</li><li>在2019年进行了开源反馈给社区，从此更多的是以Flink merge Blink新功能后，以Flink为主继续推进开源。</li><li>基于市场量、成熟度、社区丰富度等方面，通常选择1.13.1版本。</li></ul><h1 id="六：市场前景"><a href="#六：市场前景" class="headerlink" title="六：市场前景"></a>六：市场前景</h1><ul><li>现实情况<ul><li>学习成本较高、应用场景较垂直，其实际开发者在市场上是比较衡缺的。</li><li>相对于更广大的中小型公司，Flink的使用量最主要是集中在中大型互联网科技公司。</li></ul></li><li>发展趋势<ul><li>商业市场、各种大型IT企业对大规模实时数据场景需求旺盛。</li><li>Flink在实时数据处理方面的架构设计与商用实践表现较为突出。</li><li>得到阿里系的商业收购+大规模人力财力物力的支持，未来发展不可限量。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：产生背景&quot;&gt;&lt;a href=&quot;#一：产生背景&quot; class=&quot;headerlink&quot; title=&quot;一：产生背景&quot;&gt;&lt;/a&gt;一：产生背景&lt;/h1&gt;&lt;h2 id=&quot;1-1-历史背景&quot;&gt;&lt;a href=&quot;#1-1-历史背景&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="Flink" scheme="https://tuumest.cn/categories/Flink/"/>
    
    
    <category term="Flink" scheme="https://tuumest.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark 交互操作</title>
    <link href="https://tuumest.cn/blog/75610dc7.html/"/>
    <id>https://tuumest.cn/blog/75610dc7.html/</id>
    <published>2023-02-19T07:25:39.000Z</published>
    <updated>2023-02-19T08:01:27.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Spark-运行模式"><a href="#一：Spark-运行模式" class="headerlink" title="一：Spark 运行模式"></a>一：Spark 运行模式</h1><p>即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节选择。</p><table><thead><tr><th><strong>序号</strong></th><th align="center"><strong>模式名称</strong></th><th align="center"><strong>特点</strong></th><th align="center"><strong>应用场景</strong></th></tr></thead><tbody><tr><td>1</td><td align="center">本地运行模式(local)</td><td align="center">单台机器多线程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>2</td><td align="center">伪分布式模式</td><td align="center">单台机器多进程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>3</td><td align="center">standalone(client)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark submit client端</td><td align="center">机器资源充分纯用spark计算框架任务提交后在spark submit   client端实时查看反馈信息数据共享性弱测试使用还可以，生产环境极少使用该种模式</td></tr><tr><td>4</td><td align="center">standalone(cluster)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark worker node端</td><td align="center">机器资源充分纯用spark计算框架任务提交后将退出spark   submit client端数据共享性弱测试和生产环境均可以自由使用，但更多用于生产环境</td></tr><tr><td>5</td><td align="center">spark on yarn(yarn-client)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在yarn client上</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后在yarn client端实时查看反馈信息</td></tr><tr><td>6</td><td align="center">spark on yarn(yarn-cluster)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在集群的am contianer中</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后将退出yarn client端</td></tr><tr><td>7</td><td align="center">spark on mesos&#x2F;ec2</td><td align="center">与spark on yarn类似</td><td align="center">与spark on yarn类似在国内应用较少</td></tr></tbody></table><h1 id="二：Spark-用户交互方式"><a href="#二：Spark-用户交互方式" class="headerlink" title="二：Spark 用户交互方式"></a>二：Spark 用户交互方式</h1><ol><li>spark-shell：spark命令行方式来操作spark作业。<ul><li>多用于简单的学习、测试、简易作业操作。</li></ul></li><li>spark-submit：通过程序脚本，提交相关的代码、依赖等来操作spark作业。<ul><li>最多见的提交任务的交互方式，简单易用、参数齐全。</li></ul></li><li>spark-sql：通过sql的方式操作spark作业。<ul><li>sql相关的学习、测试、生产环境研发均可以使用该直接操作交互方式。</li></ul></li><li>spark-class：最低层的调用方式，其它调用方式多是最终转化到该方式中去提交。<ul><li>直接使用较少</li></ul></li><li>sparkR、sparkPython：通过其它非java、非scala语言直接操作spark作业的方式。<ul><li>R、python语言使用者的交互方式。</li></ul></li></ol><h1 id="三：Spark-Shell-操作"><a href="#三：Spark-Shell-操作" class="headerlink" title="三：Spark-Shell 操作"></a>三：Spark-Shell 操作</h1><h2 id="3-1-交互方式定位"><a href="#3-1-交互方式定位" class="headerlink" title="3.1 交互方式定位"></a>3.1 交互方式定位</h2><ul><li>一个强大的交互式数据操作与分析的工具，提供一个简单的方式快速学习spark相关的API。</li></ul><h2 id="3-2-启动方式"><a href="#3-2-启动方式" class="headerlink" title="3.2 启动方式"></a>3.2 启动方式</h2><ul><li>前置环境：已将spark-shell等交互式脚本已加入系统PATH变量，可在任意位置使用。</li><li>以本地2个线程来模拟运行spark相关操作，该数量一般与本机的cpu核数相一致为最佳spark-shell –master local[2]</li></ul><h2 id="3-3-相关参数"><a href="#3-3-相关参数" class="headerlink" title="3.3 相关参数"></a>3.3 相关参数</h2><ul><li>参数列表获取方式：spark-shell –help</li></ul><h1 id="四：Spark-submit-操作"><a href="#四：Spark-submit-操作" class="headerlink" title="四：Spark-submit 操作"></a>四：Spark-submit 操作</h1><h2 id="4-1-交互方式定位"><a href="#4-1-交互方式定位" class="headerlink" title="4.1 交互方式定位"></a>4.1 交互方式定位</h2><ul><li>最常用的通过程序脚本，提交相关的代码、依赖等来操作spark作业的方式。</li></ul><h2 id="4-2-启动方式"><a href="#4-2-启动方式" class="headerlink" title="4.2 启动方式"></a>4.2 启动方式</h2><ul><li>spark-submit提交任务的模板</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --jars jar_list_by_comma \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><h2 id="4-3-spark-submit-详细参数说明"><a href="#4-3-spark-submit-详细参数说明" class="headerlink" title="4.3 spark-submit 详细参数说明"></a>4.3 spark-submit 详细参数说明</h2><table><thead><tr><th align="center">参数名</th><th>参数说明</th></tr></thead><tbody><tr><td align="center">–master</td><td>master 的地址，提交任务到哪里执行，例如 spark:&#x2F;&#x2F;host:port,   yarn, local</td></tr><tr><td align="center">–deploy-mode</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td align="center">–class</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td align="center">–name</td><td>应用程序的名称</td></tr><tr><td align="center">–jars</td><td>用逗号分隔的本地jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td align="center">–packages</td><td>包含在driver 和executor 的  classpath 中的 jar 的 maven 坐标</td></tr><tr><td align="center">–exclude-packages</td><td>为了避免冲突   而指定不包含的 package</td></tr><tr><td align="center">–repositories</td><td>远程 repository</td></tr><tr><td align="center">–conf PROP&#x3D;VALUE</td><td>指定 spark 配置属性的值， 例如  -conf spark.executor.extraJavaOptions&#x3D;”-XX:MaxPermSize&#x3D;256m”</td></tr><tr><td align="center">–properties-file</td><td>加载的配置文件，默认为 conf&#x2F;spark-defaults.conf</td></tr><tr><td align="center">–driver-memory</td><td>Driver内存，默认 1G</td></tr><tr><td align="center">–driver-java-options</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td align="center">–driver-library-path</td><td>传给 driver 的额外的库路径</td></tr><tr><td align="center">–driver-class-path</td><td>传给 driver 的额外的类路径</td></tr><tr><td align="center">–driver-cores</td><td>Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</td></tr><tr><td align="center">–executor-memory</td><td>每个 executor 的内存，默认是1G</td></tr><tr><td align="center">–total-executor-cores</td><td>所有 executor 总共的核数。仅仅在  mesos 或者 standalone 下使用</td></tr><tr><td align="center">–num-executors</td><td>启动的 executor 数量。默认为2。在 yarn 下使用</td></tr><tr><td align="center">–executor-core</td><td>每个 executor 的核数。在yarn或者standalone下使用</td></tr></tbody></table><h2 id="4-4-关于–master取值的特别说明"><a href="#4-4-关于–master取值的特别说明" class="headerlink" title="4.4 关于–master取值的特别说明"></a>4.4 关于–master取值的特别说明</h2><table><thead><tr><th>local</th><th>本地worker线程中运行spark，完全没有并行</th></tr></thead><tbody><tr><td>local[K]</td><td>在本地work线程中启动K个线程运行spark</td></tr><tr><td>local[*]</td><td>启动与本地work机器的core个数相同的线程数来运行spark</td></tr><tr><td>spark:&#x2F;&#x2F;HOST:PORT</td><td>连接指定的standalone集群的master，默认7077端口</td></tr><tr><td>mesos:&#x2F;&#x2F;HOST:PORT</td><td>连接到mesos集群，国内用的极少</td></tr><tr><td>yarn</td><td>使用yarn的cluster或者yarn的client模式连接。取决于–deploy-mode参数，由deploy-mode的取值为client或是cluster来最终决定。也可以用yarn-client或是yarn-cluster进行二合一参数使用，保留–master去掉—deploy-mode参数亦可。–master   yarn-client，相当于—master   yarn –deploy-mode client的二合一</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Spark-运行模式&quot;&gt;&lt;a href=&quot;#一：Spark-运行模式&quot; class=&quot;headerlink&quot; title=&quot;一：Spark 运行模式&quot;&gt;&lt;/a&gt;一：Spark 运行模式&lt;/h1&gt;&lt;p&gt;即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 相关术语</title>
    <link href="https://tuumest.cn/blog/f278f015.html/"/>
    <id>https://tuumest.cn/blog/f278f015.html/</id>
    <published>2023-02-19T07:00:48.000Z</published>
    <updated>2023-02-19T07:19:37.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：RDD-Resilient-Distributed-DataSet"><a href="#一：RDD-Resilient-Distributed-DataSet" class="headerlink" title="一：RDD (Resilient Distributed DataSet)"></a>一：RDD (Resilient Distributed DataSet)</h1><ul><li>弹性分布式数据集，是对数据集在spark存储和计算过程中的一种抽象。</li><li>是一组只读、可分区的分布式数据集合。</li><li>一个RDD 包含多个分区Partition(类似于MapReduce中的InputSplit中的block)，分区是依照一定的规则的，将具有相同规则的属性的数据记录放在一起。</li><li>横向上可切分并行计算，以分区Partition为切分后的最小存储和计算单元。</li><li>纵向上可进行内外存切换使用，即当数据在内存不足时，可以用外存磁盘来补充。</li></ul><h1 id="二：Partition-（分区）"><a href="#二：Partition-（分区）" class="headerlink" title="二：Partition （分区）"></a>二：Partition （分区）</h1><ul><li>Partition类似hadoop的Split中的block，计算是以partition为单位进行的，提供了一种划分数据的方式。</li><li>Partition的划分依据有很多，常见的有Hash分区、范围分区等，也可以自己定义的，像HDFS文件，划分的方式就和MapReduce一样，以文件的block来划分不同的partition。</li><li>一个Partition交给一个Task去计算处理。</li></ul><h1 id="三：算子"><a href="#三：算子" class="headerlink" title="三：算子"></a>三：算子</h1><ul><li>英文简称：Operator，简称op</li><li>广义上讲，对任何函数进行某一项操作都可以认为是一个算子</li><li>通俗上讲，算子即为映射、关系、变换。</li><li>MapReduce算子，主要分为两个，即为Map和Reduce两个主要操作的算子，导致灵活可用性比较差。</li><li>Spark算子，分为两大类，即为Transformation和Action类，合计有80多个。</li></ul><h1 id="四：Transformation类算子"><a href="#四：Transformation类算子" class="headerlink" title="四：Transformation类算子"></a>四：Transformation类算子</h1><ul><li>操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</li><li>细分类<ul><li>Value数据类型的Transformation算子</li><li>Key-Value数据类型的Transfromation算子</li></ul></li></ul><h1 id="五：Action类算子"><a href="#五：Action类算子" class="headerlink" title="五：Action类算子"></a>五：Action类算子</h1><ul><li>会触发 Spark 提交作业（Job），并将数据输出 Spark 系统。</li></ul><h1 id="六：窄依赖"><a href="#六：窄依赖" class="headerlink" title="六：窄依赖"></a>六：窄依赖</h1><ul><li>如果一个父RDD的每个分区只被子RDD的一个分区使用 —-&gt; 一对一关系</li></ul><h1 id="七：宽依赖"><a href="#七：宽依赖" class="headerlink" title="七：宽依赖"></a>七：宽依赖</h1><ul><li>如果一个父RDD的每个分区要被子RDD 的多个分区使用 —-&gt; 一对多关系</li></ul><h1 id="八：Application"><a href="#八：Application" class="headerlink" title="八：Application"></a>八：Application</h1><ul><li>Spark Application的概念和MapReduce中的job或者yarn中的application类似，指的是用户编写的Spark应用程序，包含了一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码</li><li>一般是指整个Spark项目从开发、测试、布署、运行的全部。</li></ul><h1 id="九：Driver"><a href="#九：Driver" class="headerlink" title="九：Driver"></a>九：Driver</h1><ul><li>运行main函数并且创建SparkContext的程序。</li><li>称为驱动程序，Driver Program类似于hadoop的wordcount程序中的driver类的main函数。</li></ul><h1 id="十：Cluster-Manager"><a href="#十：Cluster-Manager" class="headerlink" title="十：Cluster Manager"></a>十：Cluster Manager</h1><ul><li>集群的资源管理器，在集群上获取资源的服务。如Yarn、Mesos、Spark Standalone等。</li><li>以Yarn为例，驱动程序会向Yarn申请计算我这个任务需要多少的内存，多少CPU等，后由Cluster Manager会通过调度告诉驱动程序可以使用，然后驱动程序将任务分配到既定的Worker Node上面执行。</li></ul><h1 id="十一：WorkerNode"><a href="#十一：WorkerNode" class="headerlink" title="十一：WorkerNode"></a>十一：WorkerNode</h1><ul><li>集群中任何一个可以运行spark应用代码的节点。</li><li>Worker Node就是物理机器节点，可以在上面启动Executor进程。</li></ul><h1 id="十二：Executor"><a href="#十二：Executor" class="headerlink" title="十二：Executor"></a>十二：Executor</h1><ul><li>Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立专享的一批Executor。</li><li>Executor即为spark概念的资源容器，类比于yarn的container容器，真正承载Task的运行与管理，以多线程的方式运行Task，更加高效快速。</li></ul><h1 id="十三：Task"><a href="#十三：Task" class="headerlink" title="十三：Task"></a>十三：Task</h1><ul><li>与Hadoop中的Map Task或者Reduce Task是类同的。</li><li>分配到executor上的基本工作单元，执行实际的计算任务。</li><li>Task分为两类，即为ShuffleMapTask和ResultTask。<ul><li>ShuffleMapTask：即为Map任务和发生Shuffle的任务的操作，由Transformation操作组成，其输出结果是为下个阶段任务(ResultTask)进行做准备，不是最终要输出的结果。</li><li>ResultTask：即为Action操作触发的Job作业的最后一个阶段任务，其输出结果即为Application最终的输出或存储结果。</li></ul></li></ul><h1 id="十四：Job（作业）"><a href="#十四：Job（作业）" class="headerlink" title="十四：Job（作业）"></a>十四：Job（作业）</h1><ul><li>Spark RDD里的每个action的计算会生成一个job。</li><li>用户提交的Job会提交给DAGScheduler（Job调度器），Job会被分解成Stage去执行，每个Stage由一组相同计算规则的Task组成，该组Task也称为TaskSet，实际交由TaskScheduler去调度Task的机器执行节点，最终完成作业的执行。</li></ul><h1 id="十五：Stage（阶段）"><a href="#十五：Stage（阶段）" class="headerlink" title="十五：Stage（阶段）"></a>十五：Stage（阶段）</h1><ul><li>Stage是Job的组成部分，每个Job可以包含1个或者多个Stage。</li><li>Job切分成Stage是以Shuffle作为分隔依据，Shuffle前是一个Stage，Shuffle后是一个Stage。即为按RDD宽窄依赖来划分Stage。  </li><li>每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业可以被分为一个或多个阶段。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：RDD-Resilient-Distributed-DataSet&quot;&gt;&lt;a href=&quot;#一：RDD-Resilient-Distributed-DataSet&quot; class=&quot;headerlink&quot; title=&quot;一：RDD (Resilient Distr</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 架构设计</title>
    <link href="https://tuumest.cn/blog/6a386de9.html/"/>
    <id>https://tuumest.cn/blog/6a386de9.html/</id>
    <published>2023-02-19T06:36:20.000Z</published>
    <updated>2023-02-19T07:06:56.914Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：架构总览"><a href="#一：架构总览" class="headerlink" title="一：架构总览"></a>一：架构总览</h1><p><img src="/blog/6a386de9.html/1629182444027-d40b52f7-91de-4983-9fe1-c9c7e82c2a3d.png"></p><h1 id="二：角色作用"><a href="#二：角色作用" class="headerlink" title="二：角色作用"></a>二：角色作用</h1><ul><li>Client：面向用户，对外提供接口，提交代码的入口。</li><li>Driver Program：驱动器程序，用于解耦客户端和内部实际操作，将用户程序转化为任务。</li><li>SparkContent：Spark 上下文，承接作用，用于配置上下文环境。</li><li>Cluster Manager（Resource Manager）：集群资源管理器，统一资源管理与任务调度。</li><li>Application Master：任务的执行，调度指挥者。</li><li>Worker Node：工作节点，任务的实际执行者。</li></ul><h1 id="三：角色间关系"><a href="#三：角色间关系" class="headerlink" title="三：角色间关系"></a>三：角色间关系</h1><ol><li>客户端接收到用户指令、代码；</li><li>驱动器服务于客户端，承接指令传达给集群资源管理器；</li><li>集群资源管理器根据当前情况，进行资源调度，生成一个任务调度者 AM（Application Master）</li><li>AM 给相应的工作节点分配任务；</li><li>工作节点执行任务，执行完毕，结果返回给 AM，并向资源管理器汇报自身资源情况，任务已完成当前空闲状态。</li><li>AM 接收到计算结果进行汇总，返回给客户端。</li></ol><h1 id="四：工作特性"><a href="#四：工作特性" class="headerlink" title="四：工作特性"></a>四：工作特性</h1><ul><li>内存计算</li><li>多线程</li><li>缓存</li><li>每一个 AM 都有一批专享的 Executor，以多线程方式启动多个 Task 任务，并行的线程计算任务缓存 RDD数据缓存块，存储复用的数据模块。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：架构总览&quot;&gt;&lt;a href=&quot;#一：架构总览&quot; class=&quot;headerlink&quot; title=&quot;一：架构总览&quot;&gt;&lt;/a&gt;一：架构总览&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/6a386de9.html/1629182444027-d40b52f7-9</summary>
      
    
    
    
    <category term="Spark" scheme="https://tuumest.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://tuumest.cn/tags/Spark/"/>
    
  </entry>
  
</feed>
